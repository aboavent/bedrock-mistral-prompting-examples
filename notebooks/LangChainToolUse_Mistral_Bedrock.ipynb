{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# LangChain Tool Use with Mistral models on Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we walkthrough an implementation of function calling and agentic workflows using Mistral models on Amazon Bedrock. Function Calling is a powerful technique that allows large language models to connect to external tools, systems, or APIs to enable, which can be executed to perform actions based on user's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58ce86-3cb5-4a4b-9735-dc4684db731f",
   "metadata": {},
   "source": [
    "Throughout this notebook, we demonstrate an agentic workflow that leverages Mistral models on Amazon Bedrock to create a seamless function calling experience. We explore techniques for crafting effective prompts over function calling and developing custom helper functions capable of understanding an API's data structure. These helper functions can identify the necessary tools and methods to be executed during the agentic workflow interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629d7aa-5f4a-4f53-b1fb-e19521e985a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "This notebook does not use native function calling. Naive function calling for Mistral models on Amazon Bedrock is coming soon.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fcd65-01b7-418d-988e-f80d05ebb4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Mistral Model Selection\n",
    "\n",
    "Today, there are three Mistral models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Mistral 7B Instruct\n",
    "\n",
    "- **Description:** A 7B dense Transformer model, fast-deployed and easily customizable. Small yet powerful for a variety of use cases.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 2. Mixtral 8X7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Max Tokens:** 4,096\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 3. Mistral Large\n",
    "\n",
    "- **Description:** A cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Code Generation, RAG, or Agents\n",
    "\n",
    "### 4. Mistral Large 2\n",
    "- **Description:** [Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) is the most advanced language model developed by French AI startup Mistral AI. It also has support for function calling and JSON format.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 128k\n",
    "- **Languages:** Natively fluent in French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean\n",
    "- **Supported Use Cases:** precise instruction following, text summarization, translation, complex multilingual reasoning tasks, math and coding tasks including code generation\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below compares the model performance on the Massive Multitask Language Understanding (MMLU) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Mistral 7B Instruct | 62.5%      | \\$0.00015                    | \\$0.0002                      |\n",
    "| Mixtral 8x7B Instruct | 70.6%      | \\$0.00045                    | \\$0.0007                      |\n",
    "| Mistral Large | 81.2%      | \\$0.008                   | \\$0.024                     |\n",
    "| Mistral Large 2 | 84.0%      | \\$0.004                   | \\$0.012                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Mistral Model Selection Guide](https://docs.mistral.ai/guides/model-selection/)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3978ae9-57d0-4dca-b94b-b6493f8eade0",
   "metadata": {},
   "source": [
    "---\n",
    "## Supported parameters\n",
    "\n",
    "The Mistral AI models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "The Mistral AI models have the following inference parameters:\n",
    "\n",
    "- **Temperature** - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- **Top P** - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- **Top K** - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- **Maximum Length** - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- **Stop sequences** - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db24c2-384a-4d1e-b6b6-a832ac002500",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11710041-4077-4700-a47a-2459d1be4fb9",
   "metadata": {},
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "## Setup and Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba91a5b-4ceb-4e60-9f89-b31a4e446c00",
   "metadata": {},
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ef5d8-904d-4cce-8522-e4b21279ceb1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e8ec-674a-44c2-be1f-0a92df6b7ca6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. In this notebook, it's only used to initialize a `Bedrock Client` and convert our functions into a JSON schema that allows Mistral to interact with our custom function calling workflow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.20\n",
    "langchain-experimental==0.0.58\n",
    "boto3==1.34.105\n",
    "sqlalchemy==2.0.30\n",
    "pandas==2.2.2\n",
    "pydantic==2.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf00ca0-9a89-4a3e-ac25-5f172dac43d5",
   "metadata": {},
   "source": [
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc6fb1-8ed0-45ab-b30c-c7f7a004e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-aws --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d59d834-3f3c-46e5-8cb8-56def569a934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains.openai_functions import convert_to_openai_function as convert_to_llm_fn\n",
    "from langchain.tools import tool\n",
    "import pandas as pd\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc107b3-7dfa-494a-9269-6229b6dccb24",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18477cd8-aeb2-4fec-b9ea-74b73f7386f4",
   "metadata": {},
   "source": [
    "Let's say, we have a transactional dataset tracking customers, payment amounts, payment dates, and whether payments have been fully processed for each transaction identifier. For more information about the sample dataset, please visit the documentation for [Mistral Function Calling](https://docs.mistral.ai/capabilities/function_calling/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data()-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a JSON file into a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing the data loaded from the JSON file.\n",
    "        If an error occurs during file loading, an error message is returned instead.\n",
    "\n",
    "    \"\"\"\n",
    "    local_path = \"sample_data/transactions.json\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_json(local_path)\n",
    "        return df\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        return  f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ad9e5a-5525-49a8-be42-1b8e7263aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id customer_id  payment_amount payment_date payment_status\n",
      "0          T1001        C001          125.50   2021-10-05           Paid\n",
      "1          T1002        C002           89.99   2021-10-06         Unpaid\n",
      "2          T1003        C003          120.00   2021-10-07           Paid\n",
      "3          T1004        C002           54.30   2021-10-05           Paid\n",
      "4          T1005        C001          210.20   2021-10-08        Pending\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb7ad0-64ea-408f-a946-8a0601202838",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc0e89-c9fe-4acd-8a9e-bc3fd7a5c828",
   "metadata": {},
   "source": [
    "Function calling is the ability to reliably connect a large language model (LLM) to external tools and enable effective tool usage and interaction with external APIs. Mistral models provide the ability for building LLM powered chatbots or agents that need to retrieve context for the model or interact with external tools by converting natural language into API calls to retrieve specific domain knowledge. From conversational agents and math problem solving to API integration and information extraction, multiple use cases can benefit from this capability provided by Mistral models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18150cc-1aac-4a7f-83f2-49d3c1a91abf",
   "metadata": {},
   "source": [
    "---\n",
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8ce00-28f4-4074-9c27-14c9d2263b28",
   "metadata": {},
   "source": [
    "LangChain Tools are interfaces that allow agents, chains, or language models to interact with the world. They typically include a name, description, JSON schema for input parameters, and the function to call. This information is used to prompt the language model on how to specify and take actions. We will use LangChain Tools to quickly transform our functions as a prompt that can be used by Mistral to execute function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37320d-94c0-4e25-9457-74ca7cd37297",
   "metadata": {},
   "source": [
    "Let’s consider we have two functions as our two tools: **retrieve_payment_status** and **retrieve_payment_date** to retrieve payment status and payment date given a transaction ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953f6f54-ff34-422b-9d99-873fe6f401c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(BaseModel):\n",
    "    transaction_id: str = Field(..., description='Transaction ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d4ad2-93a3-434c-b3a1-b310917784af",
   "metadata": {},
   "source": [
    "---\n",
    "### Function Call 1: Retrieve payment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db00433-5194-40e5-a1cc-a05a7d5c2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_payment_status(params: Params) -> str:\n",
    "    \"Get payment status of a transaction\"\n",
    "    data = load_data()\n",
    "\n",
    "    try:\n",
    "        # Attempt to retrieve the payment status for the given transaction ID\n",
    "        status = data[data.transaction_id == params.transaction_id].payment_status.item()\n",
    "    except ValueError:\n",
    "        # If the transaction ID is not found, return an error message\n",
    "        return {'error': f\"Transaction ID {params.transaction_id} not found.\"}\n",
    "\n",
    "    # Retrieve the payment status for the corresponding index\n",
    "    return json.dumps({'status': f\"{status}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3fe51-fe41-45dc-ba79-fcfa13dd7a7b",
   "metadata": {},
   "source": [
    "---\n",
    "### Function Call 2: Retrieve payment date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0be1320-f94d-46ae-a0cc-738681a4ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_payment_date(params: Params) -> str:\n",
    "    \"Get payment date of a transaction\"\n",
    "    data = load_data()\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve the payment date for the given transaction ID\n",
    "        date = data[data.transaction_id == params.transaction_id].payment_date.item()\n",
    "    except ValueError:\n",
    "        # If the transaction ID is not found, return an error message\n",
    "        return {'error': f\"Transaction ID {params.transaction_id} not found.\"}\n",
    "    \n",
    "    return json.dumps({'date': date})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779e6db-2322-4cdf-8f4b-2b97e532c380",
   "metadata": {},
   "source": [
    "---\n",
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae18c68-8ddd-4162-a036-f7dc496fca62",
   "metadata": {},
   "source": [
    "In this step, we will utilize another function from the LangChain library to transform a raw function or class into a format that can be easily understood and processed by a large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5e6964-e502-451c-9be5-5e7c171a133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_payment_status, retrieve_payment_date]\n",
    "functions = [convert_to_llm_fn(f) for f in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a932f565-d50a-4536-81b7-bd9af2fc1aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'retrieve_payment_status',\n",
       "  'description': 'retrieve_payment_status(params: __main__.Params) -> str - Get payment status of a transaction',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'params': {'type': 'object',\n",
       "     'properties': {'transaction_id': {'description': 'Transaction ID',\n",
       "       'type': 'string'}},\n",
       "     'required': ['transaction_id']}},\n",
       "   'required': ['params']}},\n",
       " {'name': 'retrieve_payment_date',\n",
       "  'description': 'retrieve_payment_date(params: __main__.Params) -> str - Get payment date of a transaction',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'params': {'type': 'object',\n",
       "     'properties': {'transaction_id': {'description': 'Transaction ID',\n",
       "       'type': 'string'}},\n",
       "     'required': ['transaction_id']}},\n",
       "   'required': ['params']}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855eea6-7a35-4a4f-b0db-93c4a9d3641a",
   "metadata": {},
   "source": [
    "---\n",
    "## Agentic Workflow Orchestration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d07d8-236e-4c19-a7df-05ff1c6b5e0a",
   "metadata": {},
   "source": [
    "An **Agentic Workflow** refers to an iterative and multi-step approach which uses large language models (LLMs) as AI Agents to perform a list of actions before the user receives an actual response. The agents can be configured to embody specific personalities/roles by not just generating \"responses\" but also engaging with multiple systems and tools. In this section, we aim to orchestrate all the steps to create a simple agentic workflow that takes a question,then searches for the right tool/function, executes the function, and answers the user in a human-readable way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3804805-66c7-43dc-bae5-d6b19024e6ab",
   "metadata": {},
   "source": [
    "In this section, we defined helper functions to automate the process of identifying the **function call** to be used by the LLM, extract the function from its response, and execute the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ea878-0da2-4a9d-ab1a-4af3e8a10638",
   "metadata": {},
   "source": [
    "Helper Function: Extracts function call representations enclosed within XML tags (`<functioncall>` and `<multiplefunctions>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4e1954-aed0-416d-966a-56d3e4d8ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_calls(completion: str):\n",
    "    if isinstance(completion, str):\n",
    "        content = completion\n",
    "    else:\n",
    "        content = completion.content\n",
    "\n",
    "    # Multiple functions lookup\n",
    "    mfn_pattern = r\"<multiplefunctions>(.*?)</multiplefunctions>\"\n",
    "    mfn_match = re.search(mfn_pattern, content, re.DOTALL)\n",
    "\n",
    "    # Single function lookup\n",
    "    single_pattern = r\"<functioncall>(.*?)</functioncall>\"\n",
    "    single_match = re.search(single_pattern, content, re.DOTALL)\n",
    "    \n",
    "    functions = []\n",
    "    \n",
    "    if not mfn_match and not single_match:\n",
    "         # No function calls found\n",
    "        return None\n",
    "    elif mfn_match:\n",
    "        # Multiple function calls found\n",
    "        multiplefn = mfn_match.group(1)\n",
    "        for fn_match in re.finditer(r\"<functioncall>(.*?)</functioncall>\", multiplefn, re.DOTALL):\n",
    "            fn_text = fn_match.group(1)\n",
    "            try:\n",
    "                functions.append(json.loads(fn_text.replace('\\\\', '')))\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # Ignore invalid JSON\n",
    "    else:\n",
    "        # Single function call found\n",
    "        fn_text = single_match.group(1)\n",
    "        try:\n",
    "            functions.append(json.loads(fn_text.replace('\\\\', '')))\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Ignore invalid JSON\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2f00a-6f53-4572-ab12-889d5a4397ca",
   "metadata": {},
   "source": [
    "Helper Function: Executes function call with the arguments captured by the LLM during the AI/user interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be629d9-fe51-43ba-98fd-8ca85459ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(function_list: list):\n",
    "    for function_dict in function_list:\n",
    "        function_name = function_dict['name']\n",
    "        arguments = function_dict['arguments']\n",
    "        \n",
    "        # Check if the function exists in the current scope\n",
    "        if function_name in globals():\n",
    "            func = globals()[function_name]\n",
    "            \n",
    "            # Call the function with the provided arguments\n",
    "            result = func.invoke(input=arguments)\n",
    "            return result\n",
    "        else:\n",
    "            return {'error': f\"Function '{function_name}' not found.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996ba3f-4838-4c2e-845e-207ea363ece7",
   "metadata": {},
   "source": [
    "Helper Function: The agentic workflow function contains the logic for orchestrating the execution of function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "908dbb19-8636-470d-84db-d85f3dc3df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agentic_workflow(prompt: str, model: str, functions: list):\n",
    "    # Define the function call format\n",
    "    fn = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\"\n",
    "\n",
    "    # Prepare the function string for the system prompt\n",
    "    fn_str = \"\\n\".join([str(f) for f in functions])\n",
    "    \n",
    "    # Define the system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "You are a helpful assistant with access to the following functions:\n",
    "\n",
    "{fn_str}\n",
    "\n",
    "To use these functions respond with:\n",
    "\n",
    "<multiplefunctions>\n",
    "    <functioncall> {fn} </functioncall>\n",
    "    <functioncall> {fn} </functioncall>\n",
    "    ...\n",
    "</multiplefunctions>\n",
    "\n",
    "Edge cases you must handle:\n",
    "- If there are no functions that match the user request, you will respond politely that you cannot help.\n",
    "- If the user has not provided all information to execute the function call, ask for more details. Only, respond with the information requested and nothing else.\n",
    "- If asked something that cannot be determined with the user's request details, respond that it is not possible to fullfill the request and explain why.\n",
    "\"\"\"\n",
    "    # Prepare the messages for the language model\n",
    "    messages = [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", prompt),\n",
    "    ]\n",
    "    \n",
    "    # Invoke the language model and get the completion\n",
    "    completion = llm.invoke(messages)\n",
    "    content = completion.content.strip()\n",
    "\n",
    "    # Extract function calls from the completion\n",
    "    functions = extract_function_calls(content)\n",
    "\n",
    "    if functions:\n",
    "        # If function calls are found, execute them and return the response\n",
    "        fn_response = execute_function(functions)\n",
    "        return fn_response\n",
    "    else:\n",
    "        # If no function calls are found, return the completion content\n",
    "        return {\"error\": content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968670b-bca2-483c-86a0-777ab8fd14b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Q&A: Agentic Worklflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8d2df-4287-437b-8b52-1fe7855ba478",
   "metadata": {},
   "source": [
    "Here, we defined a prompt to handle the conversation history and answer follow-up questions withi the agentic workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb120d74-3299-4191-8fb5-a5f3b394ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_history_prompt = \"\"\"\n",
    "#############\n",
    "Chat History:\n",
    "{chat_history}\n",
    "#############\n",
    "\n",
    "You are an AI assistant designed to human-like responses based on the transaction details provided to you.\n",
    "\n",
    "Transaction details:\n",
    "{transaction_details}\n",
    "\n",
    "Provide clear and concise responses based solely on the data, without making any assumptions or inferences beyond what is contained in the transaction details. \n",
    "If the transaction details shows an 'error' message, it means we do not have enough information. In this case, respond that it is not possible to fullfill the request and explain why\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb9a3e-f1f9-48bf-bb4b-5c990f40ff68",
   "metadata": {},
   "source": [
    "Next, the following function allows a user to have a conversation with Mistral models, where Mistral generates responses based on the user's input and some action or function call is executed to perform actions on your behalf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78590e50-0fe1-4f98-aaf3-9a22f7a47d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display text as markdown\n",
    "def printmd(text: str):\n",
    "    display(Markdown(text))\n",
    "\n",
    "\n",
    "# Run agent for Questions and Answers\n",
    "def run_qa_agent(llm: ChatBedrock):\n",
    "\n",
    "    generation_func = partial(run_agentic_workflow, model=llm, functions=functions)\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    conversation_history = []\n",
    "    \n",
    "    print(\"Welcome to the LLM Conversation! Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Ask a question: \\n\")\n",
    "    \n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "    \n",
    "        # Add user input to the conversation history\n",
    "        conversation_history.append((\"user\", user_input))\n",
    "    \n",
    "        # Prepare the prompt from the conversation history\n",
    "        prompt = \"\\n\".join([q[1] for q in conversation_history if 'user' in q])\n",
    "    \n",
    "        # Generate the action to take based on the detected function call\n",
    "        action_response = generation_func(prompt)\n",
    "    \n",
    "        # Prepare the question-answer prompt\n",
    "        qa_prompt = conv_history_prompt.format(chat_history=action_response, transaction_details=prompt)\n",
    "    \n",
    "        # Prepare the messages for final LLM response\n",
    "        messages = [\n",
    "            (\"system\", qa_prompt),\n",
    "            (\"user\", str(action_response)),\n",
    "        ]\n",
    "    \n",
    "        # Get the response from the LLM\n",
    "        response = llm.invoke(messages).content.strip()\n",
    "    \n",
    "        if 'error' in action_response:\n",
    "            # If there is an error, print the LLM response and keep the conversation going\n",
    "            printmd(f\"**Answer:**\\n {response}\")\n",
    "        else:\n",
    "            # If there is no error, print the LLM response and exit the loop\n",
    "            printmd(f\"**Answer:**\\n {response}\")\n",
    "            conversation_history = []\n",
    "            printmd(\"**Is there anything else I can help you with? If not, type 'exit' to finish the conversation.**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285ba1f-bd7c-4d25-8c4b-1331bbc32ec0",
   "metadata": {},
   "source": [
    "In this example, Mistral 7B Instruct is our default model, but feel free to pick any other available Mistral model to experiment with this agentic workflow. You just need to change the `DEFAULT_MODEL` variable.\n",
    "\n",
    "Additionally, you may want to change the AWS region as well. If so, just change the `AWS_REGION` variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3189c41a-a7e5-4ee5-b0c3-2445a6f8aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2402_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "mistral_large_2407_id = 'mistral.mistral-large-2407-v1:0'\n",
    "\n",
    "DEFAULT_MODEL = instruct_mistral7b_id\n",
    "AWS_REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2b6c66-d996-482e-905b-cbbc8c6d2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock with LangChain\n",
    "llm = ChatBedrock(\n",
    "        model_id=DEFAULT_MODEL,\n",
    "        model_kwargs={\"temperature\": 0.1},\n",
    "        region_name=AWS_REGION\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8be06da-a0b5-4007-87f7-c10059dd7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the LLM Conversation! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " What's the status of my transaction?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " I'm sorry, but it's not possible to fulfill your request at the moment. The transaction details provided do not contain enough information, such as a transaction ID, to determine the status of your transaction. Please provide the transaction ID so I can assist you better."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " My transaction ID is T1001\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " The status of your transaction with ID T1001 is \"Paid\". This means that the payment has been successfully processed and completed. If you have any further questions or need additional assistance, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Is there anything else I can help you with? If not, type 'exit' to finish the conversation.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " On what date was my transaction ID made?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " I'm sorry, but it is not possible to fulfill your request at this time. In order to provide you with the date of your transaction, I need the transaction ID. Without this information, I am unable to look up the details of your transaction. Please provide me with the transaction ID so that I can assist you further."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " My transaction ID is T1002\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       " Based on the transaction details provided, your transaction ID T1002 was made on October 6, 2021."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Is there anything else I can help you with? If not, type 'exit' to finish the conversation.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "run_qa_agent(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458ce6a-216a-478d-968b-862fcfc8fe18",
   "metadata": {},
   "source": [
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a51e5-3c22-496d-9110-bbc7f07e9dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
