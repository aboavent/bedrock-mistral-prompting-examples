{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Mistral Nemo 12B on Amazon SageMaker with HuggingFace LLM DLC \n",
    "\n",
    "---\n",
    "\n",
    "[Mistral NeMo](https://mistral.ai/news/mistral-nemo/) is a 12-billion (12B) parameter model developed in collaboration between Mistral AI and NVIDIA. The model provides a large context window of up to 128k tokens, making it highly proficient in reasoning, world knowledge, and coding accuracy within its size category. \n",
    "\n",
    "Designed for seamless integration, Mistral NeMo can replace existing systems using the Mistral 7B model effortlessly. It is available under the Apache 2.0 license to foster widespread adoption among researchers and enterprises. \n",
    "\n",
    "Mistral NeMo employs the Tekken tokenizer, which is more efficient than previous models, particularly in compressing source code and various languages. The model provides robust multilingual capabilities, covering languages such as English, French, Gerxman, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n",
    "\n",
    "Additionally, the model has undergone advanced fine-tuning to excel in following instructions, reasoning, multi-turn conversations, and code generation. Pre-trained base and instruction-tuned checkpoints are accessible on HuggingFace.\n",
    "\n",
    "---\n",
    "\n",
    "Mistral NeMo 12B base performance and accuracy compared to Gemma 2 9B and Llama 8B:\n",
    "\n",
    "![nemo](imgs/nemo-base-performance.png)\n",
    "\n",
    "Mistral NeMo 12B instruction fine tuned results:\n",
    "\n",
    "![nemo](imgs/nemo-instruct-performance.png)\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to deploy the [mistralai/Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407), as well as the [mistralai/Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407) model to [Amazon SageMaker](https://aws.amazon.com/sagemaker/) and perform inference. We will utilize the Hugging Face LLM DLC, a purpose-built Inference Container designed to facilitate the deployment of Large Language Models (LLMs) in a secure and managed environment. This Deep Learning Container (DLC) is powered by <b>Text Generation Inference (TGI)</b>, a scalable and optimized solution for deploying and serving LLMs efficiently. \n",
    "\n",
    "In the notebook, we will cover how to:\n",
    "1. Set up environment\n",
    "2. Retrieve the DLC\n",
    "3. Model architecture and hardware requirements\n",
    "4. Deploy Mistral NeMo to Amazon SageMaker\n",
    "5. Run inference and chat with the model\n",
    "6. Clean up\n",
    "7. Conclusion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1 Set up environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "\n",
    "You can either execute this notebook in SageMaker Studio or locally. If you are new to Amazon SageMaker, please follow the guidance provided here: [Guide to getting set up with Amazon SageMaker\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html). \n",
    "\n",
    "For local setup please make sure to configure configure the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your credentials, install necessary libraries and ensure you have [permissions for SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n",
    "In addition to either setting up your local environment with the local permissions or setting up SageMaker Studio please also make sure to check instance quota. This Jupyter Notebook itself can be run on a ml.t3.medium instance. To deploy the model to the SageMaker endpoint, you may need to request a quota increase. To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the Service Quotas console.\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "    - ml.g5.12xlarge for endpoint usage\n",
    "4. If needed, request a quota increase for these resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install necessary packages and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker pip boto3 botocore python-dotenv --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary libraries\n",
    "\n",
    "In the below section, we import the necessary libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.219.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name power-user-plus to get Role path.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.219.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "HF_HUB_TOKEN = os.environ.get('HF_HUB_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Retrieve the HuggingFace Deep Learning Container (DLC)\n",
    "\n",
    "The first step is to retrieve the DLC URI. This URI is crucial as it serves as a reference point for the HuggingFaceModel class, specifically through the image_uri parameter. The DLC is a pre-configured Docker image that encapsulates all the necessary dependencies and frameworks required to run our LLM efficiently in the SageMaker environment.\n",
    "To streamline this process, the sagemaker SDK provides a specialized method called `get_huggingface_llm_image_uri`. This method is designed to retrieve the most suitable Hugging Face LLM DLC URI based on two key parameters:\n",
    "\n",
    "<b>backend</b>: This specifies the deep learning inference framework, in this case which can be huggingface/tgi, lmi, etc.\n",
    "\n",
    "<b>region</b>: This refers to the AWS region where you're deploying your model. It's important to use the correct region to ensure optimal performance and compliance with data residency requirements.\n",
    "\n",
    "In this case we manually added the latest TGI image URI from the [deep learning containers repository](https://github.com/aws/deep-learning-containers/releases?q=tgi+AND+gpu&expanded=true). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface llm image uri: 763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0\n"
     ]
    }
   ],
   "source": [
    "tgi_image_uri = \"763104351884.dkr.ecr.eu-central-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.2.0-gpu-py310-cu121-ubuntu22.04-v2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model architecture and hardware requirements\n",
    "\n",
    "[Mistral NeMo](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407) is a 12-billion parameter open-weight model with a 128k context length. \n",
    "\n",
    "Taking into account the model architecture (parameters, precision factor, ...) we can define the model loading size and KV cache size. Further taking into account batch size and token length we can approximate GPU vRAM requirement. \n",
    "\n",
    "For small scale tasks, the `g5.12xlarge` with 96GB VRAM comfortably accommodates the model's memory requirements.\n",
    "\n",
    "When running into CUDA out of memory issues you can either play with the hyperparameters (e.g. reduce token length), leverage quantization or change to a larger instance. \n",
    "\n",
    "> For the purpose of this notebook, we will be deploying the unquantized version of the model to a sagemaker endpoint with TGI on the g5.12xlarge.\n",
    "\n",
    "\n",
    "| Model                                                                       | Instance Type       | Quantization | NUM_GPUS | VRAM |\n",
    "|-----------------------------------------------------------------------------|---------------------|--------------|----------|------|\n",
    "| [Mistral NeMo Instruct](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407) | `(ml.)g5.12xlarge` | `-` / fp8 (8-bit)        | 4        | 96GB |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deploy Mistral NeMo to Amazon SageMaker\n",
    "\n",
    "To deploy [Mistral NeMo](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-24078) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type`, and `huggingface_hub_token`. We will use a `g5.12xlarge` instance type, which has 48 vCPUs, 192 GB of memory, 4 NVIDIA A10G GPUs and 96GB of GPU memory. Depending on the instance type being used, you will also need to chnage the `number_of_gpus` to reflect this (refer to the table above).\n",
    "\n",
    "We set the `health_check_timeout` to 900 to provide the model with enough time to load into memory. This can be adjusted as needed. If your container is correctly set up and the CloudWatch logs indicate a health check timeout, you should increase this quota so the container has enough time to respond to health checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sagemaker endpoint configuration\n",
    "endpoint_name=\"mistral-nemo-instruct-hf-dlc-tgi\"\n",
    "instance_type = \"ml.g5.12xlarge\"   \n",
    "number_of_gpus = 4                 #number of gpus the instance in use has\n",
    "health_check_timeout = 900         #additional time to load in the model\n",
    "\n",
    "config = {\n",
    "  'HUGGING_FACE_HUB_TOKEN': HF_HUB_TOKEN,                     # add your huggingface hub access token with read permissions\n",
    "  'HF_MODEL_ID': \"mistralai/Mistral-Nemo-Instruct-2407\",      # model_id from HuggingFace\n",
    "  'HF_TASK': \"text-generation\",                               # HuggingFace inference pipeline\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpus),                  # Number of GPU used per replica\n",
    "  'HF_MODEL_QUANTIZE': 'fp8'                                  # TGI can quantize the model to 8-bit quantization to further improve performance at the cost of a certain degree of loss to precision\n",
    "}\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=tgi_image_uri, \n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g5.12xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    "  endpoint_name=endpoint_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. The mistral models have the following prompt structure:\n",
    "  \n",
    "```\n",
    "<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2 [/INST]\n",
    "```\n",
    "\n",
    "Additionally parameters can be defined as in the `parameters` attribute of the payload. Let's now define the parameters and the prompt for our payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload params\n",
    "payload = {\n",
    "    \"temperature\": 0.1, # controls the randomness of the predictions\n",
    "    \"top_p\": 0.6, # controls the diversity of the generated text\n",
    "    \"top_k\": 50, # controls the diversity of the generated text\n",
    "    \"max_new_tokens\": 4000, # specifies the maximum number of tokens to generate in the response\n",
    "    \"stop\": [\"</s>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General knowledge question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]What is the history behind Artificial Intelligence? [/INST]Artificial Intelligence (AI) has a rich history that spans over seven decades, evolving from a theoretical concept to the powerful technology it is today. Here's a brief overview of its history:\n",
      "\n",
      "1. **Alan Turing (1936-1950)**: The concept of AI can be traced back to the work of British mathematician and computer scientist Alan Turing. In 1936, he proposed the idea of a \"universal machine\" that could carry out calculations of any complexity, given enough time and resources. In 1950, he introduced the \"Turing Test\" to determine whether a machine could exhibit intelligent behavior indistinguishable from that of a human.\n",
      "\n",
      "2. **John McCarthy Coins the Term \"Artificial Intelligence\" (1956)**: While working at Dartmouth College, John McCarthy coined the term \"Artificial Intelligence\" in a proposal for a conference on the subject. This conference is often considered the birth of the AI field.\n",
      "\n",
      "3. **Early AI Programs (1950s-1960s)**: In the 1950s and 1960s, early AI programs were developed, such as ELIZA, a natural language processing program created by Joseph Weizenbaum at MIT. These programs laid the groundwork for modern AI.\n",
      "\n",
      "4. **Expert Systems (1970s-1980s)**: During this period, AI research focused on creating \"expert systems\" that could make decisions based on a set of rules defined by human experts. These systems were among the first commercially successful applications of AI.\n",
      "\n",
      "5. **Machine Learning Emerges (1980s-1990s)**: The field of machine learning began to gain prominence during this time. Machine learning algorithms allow computers to learn from data without being explicitly programmed. This shift enabled AI to move beyond rule-based systems and start learning from and making predictions based on data.\n",
      "\n",
      "6. **Deep Learning Breakthrough (2006)**: Geoffrey Hinton's introduction of the autoencoder, a type of neural network, marked a significant breakthrough in deep learning. This led to a resurgence in AI capabilities, particularly in image and speech recognition.\n",
      "\n",
      "7. **AI in Everyday Life (2010s-Present)**: AI has become ubiquitous in our daily lives, powering technologies like voice assistants (Siri, Alexa), recommendation systems (Netflix, Amazon), autonomous vehicles (Tesla, Waymo), and more. The field continues to evolve, with advancements in areas like reinforcement learning, explainable AI, and AI ethics.\n",
      "\n",
      "This is a high-level overview, and there's much more to explore in the history of AI, including the contributions of many other researchers and the impact of AI on society.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"<s> [INST]What is the history behind Artificial Intelligence? [/INST]\"\n",
    "\n",
    "chat = llm.predict({\"inputs\":prompt, \"parameters\":payload})\n",
    "\n",
    "print(chat[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample math & reasoning generation question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Please provide a step by step reasoning process to estimate the number of smarties (sweet) that fit into a smart (car). [/INST]To estimate the number of Smarties (sweets) that can fit into a Smart (car), we'll follow a step-by-step reasoning process that involves several assumptions and calculations. Here's how we can approach this:\n",
      "\n",
      "1. **Determine the volume of the Smart car:**\n",
      "   - The Smart Fortwo (the smallest Smart car model) has a length of approximately 2.69 meters, a width of 1.55 meters, and a height of 1.66 meters.\n",
      "   - Calculate the volume (V) of the car using the formula for the volume of a rectangular prism: V = length × width × height.\n",
      "   - V = 2.69 m × 1.55 m × 1.66 m ≈ 6.77 cubic meters.\n",
      "\n",
      "2. **Determine the volume of a single Smartie:**\n",
      "   - A typical Smartie sweet is roughly spherical with a diameter of about 1.5 cm (or 0.015 meters).\n",
      "   - Calculate the radius (r) of the Smartie: r = diameter / 2 = 0.015 m / 2 = 0.0075 m.\n",
      "   - Calculate the volume (v) of a single Smartie using the formula for the volume of a sphere: v = (4/3)πr³.\n",
      "   - v ≈ (4/3)π(0.0075 m)³ ≈ 1.41 × 10^-6 cubic meters.\n",
      "\n",
      "3. **Estimate the number of Smarties that can fit into the car:**\n",
      "   - To find out how many Smarties can fit into the car, divide the volume of the car by the volume of a single Smartie.\n",
      "   - Number of Smarties = V_car / v_Smartie ≈ 6.77 m³ / 1.41 × 10^-6 m³ ≈ 4.79 × 10^6.\n",
      "\n",
      "So, based on these calculations and assumptions, approximately 4.79 million Smarties could fit into a Smart car. Keep in mind that this is a rough estimate, as it doesn't account for factors like the space between Smarties, the car's interior design, or the fact that Smarties aren't perfectly spherical.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"<s> [INST] Please provide a step by step reasoning process to estimate the number of smarties (sweet) that fit into a smart (car). [/INST]\"\n",
    "\n",
    "chat = llm.predict({\"inputs\":prompt, \"parameters\":payload})\n",
    "\n",
    "print(chat[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample code generation question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Create a React component that calculates body mass index. [/INST]Here is a simple React component that calculates Body Mass Index (BMI). This component takes weight and height as props and calculates the BMI using the formula: weight(kg) / height(m)².\n",
      "\n",
      "```jsx\n",
      "import React from 'react';\n",
      "\n",
      "const BmiCalculator = ({ weight, height }) => {\n",
      "  const bmi = weight / Math.pow(height, 2);\n",
      "\n",
      "  return (\n",
      "    <div>\n",
      "      <h2>Body Mass Index Calculator</h2>\n",
      "      <p>Weight: {weight} kg</p>\n",
      "      <p>Height: {height} m</p>\n",
      "      <p>BMI: {bmi.toFixed(2)}</p>\n",
      "      <p>\n",
      "        Interpretation:\n",
      "        {bmi < 18.5 ? 'Underweight' :\n",
      "        bmi >= 18.5 && bmi < 24.9 ? 'Normal weight' :\n",
      "        bmi >= 25 && bmi < 29.9 ? 'Overweight' :\n",
      "        bmi >= 30 ? 'Obesity' : ''}\n",
      "      </p>\n",
      "    </div>\n",
      "  );\n",
      "};\n",
      "\n",
      "export default BmiCalculator;\n",
      "```\n",
      "\n",
      "You can use this component in your app like this:\n",
      "\n",
      "```jsx\n",
      "import React from 'react';\n",
      "import BmiCalculator from './BmiCalculator';\n",
      "\n",
      "function App() {\n",
      "  return (\n",
      "    <div className=\"App\">\n",
      "      <BmiCalculator weight={75} height={1.8} />\n",
      "    </div>\n",
      "  );\n",
      "}\n",
      "\n",
      "export default App;\n",
      "```\n",
      "\n",
      "This will display the BMI and its interpretation based on the provided weight and height.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"<s> [INST] Create a React component that calculates body mass index. [/INST]\"\n",
    "\n",
    "chat = llm.predict({\"inputs\":prompt, \"parameters\":payload})\n",
    "\n",
    "print(chat[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Streaming Responses\n",
    "\n",
    "[Amazon SageMaker supports streaming responses](https://aws.amazon.com/de/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/) from your model. Below we will demonstrate how to create a streaming response using our existing endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import io\n",
    "\n",
    "from sagemaker.base_deserializers import StreamDeserializer\n",
    "\n",
    "boto3.set_stream_logger(\"\",logging.INFO)\n",
    "\n",
    "llm.deserializer=StreamDeserializer()\n",
    "\n",
    "smr = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class to iterate the lines via https://github.com/aws-samples/sagemaker-hosting/blob/main/GenAI-Hosting/Large-Language-Model-Hosting/LLM-Streaming/Falcon-40b-and-7b/falcon-40b-and-7b-tgi-streaming/falcon-7b-streaming_tgi.ipynb\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input from TGI container. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'data:{\"token\": {\"text\": \" a\"}}\\n\\n'\n",
    "    b'data:{\"token\": {\"text\": \" challenging\"}}\\n\\n'\n",
    "    b'data:{\"token\": {\"text\": \" problem\"\n",
    "    b'}}'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. It will also save any pending \n",
    "    lines that doe not end with a '\\n' to make sure truncations are concatinated\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '757670cc-b895-4331-a0f3-810fced965a6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '757670cc-b895-4331-a0f3-810fced965a6', 'x-amzn-invoked-production-variant': 'AllTraffic', 'x-amzn-sagemaker-content-type': 'text/event-stream', 'date': 'Sat, 10 Aug 2024 17:17:49 GMT', 'content-type': 'application/vnd.amazon.eventstream', 'transfer-encoding': 'chunked', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'text/event-stream', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.eventstream.EventStream object at 0x12d11da10>}\n",
      "Artificial Intelligence (AI) has a rich history that spans over seven decades, evolving from a theoretical concept to the powerful technology it is today. Here's a brief overview of its history:\n",
      "\n",
      "1. **Alan Turing (1936-1950)**: The concept of AI can be traced back to the work of British mathematician and computer scientist Alan Turing. In 1936, he proposed the \"Turing Machine,\" a theoretical model that could carry out calculations of any complexity, given enough time and resources. In 1950, he introduced the \"Turing Test\" to determine whether a machine could exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.\n",
      "\n",
      "2. **Birth of the Term \"Artificial Intelligence\" (1956)**: The term \"Artificial Intelligence\" was coined by John McCarthy at the Dartmouth Conference in 1956. This conference is often considered the birthplace of AI, where the idea of creating intelligent machines was first proposed.\n",
      "\n",
      "3. **Early AI Programs (1950s-1960s)**: In the 1950s and 1960s, early AI programs were developed, such as ELIZA, a natural language processing program created by Joseph Weizenbaum at MIT in the mid-1960s. ELIZA could engage in simple conversations with users, mimicking a psychotherapist.\n",
      "\n",
      "4. **AI Winter (1974-1980)**: Despite initial enthusiasm, AI research faced significant challenges and setbacks, leading to a period known as the \"AI Winter.\" During this time, funding for AI research was significantly reduced, and many researchers left the field.\n",
      "\n",
      "5. **Expert Systems and the Resurgence of AI (1980s)**: The 1980s saw a resurgence in AI research, driven by the development of expert systems. These were computer programs that could make decisions or solve complex problems based on the knowledge and expertise of human experts. The first commercial expert system, XCON (eXpert CONfigurer), was developed by Digital Equipment Corporation in 1980.\n",
      "\n",
      "6. **Machine Learning and Deep Learning (1990s-Present)**: The 1990s and early 2000s saw significant advancements in machine learning, a subset of AI that focuses on the idea that systems can learn from data, rather than being explicitly programmed. Deep learning, a type of machine learning inspired by the structure and function of the brain, has driven many of the recent breakthroughs in AI, including advances in image and speech recognition.\n",
      "\n",
      "7. **AI in Everyday Life (2010s-Present)**: Today, AI is ubiquitous, powering a wide range of technologies and services, from voice assistants like Siri and Alexa to recommendation systems used by Netflix and Amazon. It's also being used in more complex fields like healthcare, autonomous vehicles, and climate modeling.\n",
      "\n",
      "This is a very high-level overview, and there's a lot more to explore in the history of AI, including the contributions of many other researchers, the development of different AI subfields, and the ethical debates surrounding AI."
     ]
    }
   ],
   "source": [
    "stop_token = '</s>'\n",
    "\n",
    "body = {\n",
    "    \"inputs\":\"<s> [INST]What is the history behind Artificial Intelligence? [/INST]\", # prompt\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 4000,\n",
    "        \"return_full_text\": False\n",
    "    },\n",
    "    \"stream\": True # set stream to True to enable streaming response\n",
    "}\n",
    "\n",
    "resp = smr.invoke_endpoint_with_response_stream(EndpointName=llm.endpoint_name, Body=json.dumps(body), ContentType='application/json')\n",
    "print(resp)\n",
    "event_stream = resp['Body']\n",
    "start_json = b'{'\n",
    "for line in LineIterator(event_stream):\n",
    "    if line != b'' and start_json in line:\n",
    "        data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "        if data['token']['text'] != stop_token:\n",
    "            print(data['token']['text'],end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 19:19:44,453 sagemaker [INFO] Deleting model with name: huggingface-pytorch-tgi-inference-2024-08-10-15-57-56-989\n",
      "2024-08-10 19:19:44,453 sagemaker [INFO] Deleting model with name: huggingface-pytorch-tgi-inference-2024-08-10-15-57-56-989\n",
      "2024-08-10 19:19:44,621 sagemaker [INFO] Deleting endpoint configuration with name: mistral-nemo-instruct-hf-dlc-tgi\n",
      "2024-08-10 19:19:44,621 sagemaker [INFO] Deleting endpoint configuration with name: mistral-nemo-instruct-hf-dlc-tgi\n",
      "2024-08-10 19:19:44,787 sagemaker [INFO] Deleting endpoint with name: mistral-nemo-instruct-hf-dlc-tgi\n",
      "2024-08-10 19:19:44,787 sagemaker [INFO] Deleting endpoint with name: mistral-nemo-instruct-hf-dlc-tgi\n"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've explored the process of deploying the Mistral NeMo 12B base and instruction tuned model on Amazon SageMaker and performing inference using the Hugging Face LLM Deep Learning Container. Leveraging the power of Text Generation Inference (TGI), we've demonstrated how to efficiently deploy and serve this large language model in a secure, managed environment.\n",
    "\n",
    "We've walked through the steps of setting up the SageMaker environment, configuring the model deployment, and showcasing both standard inference and streaming responses. \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
