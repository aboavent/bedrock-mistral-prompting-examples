{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral NeMo: A Comparative Analysis of Text Models\n",
    "\n",
    "This notebook provides a comprehensive comparison of Mistral NeMo with Mistral 7B and Mixtral 8x7B, three advanced language models developed by Mistral AI. Our primary objective is to evaluate Mistral NeMo's performance, identify its strengths and limitations, and establish best practices for integrating it into workflows that require accurate and efficient natural language processing.\n",
    "\n",
    "To accomplish this, we will conduct a series of controlled tests and qualitative assessments, utilizing appropriate APIs and inference endpoints for each model. We will explore model outputs on various natural language processing tasks, including but not limited to:\n",
    "\n",
    "- General text generation and completion\n",
    "- Question answering\n",
    "- Sentiment analysis\n",
    "- Text summarization\n",
    "- Language translation\n",
    "\n",
    "Additionally, we will employ a judging model to systematically evaluate and rank the quality of responses from each model.\n",
    "\n",
    "Through this process, the notebook will:\n",
    "\n",
    "- Demonstrate how to efficiently use Mistral NeMo's endpoints for real-time inference.\n",
    "- Compare Mistral NeMo's capabilities to Mistral 7B and Mixtral 8x7B using standardized prompts and test datasets.\n",
    "- Help you understand the relative advantages of Mistral NeMo, guiding you in deciding when and how to deploy it in your own applications.\n",
    "\n",
    "We have included licensing details and quick-start references for further exploration. By the end of this analysis, you should have a clear perspective on Mistral NeMo's performance profile and actionable insights into optimizing its use in your specific scenarios.\n",
    "\n",
    "All example outputs have been preserved in this notebook, allowing you to review the results without needing to run the code on your own instance or pay for compute costs.\n",
    "\n",
    "## Model Overview\n",
    "- **Mistral NeMo**: A state-of-the-art 12 billion multilingual model with 128k context length\n",
    "- **Mistral 7B**: A 7 billion parameter model known for its efficiency and performance\n",
    "- **Mixtral 8x7B**: A mixture-of-experts model with 8 experts, each containing 7 billion parameters\n",
    "\n",
    "\n",
    "By conducting these comparisons, we aim to provide a clear understanding of how Mistral NeMo stands in relation to its predecessors and guide users in selecting the most appropriate model for their specific use cases.\n",
    "\n",
    "## Model License\n",
    "\n",
    "- **License:** Apache 2.0 - Mistral NeMo\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The instructions to deploy Mistral NeMo from Bedrock Marketplace and it's capabilities can be found in the [Deploy-Mistral-NeMo-from-Bedrock-Marketplace-and-its-Capabilities.ipynb](https://github.com/aws-samples/mistral-on-aws/blob/main/notebooks/Deploy-Mistral-NeMo-from-Bedrock-Marketplace-and-its-Capabilities.ipynb) notebook.\n",
    "\n",
    "Want to learn more about Mistra NeMo?  \n",
    "[Mistral AI Blog](https://mistral.ai/news/mistral-nemo/)  \n",
    "[NVIDIA Model Card](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/modelcard)  \n",
    "[AWS Blog](https://aws.amazon.com/blogs/machine-learning/mistral-nemo-instruct-2407-and-mistral-nemo-base-2407-are-now-available-on-sagemaker-jumpstart/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will install python modules needed in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "aiobotocore 2.13.3 requires botocore<1.34.163,>=1.34.70, but you have botocore 1.36.6 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.3 which is incompatible.\n",
      "autogluon-core 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-core 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-features 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires torch<2.4,>=2.2, but you have torch 2.4.1.post100 which is incompatible.\n",
      "autogluon-tabular 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-tabular 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires gluonts==0.15.1, but you have gluonts 0.14.3 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires torch<2.4,>=2.2, but you have torch 2.4.1.post100 which is incompatible.\n",
      "langchain-aws 0.1.18 requires boto3<1.35.0,>=1.34.131, but you have boto3 1.36.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install botocore boto3 sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker.djl_inference import DJLModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors to display information\n",
    "RESET = \"\\033[0m\"\n",
    "GREEN = \"\\033[38;5;29m\"\n",
    "BLUE = \"\\033[38;5;43m\"\n",
    "ORANGE = \"\\033[38;5;208m\"\n",
    "PURPLE = \"\\033[38;5;93m\"\n",
    "RED = \"\\033[38;5;196m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration\n",
    "MODEL_SOURCE_ID = 'huggingface-llm-mistral-nemo-instruct-2407'\n",
    "MODEL_SOURCE_ARN = 'arn:aws:sagemaker:{region}:aws:hub-content/SageMakerPublicHub/Model/huggingface-llm-mistral-nemo-instruct-2407/1.0.3'\n",
    "INSTANCE_TYPE = 'ml.g6.24xlarge'\n",
    "ENDPOINT_NAME = 'nemo-on-bedrock'\n",
    "\n",
    "MISTRAL_7B_MODEL_ID = 'mistral.mistral-7b-instruct-v0:2'\n",
    "MIXTRAL_8x7B_MODEL_ID = 'mistral.mixtral-8x7b-instruct-v0:1'\n",
    "JUDGE_MODEL_ID = 'anthropic.claude-3-5-sonnet-20241022-v2:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "# function to grab aws account id, sagemaker execution role and region\n",
    "def get_current_session_info():\n",
    "    sagemaker_role_arn = sagemaker.get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    account_id = session.account_id()\n",
    "    region = session._region_name\n",
    "\n",
    "    return account_id, region, sagemaker_role_arn\n",
    "\n",
    "aws_account_id, aws_region, sagemaker_role_arn = get_current_session_info()\n",
    "\n",
    "print(f'aws region: {aws_region}')\n",
    "\n",
    "MODEL_SOURCE_ARN = MODEL_SOURCE_ARN.format(region=aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bedrock client object\n",
    "bedrock_client = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bedrock marketplace endpoint\n",
    "def create_endpoint(model_source_arn: str, \n",
    "                    endpoint_name: str,\n",
    "                    instance_type: str, \n",
    "                    instance_count: int = 1):\n",
    "\n",
    "    response = bedrock_client.create_marketplace_model_endpoint(\n",
    "            modelSourceIdentifier=model_source_arn,\n",
    "            endpointConfig={\n",
    "                'sageMaker': {\n",
    "                    'initialInstanceCount': instance_count,\n",
    "                    'instanceType': instance_type,\n",
    "                    'executionRole': sagemaker_role_arn,\n",
    "                }\n",
    "            },\n",
    "            acceptEula=True,\n",
    "            endpointName=endpoint_name\n",
    "        )\n",
    "    return response\n",
    "\n",
    "create_response = create_endpoint(model_source_arn=MODEL_SOURCE_ARN, endpoint_name=ENDPOINT_NAME, instance_type=INSTANCE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve endpoint arn from response text\n",
    "\n",
    "endpoint_arn = create_response['marketplaceModelEndpoint']['endpointArn']\n",
    "MISTRAL_NEMO_MODEL_ID = endpoint_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: Creating\n",
      "endpoint status: InService\n"
     ]
    }
   ],
   "source": [
    "# Check endpoint creation status until it's in service\n",
    "\n",
    "while(True):\n",
    "    endpoint_reponse = bedrock_client.get_marketplace_model_endpoint(endpointArn=endpoint_arn)\n",
    "    status = endpoint_reponse['marketplaceModelEndpoint']['endpointStatus']\n",
    "    print(f'endpoint status: {status}')\n",
    "    if (status != 'Creating'):\n",
    "        break\n",
    "\n",
    "    # wait for 10 seconds\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bedrock runtime object\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to invoke Bedrock model using invoke APIs\n",
    "def invoke_model(model_id: str, prompt: str, display_usage=False):\n",
    "    \n",
    "    prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.7,\n",
    "        \"top_k\": 50\n",
    "    })\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    response = bedrock_runtime.invoke_model(body=body,\n",
    "                                            modelId=model_id,\n",
    "                                            accept=accept,\n",
    "                                            contentType=contentType)\n",
    "    \n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    outputs = response_body.get('outputs')\n",
    "    response = ''\n",
    "    for index, output in enumerate(outputs):\n",
    "        response = response + output['text']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to invoke Bedrock model using invoke APIs and message format\n",
    "def invoke_model_with_message_format(model_id: str, prompt: str, display_usage=False):\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0.7,\n",
    "             \"top_p\": 0.7,\n",
    "            \"top_k\": 50\n",
    "        }\n",
    "\n",
    "    body = json.dumps(payload)\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    response = bedrock_runtime.invoke_model(body=body,\n",
    "                                            modelId=model_id,\n",
    "                                            accept=accept,\n",
    "                                            contentType=contentType)\n",
    "    \n",
    "    response = json.loads(response.get('body').read())\n",
    "    response = response['choices'][0]['message']['content']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use an LLM as a \"judge\" to assess the quality of each response. While this automated evaluation can provide useful insights, it's important to complement it with human judgment to ensure the chosen response aligns with your specific goals. If all three outputs seem equally strong, your personal criteria and preferences will help make the final decision.\n",
    "\n",
    "For this demo, we’ll use Sonnet 3.5 as the judge. We’ll present the original image along with the three responses to determine which one is the most accurate and helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to invoke judge model using invoke APIs\n",
    "def invoke_judge_model(model_id: str, prompt: str, display_usage=False):\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"temperature\": 0.7,\n",
    "             \"top_p\": 0.7,\n",
    "            \"top_k\": 50\n",
    "        }\n",
    "\n",
    "    body = json.dumps(payload)\n",
    "    accept = 'application/json'\n",
    "    contentType = 'application/json'\n",
    "    response = bedrock_runtime.invoke_model(body=body,\n",
    "                                            modelId=model_id,\n",
    "                                            accept=accept,\n",
    "                                            contentType=contentType)\n",
    "    \n",
    "    response = json.loads(response.get('body').read())\n",
    "\n",
    "    #response = response['choices'][0]['message']['content']\n",
    "    return response['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(task:str, mistral7b_response:str, mixtral8x7b_response:str, nemo_response:str):\n",
    "\n",
    "    evaluation_prompt = f\"\"\"\n",
    "        You are evaluating output of 3 models for a given task. Please evaluate which model produced the best output and explain why.\n",
    "        \n",
    "        Task: {task}\n",
    "        \n",
    "        Model A (Mistral 7b): {mistral7b_response}\n",
    "        \n",
    "        Model B (Mixtral 8x7b): {mixtral8x7b_response}\n",
    "\n",
    "        Model C (Mistral NeMo): {nemo_response}\n",
    "        \n",
    "        Which model provided the best output? Please explain your reasoning and declare a winner.\"\"\"\n",
    "\n",
    "    judge_response = invoke_judge_model(\n",
    "        model_id=JUDGE_MODEL_ID,\n",
    "        prompt=evaluation_prompt\n",
    "    )\n",
    "    print(f\"{RED}Judge's Evaluation:{RESET}\")\n",
    "    print(f\"{GREEN}{judge_response}{RESET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparitive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text summarization is a common use case for large language models, and Mistral models have performed well in this area. They are effective at condensing long texts into clear and concise summaries, making them valuable for tasks that require quick and accurate information extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196m#### Mistral 7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Machine learning (ML) has become mainstream due to the abundance of data and scalable cloud resources. AWS customers utilize ML for various applications such as call center operations, recommendations, fraud detection, content moderation, analysis, design services, and identity verification across industries like healthcare, industrial manufacturing, financial services, media, and telecom. AWS is dedicated to creating fair and accurate AI/ML services and providing resources for responsible use.\n",
      "\n",
      "This whitepaper offers best practices for designing and operating ML workloads, applicable to both cloud-agnostic and AWS environments. The AWS Well-Architected Framework helps understand the benefits and risks of decisions made while building workloads on AWS, ensuring operational and architectural best practices.\n",
      "\n",
      "ML models rely on high-quality input data for accurate results, necessitating continuous monitoring and potential retraining as data changes. The ML Lens complements the Well-Architected Framework by addressing the differences between application and ML workloads.\n",
      "\n",
      "This paper is targeted towards technology professionals, including CTOs, architects, developers, data scientists, and ML engineers, to provide them with the knowledge needed to effectively design and operate ML workloads on AWS.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mixtral 8x7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Machine learning (ML) has become mainstream, powered by increased data sources and cloud-based compute resources. AWS' customers use ML for various applications across industries like healthcare, manufacturing, and finance. AWS is committed to developing responsible AI and ML services, providing customers with tools and guidance for building ML applications ethically.\n",
      "\n",
      "A new whitepaper offers best practices for designing ML workloads, both during development and for continuous improvement in production. The AWS Well-Architected Framework helps users understand the benefits and risks of decisions made while building workloads on AWS, promoting best practices for design and operation.\n",
      "\n",
      "High-quality input data is crucial for accurate ML model results. Monitoring is necessary to detect, correct, and mitigate issues with accuracy and performance over time, potentially requiring model retraining with refined data. The ML Lens, built upon the Well-Architected Framework, addresses the differences between traditional application workloads and ML workloads.\n",
      "\n",
      "This paper is aimed at technology professionals, including CTOs, architects, developers, data scientists, and ML engineers, helping them understand best practices and strategies for designing and operating ML workloads on AWS.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mistral NeMo Response:\u001b[0m\n",
      "\u001b[38;5;43m**Summary:**\n",
      "\n",
      "- **Machine Learning (ML) Adoption & Applications:** ML has moved to the mainstream, driven by abundant data and scalable cloud resources. AWS customers use it for various applications like call center operations, fraud detection, content moderation, and more, across industries such as healthcare, finance, and media.\n",
      "\n",
      "- **Responsible ML:** AWS emphasizes responsible ML use, offering tools and guidance to build fair and accurate AI/ML applications. They provide resources on 'Responsible AI' for more information.\n",
      "\n",
      "- **Best Practices & Framework:** This whitepaper offers proven best practices for designing and operating ML workloads on AWS. The AWS Well-Architected Framework helps understand benefits, risks, and best practices for cloud workloads. ML-specific considerations include data quality monitoring and iterative learning cycles.\n",
      "\n",
      "- **Intended Audience:** This paper is for technology roles like CTOs, architects, developers, data scientists, and ML engineers, aiming to help them design and operate ML workloads on AWS effectively.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = ''''You are a technical writer. Summarize the following technical content:\n",
    "\n",
    "\n",
    "In recent years, machine learning (ML) has moved from research and development to the mainstream, driven by the increasing number of data sources and scalable cloud-based compute resources. AWS’ customers currently use AI/ML for a wide variety of applications such as call center operations, personalized recommendations, identifying fraudulent activities, social media content moderation, audio and video content analysis, product design services, and identity verification. Industries using AI/ML include healthcare and life sciences, industrial and manufacturing, financial services, media and entertainment, and telecom.\n",
    "\n",
    "Machine learning, through its use of algorithms to find patterns in data, can bring considerable power to its customers and thus recommends responsibility in its use. AWS is committed to developing fair and accurate AI and ML services and providing you with the tools and guidance needed to build AI and ML applications responsibly. For more information on this important topic, refer to AWS' Responsible AI.\n",
    "\n",
    "This whitepaper provides you with a set of proven best practices. You can apply this guidance and architectural principles when designing your ML workloads, and after your workloads have entered production as part of continuous improvement. Although the guidance is cloud- and technology-agnostic, the paper also includes guidance and resources to help you implement these best practices on AWS.\n",
    "\n",
    "The AWS Well-Architected Framework helps you understand the benefits and risks of decisions you make while building workloads on AWS. By using the Framework, you learn operational and architectural best practices for designing and operating workloads in the cloud. It provides a way to consistently measure your operations and architectures against best practices and identify areas for improvement.\n",
    "\n",
    "Your ML models depend on the quality of input data to generate accurate results. As data changes with time, monitoring is required to continually detect, correct, and mitigate issues with accuracy and performance. This monitoring step might require you to retrain your model over time using the latest refined data.\n",
    "\n",
    "While application workloads rely on step-by-step instructions to solve a problem, ML workloads enable algorithms to learn from data through an iterative and continuous cycle. The ML Lens complements and builds upon the Well-Architected Framework to address this difference between these two types of workloads.\n",
    "\n",
    "This paper is intended for those in a technology role, such as chief technology officers (CTOs), architects, developers, data scientists, and ML engineers. After reading this paper, you will understand the best practices and strategies to use when you design and operate ML workloads on AWS.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "mistral7b_output = invoke_model(MISTRAL_7B_MODEL_ID, prompt)\n",
    "mixtral8x7b_output = invoke_model(MIXTRAL_8x7B_MODEL_ID, prompt)\n",
    "nemo_output = invoke_model_with_message_format(MISTRAL_NEMO_MODEL_ID, prompt)\n",
    "\n",
    "print(f\"{RED}#### Mistral 7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mistral7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mixtral 8x7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mixtral8x7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mistral NeMo Response:{RESET}\")\n",
    "print(f\"{BLUE}{nemo_output}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196mJudge's Evaluation:\u001b[0m\n",
      "\u001b[38;5;29mLet me analyze each model's output based on key criteria:\n",
      "\n",
      "1. Completeness of Information:\n",
      "- Model A covers all major points but is more condensed\n",
      "- Model B misses some specific details about applications\n",
      "- Model C uses bullet points and headers, making key information easily digestible and covers all major themes\n",
      "\n",
      "2. Organization:\n",
      "- Model A presents information in traditional paragraph form, flowing logically\n",
      "- Model B follows a similar paragraph structure but is less detailed\n",
      "- Model C uses clear headers and bullet points, making it more scannable and organized\n",
      "\n",
      "3. Technical Accuracy:\n",
      "All three models maintain technical accuracy, but Model C's structured format helps prevent any confusion between concepts\n",
      "\n",
      "4. Clarity and Readability:\n",
      "- Model A is clear but dense\n",
      "- Model B is concise but might be too brief\n",
      "- Model C's bullet-point format with headers makes it easiest to read and reference\n",
      "\n",
      "WINNER: Model C (Mistral NeMo)\n",
      "\n",
      "Reasoning:\n",
      "Model C provides the best output because:\n",
      "1. It uses an effective technical writing structure with clear headers and bullet points, which is particularly appropriate for technical documentation\n",
      "2. It maintains all key information while making it more accessible\n",
      "3. The hierarchical organization helps readers quickly find specific information\n",
      "4. The bullet-point format aligns well with technical writing best practices\n",
      "5. It effectively summarizes complex information without losing important details\n",
      "\n",
      "The structured format of Model C makes it the most useful for technical readers who need to quickly grasp and reference the information, which was the core requirement of the task as a technical writer.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate_responses(task=prompt,\n",
    "                    mistral7b_response=mistral7b_output,\n",
    "                    mixtral8x7b_response=mixtral8x7b_output,\n",
    "                    nemo_response=nemo_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral LLMs are increasingly capable of generating high-quality code in various programming languages based on user prompts. These models can assist with tasks ranging from writing simple functions to generating complex algorithms, saving time and reducing errors. By understanding context and logic, these LLMs can also help debug code, suggest optimizations, and provide documentation, making them valuable tools for developers. In this section, we test code generation capabilities of Mistral models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196m#### Mistral 7b Response:\u001b[0m\n",
      "\u001b[38;5;43m I'd be happy to help you create a simple React component for calculating Body Mass Index (BMI). Here's an example of how you might implement this:\n",
      "\n",
      "```javascript\n",
      "import React, { useState } from 'react';\n",
      "\n",
      "const BmiCalculator = () => {\n",
      "  const [weight, setWeight] = useState('');\n",
      "  const [height, setHeight] = useState('');\n",
      "  const [bmi, setBmi] = useState(0);\n",
      "\n",
      "  const calculateBmi = () => {\n",
      "    if (weight && height) {\n",
      "      const calculatedBmi = parseFloat(weight) / (parseFloat(height) * parseFloat(height)) * 7030;\n",
      "      setBmi(calculatedBmi.toFixed(1));\n",
      "    }\n",
      "  };\n",
      "\n",
      "  return (\n",
      "    <div>\n",
      "      <h1>Body Mass Index (BMI) Calculator</h1>\n",
      "      <label htmlFor=\"weight\">Weight (kg):</label>\n",
      "      <input\n",
      "        type=\"number\"\n",
      "        id=\"weight\"\n",
      "        value={weight}\n",
      "        onChange={(e) => setWeight(e.target.value)}\n",
      "      />\n",
      "      <label htmlFor=\"height\">Height (cm):</label>\n",
      "      <input\n",
      "        type=\"number\"\n",
      "        id=\"height\"\n",
      "        value={height}\n",
      "        onChange={(e) => setHeight(e.target.value)}\n",
      "      />\n",
      "      <button onClick={calculateBmi}>Calculate BMI</button>\n",
      "      {bmi > 0 && <p>Your BMI is: {bmi}</p>}\n",
      "    </div>\n",
      "  );\n",
      "};\n",
      "\n",
      "export default BmiCalculator;\n",
      "```\n",
      "\n",
      "This component uses React hooks to manage component state. It initializes state for the user's weight, height, and calculated BMI. When the user clicks the \"Calculate BMI\" button, the `calculateBmi` function is called, which calculates the BMI based on the user's input and updates the component state with the result. The component then displays the calculated BMI if it's available.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mixtral 8x7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Sure, here's an example of a React component that calculates body mass index (BMI):\n",
      "```jsx\n",
      "import React, { useState } from 'react';\n",
      "\n",
      "const BmiCalculator = () => {\n",
      "  const [weight, setWeight] = useState(0);\n",
      "  const [height, setHeight] = useState(0);\n",
      "  const [bmi, setBmi] = useState(0);\n",
      "\n",
      "  const calculateBmi = () => {\n",
      "    const bmiValue = weight / ((height / 100) ** 2);\n",
      "    setBmi(bmiValue.toFixed(2));\n",
      "  };\n",
      "\n",
      "  return (\n",
      "    <div>\n",
      "      <h2>Body Mass Index Calculator</h2>\n",
      "      <label>\n",
      "        Weight (kg):\n",
      "        <input\n",
      "          type=\"number\"\n",
      "          value={weight}\n",
      "          onChange={(e) => setWeight(e.target.value)}\n",
      "        />\n",
      "      </label>\n",
      "      <label>\n",
      "        Height (cm):\n",
      "        <input\n",
      "          type=\"number\"\n",
      "          value={height}\n",
      "          onChange={(e) => setHeight(e.target.value)}\n",
      "        />\n",
      "      </label>\n",
      "      <button onClick={calculateBmi}>Calculate BMI</button>\n",
      "      {bmi > 0 && (\n",
      "        <p>\n",
      "          Your BMI is: <strong>{bmi}</strong>\n",
      "        </p>\n",
      "      )}\n",
      "    </div>\n",
      "  );\n",
      "};\n",
      "\n",
      "export default BmiCalculator;\n",
      "```\n",
      "This component uses React's `useState` hook to manage the state of the weight, height, and BMI values. The `calculateBmi` function is called when the user clicks the \"Calculate BMI\" button, which calculates the BMI value based on the entered weight and height and sets the `bmi` state to the calculated value. The BMI value is then displayed in a `p` element.\n",
      "\n",
      "Note that the height is converted to meters by dividing it by 100 before calculating the BMI value. Also, the BMI value is displayed with two decimal places using the `toFixed` method.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mistral NeMo Response:\u001b[0m\n",
      "\u001b[38;5;43mHere's a simple React component that calculates Body Mass Index (BMI) using the formula: `BMI = weight(kg) / height(m)^2`. It includes input fields for weight and height, and displays the calculated BMI along with a category (Underweight, Normal weight, Overweight, Obese).\n",
      "\n",
      "```jsx\n",
      "import React, { useState } from 'react';\n",
      "\n",
      "const BMIComponent = () => {\n",
      "  const [weight, setWeight] = useState(0);\n",
      "  const [height, setHeight] = useState(0);\n",
      "  const [bmi, setBMI] = useState(null);\n",
      "  const [category, setCategory] = useState('');\n",
      "\n",
      "  const calculateBMI = (e) => {\n",
      "    e.preventDefault();\n",
      "    if (weight > 0 && height > 0) {\n",
      "      const bmiValue = weight / Math.pow(height / 100, 2);\n",
      "      setBMI(bmiValue.toFixed(2));\n",
      "\n",
      "      if (bmiValue < 18.5) {\n",
      "        setCategory('Underweight');\n",
      "      } else if (bmiValue >= 18.5 && bmiValue < 24.9) {\n",
      "        setCategory('Normal weight');\n",
      "      } else if (bmiValue >= 25 && bmiValue < 29.9) {\n",
      "        setCategory('Overweight');\n",
      "      } else {\n",
      "        setCategory('Obese');\n",
      "      }\n",
      "    } else {\n",
      "      setBMI(null);\n",
      "      setCategory('');\n",
      "    }\n",
      "  };\n",
      "\n",
      "  return (\n",
      "    <div>\n",
      "      <h2>BMI Calculator</h2>\n",
      "      <form onSubmit={calculateBMI}>\n",
      "        <label>\n",
      "          Weight (kg):\n",
      "          <input type=\"number\" step=\"0.1\" value={weight} onChange={(e) => setWeight(parseFloat(e.target.value))} />\n",
      "        </label>\n",
      "        <br />\n",
      "        <label>\n",
      "          Height (cm):\n",
      "          <input type=\"number\" value={height} onChange={(e) => setHeight(parseFloat(e.target.value))} />\n",
      "        </label>\n",
      "        <br />\n",
      "        <button type=\"submit\">Calculate BMI</button>\n",
      "      </form>\n",
      "      {bmi && (\n",
      "        <div>\n",
      "          <h3>Your BMI is: {bmi}</h3>\n",
      "          <p>Category: {category}</p>\n",
      "        </div>\n",
      "      )}\n",
      "    </div>\n",
      "  );\n",
      "};\n",
      "\n",
      "export default BMIComponent;\n",
      "```\n",
      "\n",
      "You can use this component in your application like this:\n",
      "\n",
      "```jsx\n",
      "import React from 'react';\n",
      "import BMIComponent from './BMIComponent';\n",
      "\n",
      "function App() {\n",
      "  return (\n",
      "    <div className=\"App\">\n",
      "      <BMIComponent />\n",
      "    </div>\n",
      "  );\n",
      "}\n",
      "\n",
      "export default App;\n",
      "```\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "You are a software engineer with expertise on React.\n",
    "\n",
    "Create a React component that calculates body mass index.\n",
    "'''\n",
    "\n",
    "mistral7b_output = invoke_model(MISTRAL_7B_MODEL_ID, prompt)\n",
    "mixtral8x7b_output = invoke_model(MIXTRAL_8x7B_MODEL_ID, prompt)\n",
    "nemo_output = invoke_model_with_message_format(MISTRAL_NEMO_MODEL_ID, prompt)\n",
    "\n",
    "print(f\"{RED}#### Mistral 7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mistral7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mixtral 8x7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mixtral8x7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mistral NeMo Response:{RESET}\")\n",
    "print(f\"{BLUE}{nemo_output}{RESET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196mJudge's Evaluation:\u001b[0m\n",
      "\u001b[38;5;29mLet me analyze each model's output based on several key criteria:\n",
      "\n",
      "1. Functionality:\n",
      "- Model A: Basic BMI calculation with input validation\n",
      "- Model B: Basic BMI calculation with proper height conversion\n",
      "- Model C: Advanced BMI calculation with categorization and proper form handling\n",
      "\n",
      "2. Code Quality:\n",
      "- Model A: Has a calculation error (multiplies by 7030)\n",
      "- Model B: Clean code with correct BMI formula\n",
      "- Model C: Most comprehensive with proper error handling and form submission\n",
      "\n",
      "3. User Experience:\n",
      "- Model A: Basic input and output\n",
      "- Model B: Basic input and output with better formatting\n",
      "- Model C: Enhanced UX with:\n",
      "  - Form submission\n",
      "  - BMI categories\n",
      "  - Step attribute for weight input\n",
      "  - Input validation\n",
      "  - Clear feedback\n",
      "\n",
      "4. Technical Implementation:\n",
      "- Model C stands out with:\n",
      "  - Form handling with preventDefault()\n",
      "  - Proper type conversion with parseFloat\n",
      "  - Input validation\n",
      "  - Comprehensive state management\n",
      "  - BMI categorization logic\n",
      "  - Step attribute for more precise weight input\n",
      "\n",
      "Winner: Model C (Mistral NeMo)\n",
      "\n",
      "Reasoning:\n",
      "1. It provides the most complete solution with BMI categorization\n",
      "2. Has better error handling and input validation\n",
      "3. Uses proper form submission instead of just a button click\n",
      "4. Includes helpful features like step attributes for precise input\n",
      "5. Provides meaningful feedback through categories\n",
      "6. Has the most professional and production-ready code structure\n",
      "\n",
      "While Models A and B provide working solutions, Model C goes above and beyond by creating a more robust, user-friendly, and feature-complete component that would require minimal modification to use in a production environment.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate_responses(task=prompt,\n",
    "                    mistral7b_response=mistral7b_output,\n",
    "                    mixtral8x7b_response=mixtral8x7b_output,\n",
    "                    nemo_response=nemo_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Drafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral LLMs are highly effective at drafting professional emails, offering users a quick way to compose clear and concise messages. They can adapt to various tones—whether formal, casual, or persuasive—ensuring the right communication style for any situation. Additionally, these LLMs can help structure emails, suggest relevant content, and even proofread for grammar and clarity, making them invaluable tools for efficient communication. In this section, we evaluate email drafting capabilities of Mistral LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196m#### Mistral 7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Subject: Regrettably, Your Application for the Offer Did Not Meet the Requirements\n",
      "\n",
      "Dear Valued Customer,\n",
      "\n",
      "I hope this message finds you well. I regret to inform you that based on the current eligibility criteria, your application for the offer did not meet the necessary qualifications.\n",
      "\n",
      "We appreciate your interest in our program and value your continued support. If you have any questions or concerns regarding this decision, please do not hesitate to contact us. Our team is always here to help and provide clarification.\n",
      "\n",
      "Thank you for choosing us as your preferred service provider. We look forward to the opportunity to serve you in the future.\n",
      "\n",
      "Best regards,\n",
      "Your Support Team\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mixtral 8x7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Dear [Customer],\n",
      "\n",
      "Thank you for your interest in our offer. After reviewing your information, I regret to inform you that you do not meet the qualifications for this promotion.\n",
      "\n",
      "We appreciate your understanding and hope that you will consider our future offers. If you have any questions or concerns, please don't hesitate to reach out to us.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "Customer Support Team\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mistral NeMo Response:\u001b[0m\n",
      "\u001b[38;5;43mDear [Customer's Name],\n",
      "\n",
      "I hope this message finds you well. I am writing to inform you that, upon review of your eligibility, you do not qualify for the current promotional offer.\n",
      "\n",
      "We understand that this may be disappointing, and we apologize for any inconvenience this may cause. If you have any questions or need further clarification, please do not hesitate to ask. We are here to help.\n",
      "\n",
      "Thank you for your understanding and for choosing [Your Company's Name].\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "[Your Position]\n",
      "[Your Contact Information]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "You are a skilled customer support agent known for your polite and helpful communication. Your role involves revising poorly written emails to ensure they are clear, professional, and appropriately toned.\n",
    "\n",
    "\n",
    "hi, this is to inform u that you that u don't qualify for this offer.\n",
    "\n",
    "rgds,\n",
    "support team\n",
    "'''\n",
    "\n",
    "mistral7b_output = invoke_model(MISTRAL_7B_MODEL_ID, prompt)\n",
    "mixtral8x7b_output = invoke_model(MIXTRAL_8x7B_MODEL_ID, prompt)\n",
    "nemo_output = invoke_model_with_message_format(MISTRAL_NEMO_MODEL_ID, prompt)\n",
    "\n",
    "print(f\"{RED}#### Mistral 7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mistral7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mixtral 8x7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mixtral8x7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mistral NeMo Response:{RESET}\")\n",
    "print(f\"{BLUE}{nemo_output}{RESET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196mJudge's Evaluation:\u001b[0m\n",
      "\u001b[38;5;29mLet me analyze each response based on key criteria for professional customer service communication:\n",
      "\n",
      "1. Professionalism & Structure\n",
      "2. Clarity of message\n",
      "3. Tone & Empathy\n",
      "4. Completeness\n",
      "\n",
      "Model A (Mistral 7b):\n",
      "+ Very professional and well-structured\n",
      "+ Comprehensive with clear opening and closing\n",
      "+ Shows empathy and maintains positive tone\n",
      "+ Includes subject line\n",
      "+ Offers clear next steps\n",
      "+ Maintains relationship-building elements\n",
      "\n",
      "Model B (Mixtral 8x7b):\n",
      "+ Clear and concise\n",
      "+ Professional tone\n",
      "+ Includes next steps\n",
      "+ Somewhat brief but complete\n",
      "- Less empathetic compared to A and C\n",
      "\n",
      "Model C (Mistral NeMo):\n",
      "+ Professional and well-structured\n",
      "+ Shows empathy\n",
      "+ Includes placeholder for company details\n",
      "+ Good balance of information\n",
      "- Missing subject line\n",
      "\n",
      "Winner: Model A (Mistral 7b)\n",
      "\n",
      "Reasoning:\n",
      "Model A provides the most complete and professional response while maintaining the best balance of all desired elements. It stands out for:\n",
      "1. Including a clear subject line\n",
      "2. Having the most polished structure\n",
      "3. Striking an excellent balance between professionalism and empathy\n",
      "4. Providing comprehensive information without being overly verbose\n",
      "5. Including relationship-building elements that maintain customer goodwill\n",
      "\n",
      "While Models B and C are also professional and adequate, Model A goes the extra mile in creating a response that not only delivers the negative news but also maintains a positive customer relationship and provides the most complete communication package.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate_responses(task=prompt,\n",
    "                    mistral7b_response=mistral7b_output,\n",
    "                    mixtral8x7b_response=mixtral8x7b_output,\n",
    "                    nemo_response=nemo_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral LLMs excel at sentiment analysis by identifying the tone and emotions expressed in text, whether positive, negative, or neutral. They can analyze customer reviews, social media posts, or any form of written content to gauge sentiment, providing valuable insights for businesses and researchers. With their ability to understand context and nuance, these LLMs offer a powerful tool for monitoring brand perception and customer feedback. In this section, we evaluate sentiment analysis capabilities of Mistral models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196m#### Mistral 7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Based on the given customer review, the sentiment can be classified as Positive. The customer expressed their satisfaction with the product's build quality, performance, and ease of setup. Although they mentioned a delay in delivery, they acknowledged that the customer service team handled it well. The overall tone of the review is positive, and the customer expressed their intention to recommend the product to others.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mixtral 8x7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Based on the customer's positive comments about the product's build quality, performance, and ease of setup, as well as their overall satisfaction and recommendation, this review falls into the Positive category.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mistral NeMo Response:\u001b[0m\n",
      "\u001b[38;5;43mBased on the provided customer review, the sentiment is overwhelmingly positive. Here's why:\n",
      "\n",
      "1. The customer expresses satisfaction with the product's build quality and performance.\n",
      "2. They found the setup process easy and had no issues with the product itself.\n",
      "3. While there was a minor delay in delivery, the customer service team handled it well, which mitigates the negative impact of this issue.\n",
      "4. The customer expresses happiness with the purchase and would recommend the product to others.\n",
      "\n",
      "Therefore, the category for this customer review is: **Positive**.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = '''You are a language model trained to classify customer feedback into categories based on sentiment. \n",
    "\n",
    "Customer Review: I recently purchased this product, and I must say, I'm really impressed with it. The build quality is top-notch, and it performs exactly as advertised. \n",
    "The setup was easy, and I had no issues at all. However, I did encounter a small delay in delivery, which was a bit frustrating, but it was handled well by the customer service team. \n",
    "Overall, I'm happy with my purchase and would definitely recommend it to others. It's been a great addition to my routine!\"\n",
    "\n",
    "Task: classify customer review into one of the following categories: Positive, Negative, Neutral\n",
    "'''\n",
    "\n",
    "mistral7b_output = invoke_model(MISTRAL_7B_MODEL_ID, prompt)\n",
    "mixtral8x7b_output = invoke_model(MIXTRAL_8x7B_MODEL_ID, prompt)\n",
    "nemo_output = invoke_model_with_message_format(MISTRAL_NEMO_MODEL_ID, prompt)\n",
    "\n",
    "print(f\"{RED}#### Mistral 7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mistral7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mixtral 8x7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mixtral8x7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mistral NeMo Response:{RESET}\")\n",
    "print(f\"{BLUE}{nemo_output}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196mJudge's Evaluation:\u001b[0m\n",
      "\u001b[38;5;29mLet's analyze each model's response:\n",
      "\n",
      "Model A (Mistral 7b):\n",
      "- Provides clear classification (Positive)\n",
      "- Explains reasoning with specific examples from the review\n",
      "- Acknowledges the negative point (delivery delay) but explains why it doesn't affect overall sentiment\n",
      "- Concise yet comprehensive\n",
      "\n",
      "Model B (Mixtral 8x7b):\n",
      "- Provides classification (Positive)\n",
      "- Lists some supporting points\n",
      "- Very brief, missing some nuance\n",
      "- Doesn't address the negative aspect at all\n",
      "\n",
      "Model C (Mistral NeMo):\n",
      "- Provides clear classification (Positive)\n",
      "- Structured response with numbered points\n",
      "- Comprehensive analysis of both positive and negative aspects\n",
      "- Explains how the negative point is mitigated\n",
      "- Clear formatting with bold conclusion\n",
      "- Well-balanced and thorough explanation\n",
      "\n",
      "Winner: Model C (Mistral NeMo)\n",
      "\n",
      "Reasoning:\n",
      "1. Most structured and easy to follow response\n",
      "2. Most comprehensive analysis\n",
      "3. Balanced treatment of both positive and negative aspects\n",
      "4. Clear formatting makes the conclusion stand out\n",
      "5. Provides the most detailed justification for the classification\n",
      "\n",
      "While Model A also provided a good response, Model C's structured approach and more thorough analysis makes it the superior output for this classification task. Model B's response, while correct, was too brief and missed important nuances in the review.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate_responses(task=prompt,\n",
    "                    mistral7b_response=mistral7b_output,\n",
    "                    mixtral8x7b_response=mixtral8x7b_output,\n",
    "                    nemo_response=nemo_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196m#### Mistral 7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Negative. The customer expressed disappointment with the product's performance, perceived it as having low quality, and had a negative experience with customer service.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mixtral 8x7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Negative\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mistral NeMo Response:\u001b[0m\n",
      "\u001b[38;5;43mBased on the provided customer feedback, the review can be classified as **Negative**. Here's why:\n",
      "\n",
      "1. The customer expresses disappointment with the product (\"I’m really disappointed\").\n",
      "2. They mention that the product stopped working after a short period of use (\"It stopped working after just a few uses\"), indicating dissatisfaction with its durability.\n",
      "3. The customer feels that the product's quality is poor (\"The quality feels cheap\") and doesn't meet their expectations based on the product description and advertisement.\n",
      "4. They had a negative experience with customer service, stating that it took too long to respond and that the solution provided didn't fix the issue.\n",
      "5. The customer concludes by saying they wouldn't recommend the product to anyone, further emphasizing their negative sentiment.\n",
      "\n",
      "There are no positive aspects mentioned in the review, and while there are no explicit neutral statements, the overall tone is clearly negative. Therefore, the review can be categorized as **Negative**.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = '''You are a language model trained to classify customer feedback into categories based on sentiment. \n",
    "\n",
    "I’m really disappointed with this product. It stopped working after just a few uses, and I had high hopes based on the description. \n",
    "The quality feels cheap, and it doesn't match what was promised in the ad. \n",
    "I reached out to customer service, but they took too long to respond, and when they did, their solution didn’t fix the issue. I wouldn’t recommend this to anyone.\n",
    "\n",
    "Task: classify customer review into one of the following categories: Positive, Negative, Neutral\n",
    "'''\n",
    "\n",
    "mistral7b_output = invoke_model(MISTRAL_7B_MODEL_ID, prompt)\n",
    "mixtral8x7b_output = invoke_model(MIXTRAL_8x7B_MODEL_ID, prompt)\n",
    "nemo_output = invoke_model_with_message_format(MISTRAL_NEMO_MODEL_ID, prompt)\n",
    "\n",
    "print(f\"{RED}#### Mistral 7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mistral7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mixtral 8x7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mixtral8x7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mistral NeMo Response:{RESET}\")\n",
    "print(f\"{BLUE}{nemo_output}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196mJudge's Evaluation:\u001b[0m\n",
      "\u001b[38;5;29mLet's analyze each model's response:\n",
      "\n",
      "Model A (Mistral 7b):\n",
      "- Provides the correct classification (Negative)\n",
      "- Includes a brief but clear explanation with key points\n",
      "- Mentions three main aspects: product performance, quality, and customer service\n",
      "\n",
      "Model B (Mixtral 8x7b):\n",
      "- Provides only the classification (Negative)\n",
      "- No explanation or reasoning provided\n",
      "- Most minimal response of all three\n",
      "\n",
      "Model C (Mistral NeMo):\n",
      "- Provides the correct classification (Negative)\n",
      "- Offers the most comprehensive explanation\n",
      "- Breaks down the analysis into 5 specific points\n",
      "- Supports each point with direct quotes from the review\n",
      "- Considers alternative classifications (mentions absence of positive/neutral aspects)\n",
      "- Well-structured response with clear formatting\n",
      "\n",
      "Winner: Model C (Mistral NeMo)\n",
      "\n",
      "Reasoning:\n",
      "While all models provided the correct classification, Model C stands out as the best output because it:\n",
      "1. Provides the most thorough and well-structured analysis\n",
      "2. Uses evidence from the text to support its classification\n",
      "3. Shows consideration of all possible categories\n",
      "4. Uses formatting (bold text, bullet points) to enhance readability\n",
      "5. Demonstrates a more sophisticated understanding of the task by providing a comprehensive explanation that would be more useful for understanding the reasoning behind the classification\n",
      "\n",
      "Model A provided a decent response but was less detailed, while Model B's response was too minimal to be truly useful in a real-world application.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate_responses(task=prompt,\n",
    "                    mistral7b_response=mistral7b_output,\n",
    "                    mixtral8x7b_response=mixtral8x7b_output,\n",
    "                    nemo_response=nemo_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196m#### Mistral 7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Based on the customer feedback provided, I would classify this review as Neutral. The customer did not express any major issues or dissatisfaction with the product, but they also did not express any significant praise or excitement. The feedback suggests that the product is functional and meets expectations, but does not stand out from similar products in the market.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mixtral 8x7b Response:\u001b[0m\n",
      "\u001b[38;5;43m Neutral\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[38;5;196m#### Mistral NeMo Response:\u001b[0m\n",
      "\u001b[38;5;43mBased on the provided customer feedback, I would classify it as \"Neutral\". Here's why:\n",
      "\n",
      "- The customer states that the product \"works as expected\" and the setup was \"straightforward\", which are neutral statements and not particularly positive or negative.\n",
      "- They also mention that they \"haven't had any major issues\", which is a positive aspect, but it's presented in a neutral manner.\n",
      "- However, the customer expresses that they \"haven't been blown away by its performance\" and that they might look for alternatives next time, which leans slightly towards a negative sentiment, but it's not strongly negative.\n",
      "- Overall, the feedback doesn't express strong positive or negative feelings, hence it's neutral.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = '''You are a language model trained to classify customer feedback into categories based on sentiment. \n",
    "\n",
    "The product works as expected, but it’s nothing special. The setup was straightforward, and it functions like most similar products out there. \n",
    "I haven’t had any major issues, but I also haven’t been blown away by its performance. It’s okay, but I might look for alternatives next time.\n",
    "\n",
    "Task: classify customer review into one of the following categories: Positive, Negative, Neutral\n",
    "'''\n",
    "\n",
    "mistral7b_output = invoke_model(MISTRAL_7B_MODEL_ID, prompt)\n",
    "mixtral8x7b_output = invoke_model(MIXTRAL_8x7B_MODEL_ID, prompt)\n",
    "nemo_output = invoke_model_with_message_format(MISTRAL_NEMO_MODEL_ID, prompt)\n",
    "\n",
    "print(f\"{RED}#### Mistral 7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mistral7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mixtral 8x7b Response:{RESET}\")\n",
    "print(f\"{BLUE}{mixtral8x7b_output}{RESET}\\n\\n\")\n",
    "\n",
    "print(f\"{RED}#### Mistral NeMo Response:{RESET}\")\n",
    "print(f\"{BLUE}{nemo_output}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;196mJudge's Evaluation:\u001b[0m\n",
      "\u001b[38;5;29mLet me analyze each model's response:\n",
      "\n",
      "Model A (Mistral 7b):\n",
      "- Provides clear classification as Neutral\n",
      "- Explains reasoning with specific references to the review\n",
      "- Balanced analysis of both functional aspects and lack of standout features\n",
      "\n",
      "Model B (Mixtral 8x7b):\n",
      "- Provides only the classification \"Neutral\"\n",
      "- No explanation or reasoning provided\n",
      "- Too minimal to be helpful\n",
      "\n",
      "Model C (Mistral NeMo):\n",
      "- Provides clear classification as Neutral\n",
      "- Offers detailed explanation with bullet points\n",
      "- Breaks down specific phrases from the review\n",
      "- Shows balanced analysis of positive, negative, and neutral aspects\n",
      "- Provides clear conclusion based on the evidence presented\n",
      "\n",
      "Winner: Model C (Mistral NeMo)\n",
      "\n",
      "Reasoning:\n",
      "While Model A provides a good response with clear reasoning, Model C offers the most comprehensive and well-structured analysis. The bullet-point format makes it easier to follow the reasoning, and it specifically addresses multiple aspects of the review by quoting relevant phrases. Model C also demonstrates better analytical depth by noting how certain positive statements are presented in a neutral manner, and how slightly negative sentiments aren't strong enough to push the overall classification into the negative category.\n",
      "\n",
      "Model B's response, while correct, is too minimal to be useful in a real-world application where understanding the reasoning behind the classification is important.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate_responses(task=prompt,\n",
    "                    mistral7b_response=mistral7b_output,\n",
    "                    mixtral8x7b_response=mixtral8x7b_output,\n",
    "                    nemo_response=nemo_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Mistral NeMo consistently delivers the most comprehensive and structured responses when compared to its predecessors. Unlike earlier models, NeMo's outputs are well-organized, providing clear reasoning and a logical flow of information. It excels in tasks like summarization, code generation, and sentiment analysis, often outperforming both Mistral 7b and Mixtral 8x7b in these areas. For summarization, NeMo demonstrates a stronger ability to condense long texts while retaining key details and clarity. In code generation, it provides precise and efficient solutions, showcasing a deep understanding of programming concepts. NeMo also excels in sentiment analysis, accurately gauging the tone and emotions in text, making it highly effective for customer feedback analysis and market research. However, Mistral 7b remains superior when it comes to email drafting tasks. It provides more contextually appropriate and professionally toned responses, making it a better fit for generating emails in various business scenarios. Overall, NeMo stands out in many advanced use cases, while Mistral 7b still holds an edge in specific areas like email drafting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client.delete_marketplace_model_endpoint(endpointArn=endpoint_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
