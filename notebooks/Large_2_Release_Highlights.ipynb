{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03f6b97-8960-4ef8-9f8b-4ba430c5640c",
   "metadata": {},
   "source": [
    "# Introducing Mistral Large 2 (24.07)\n",
    "\n",
    "[Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) is an advanced large language model with state-of-the-art reasoning, knowledge, and coding capabilities according to Mistral. Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node. A significant effort was also devoted by Mistral to enhancing the model's reasoning capabilities. One of Mistral's key focuses during training was to minimize the model's tendency to \"hallucinate,\" or generate plausible-sounding but factually incorrect or irrelevant information. This was achieved by fine-tuning the model to be more cautious and discerning in its responses, ensuring that it provides reliable and accurate outputs. Additionally, the new Mistral Large is trained to acknowledge when it cannot find solutions or does not have sufficient information to provide a confident answer. Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models.\n",
    "\n",
    "The model is currently generally available for use on [Amazon Bedrock](https://aws.amazon.com/bedrock/), check out the [blog](https://aws.amazon.com/blogs/machine-learning/mistral-large-2-is-now-available-in-amazon-bedrock/) post for more info.\n",
    "\n",
    "In this notebook we will be demonstrating a number of different features like multilingual capabilities (optimized for character-based languages), function calling and tool use, json output, math and reasoning, and code generation with Bedrock's [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html).\n",
    "\n",
    "--- \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>IMPORTANT:</b>  Mistral Large 2 is being released under the <a href=\"https://mistral.ai/licenses/MRL-0.1.md\"> Mistral Research License</a>, that allows usage and modification for research and non-commercial usages. If you need to make a commercial use of Mistral Large 2, you can purchase it as-a-Service on Bedrock or buy a license via SageMaker Jumpstart\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE ON BENCHMARKS:</b> The below section goes over Benchmarks from Mistral published in their blog post for <a href=\"https://mistral.ai/news/mistral-large-2407/\"> Mistral Large 2</a>. \n",
    "</div>\n",
    "\n",
    "### Multi-lingual by design\n",
    "\n",
    "Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Arabic, Portuguese, Dutch and Polish among others. In particular, it excels in English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi. Below are the performance results of Mistral Large 2 on the multilingual MMLU benchmark from Mistral AI's official blog post on [Mistral Large 2](https://mistral.ai/news/mistral-large-2407/)\n",
    "\n",
    "#### [Performance on Multilingual MMLU (measured on the base pretrained model) --benchmarked by Mistral AI](https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-language-diversity.png)\n",
    "\n",
    "![multilingual-mmlu](imgs/mistral-large-2407-language-diversity.webp)\n",
    "\n",
    "\n",
    "---\n",
    "### Proficient in Code and Reasoning\n",
    "\n",
    "Trained on 80+ coding languages such as Python, Java, C, C++, JavaScript, Bash, Swift, Fortran. Following their experience with Codestral 22B and Codestral Mamba, Mistral has trained Mistral Large 2 on a very large proportion of code. It vastly outperforms the previous Mistral Large. \n",
    "\n",
    "#### [Performance accuracy on code generation benchmarks (all models were benchmarked through the same evaluation pipeline) --benchmarked by Mistral AI](https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-code-generation.png)\n",
    "\n",
    "![code-gen](imgs/mistral-large-2407-code-generation.png)\n",
    "\n",
    "\n",
    "#### [Performance accuracy on MultiPL-E (all models were benchmarked through the same evaluation pipeline, except for the \"paper\" row) --benchmarked by Mistral AI](https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-multiple.png)\n",
    "\n",
    "![code-multiple](imgs/mistral-large-2407-multiple.webp)\n",
    "\n",
    "---\n",
    "### [Agent-centric](https://mistral.ai/images/news/mistral-large-2407/mistral-large-2407-tool-use.png)\n",
    "\n",
    "Best-in-class agentic capabilities with native function calling and JSON outputting. Mistral Large 2 is equipped with enhanced function calling and retrieval skills and has undergone training to proficiently execute both parallel and sequential function calls, enabling it to serve as the power engine of complex business applications.\n",
    "\n",
    "---\n",
    "## Model Card\n",
    "\n",
    "<b>Available regions: US-West-2</b>\n",
    "\n",
    "<b>Model ID: mistral.mistral-large-2407-v1:0</b>\n",
    "\n",
    "<b>Context Window : 128k</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889b428-f220-4aa1-866d-43910e930bb2",
   "metadata": {},
   "source": [
    "Let's get started with trying out the model on Bedrock!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366393c-b0c6-4ee9-9ef1-9d74da0a8b6c",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee83c4b-a59b-4ae7-a443-d23b665e660e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import json\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd992f67-4ffc-4b01-b235-de2fcd2ee5b3",
   "metadata": {},
   "source": [
    "#### Create Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed39c4f-f50c-47fc-bd6a-4597594b5e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-west-2\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef8c55-7eab-4f86-ab3a-c60cb5db34c9",
   "metadata": {},
   "source": [
    "#### Instantiate model ids of mistral model being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987d7bb-a773-4ede-9fba-4b4b4ea230a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mistral_large_2 = 'mistral.mistral-large-2407-v1:0'\n",
    "\n",
    "model_id = mistral_large_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45307e95-7e1d-41cd-a13a-a4d81d2b5f17",
   "metadata": {},
   "source": [
    "### Generating JSON  & Tool Use\n",
    "\n",
    "Mistral 2 now offers a native JSON output mode. This feature allows developers to receive the model's responses in a structured, easy-to-read format that can be readily integrated into various applications and systems. With JSON being a widely adopted data exchange standard, this capability simplifies the process of working with the model's outputs, making it more accessible and practical for developers across different domains and use cases.\n",
    "\n",
    "To generate JSON with the Converse API you will need to define a toolSpec. Here we’ve outlined an example for a travel agent company that will take passenger information and requests and convert them to JSON.\n",
    "\n",
    "Refer to this  article for more information on Generating JSON with Converse API : [Article](https://community.aws/content/2hWA16FSt2bIzKs0Z1fgJBwu589/generating-json-with-the-amazon-bedrock-converse-api?lang=en)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cdcb2-7f7e-49d6-9db8-c2061534c6bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the tool configuration\n",
    "import json\n",
    "tool_list = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"travel_agent\",\n",
    "            \"description\": \"Converts trip details as a json structure.\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"origin_airport\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Origin airport (IATA code)\"\n",
    "                        },\n",
    "                        \"destination_airport\": {\n",
    "                            \"type\": \"boolean\",\n",
    "                            \"description\": \"Destination airport (IATA code)\"\n",
    "                        },\n",
    "                        \"departure_date\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Departure date\",\n",
    "                        }, \n",
    "                        \"return_date\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Return date\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\n",
    "                        \"origin_airport\",\n",
    "                        \"destination_airport\",\n",
    "                        \"departure_date\",\n",
    "                        \"return_date\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439f636-4660-48a4-9fda-7fc54427bd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "I would like to book a flight from New York (JFK) to London (LHR) for a round-trip.\n",
    "The departure date is June 15, 2023, and the return date is June 25, 2023.\n",
    "\n",
    "For the flight preferences, I would prefer to fly with Delta or United Airlines. \n",
    "My preferred departure time range is between 8 AM and 11 AM, and my preferred arrival time range is between 9 AM and 1 PM (local time in London). \n",
    "I am open to flights with one stop, but no more than that.\n",
    "Please include non-stop flight options if available.\n",
    "\"\"\"\n",
    "\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        { \"text\": f\"<content>{content}</content>\" },\n",
    "        { \"text\": \"Please create a well-structured JSON object representing the flight booking request, ensuring proper nesting and organization of the data. Include sample data for better understanding. Create the JSON based on the content within the <content> tags.\" }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835ccf3-1217-4cef-a477-82c288906085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bedrock client configuration\n",
    "response = bedrock_client.converse(\n",
    "    modelId=model_id,\n",
    "    messages=[message],\n",
    "    inferenceConfig={\n",
    "        \"maxTokens\": 500,\n",
    "        \"temperature\": 0.1\n",
    "    },\n",
    "    toolConfig={\n",
    "        \"tools\": tool_list\n",
    "    }\n",
    ")\n",
    "\n",
    "response_message = response['output']['message']\n",
    "response_content_blocks = response_message['content']\n",
    "content_block = next((block for block in response_content_blocks if 'toolUse' in block), None)\n",
    "tool_use_block = content_block['toolUse']\n",
    "tool_result_dict = tool_use_block['input']\n",
    "\n",
    "print(json.dumps(tool_result_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284f190-03d8-482f-96a5-570c1cb60526",
   "metadata": {
    "tags": []
   },
   "source": [
    "Mistral Large 2 was able to correctly take our user query and convert the appropriate information to JSON. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9347970c-0c17-43c3-9a34-bd623e6729af",
   "metadata": {},
   "source": [
    "\n",
    "### Converse API & Tool Use\n",
    "\n",
    "Mistral Large 2 also supports the Converse API & tool use. You can use the Amazon Bedrock API to give a model access to tools that can help it generate responses for messages that you send to the model. For example, you might have a chat application that lets users find out the most popular song played on a radio station. To answer a request for the most popular song, a model needs a tool that can query and return the song information. Here we have an example for getting the correct train schedule.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857ec47-db95-4bd0-a63a-36127e9d02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tool configuration\n",
    "toolConfig = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"shinkansen_schedule\",\n",
    "                \"description\": \"Fetches Shinkansen train schedule departure times for a specified station and time.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"station\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The station name.\"\n",
    "                            },\n",
    "                            \"departure_time\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The departure time in HH:MM format.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"station\", \"departure_time\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fa882-542c-406d-810e-72607867d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shinkansen_schedule(station, departure_time):\n",
    "    schedule = {\n",
    "        \"Tokyo\": {\"09:00\": \"Hikari\", \"12:00\": \"Nozomi\", \"15:00\": \"Kodama\"},\n",
    "        \"Osaka\": {\"10:00\": \"Nozomi\", \"13:00\": \"Hikari\", \"16:00\": \"Kodama\"}\n",
    "    }\n",
    "    return schedule.get(station, {}).get(departure_time, \"No train found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2372f-3330-4a39-884c-bf60f8b391b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_mistral(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    converse_api_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"toolConfig\": toolConfig,  \n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 400},\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    \n",
    "    if response['output']['message']['content'][0].get('toolUse'):\n",
    "        tool_use = response['output']['message']['content'][0]['toolUse']\n",
    "        tool_name = tool_use['name']\n",
    "        tool_inputs = tool_use['input']\n",
    "\n",
    "        if tool_name == \"shinkansen_schedule\":\n",
    "            print(\"Mistral wants to use the shinkansen_schedule tool\")\n",
    "            station = tool_inputs[\"station\"]\n",
    "            departure_time = tool_inputs[\"departure_time\"]\n",
    "            \n",
    "            try:\n",
    "                result = shinkansen_schedule(station, departure_time)\n",
    "                print(\"Train schedule result:\", result)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Mistral responded with:\")\n",
    "        print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2c11b-b406-4889-9edb-6194a2b1bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_mistral(\"What train departs Tokyo at 9:00?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4a156-9d5c-4988-bec2-27df931cb665",
   "metadata": {
    "tags": []
   },
   "source": [
    "Mistral Large 2 was able to correctly identify the shinkansen tool and demonstrate its use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109a915-b96c-4db7-a865-e164a7df798a",
   "metadata": {},
   "source": [
    "---\n",
    "### Multilingual \n",
    "\n",
    "Mistral Large 2 now supports a large number of character-based languages such as Chinese, Japanese, Korean, Arabic, and Hindi. This expanded language support allows developers to build applications and services that can cater to users from diverse linguistic backgrounds. With multilingual capabilities, developers can create localized user interfaces, provide language-specific content and resources, and ensure a seamless experience for users regardless of their native language. In particular, Mistral Large 2 is efficienct with character-based languages.\n",
    "\n",
    "Here, let's try to translate customer emails generated by the author into different languages such as Hindi, Arabic, Korean, & Japanese. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4727948-b8a4-4028-abe6-e14035469349",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails= \"\"\"\n",
    "\"I recently bought your RGB gaming keyboard and absolutely love the customizable lighting features! Can you guide me on how to set up different profiles for each game I play?\"\n",
    "\"I'm trying to use the macro keys on the gaming keyboard I just purchased, but they don't seem to be registering my inputs. Could you help me figure out what might be going wrong?\"\n",
    "\"I'm considering buying your gaming keyboard and I'm curious about the key switch types. What options are available and what are their main differences?\"\n",
    "\"I wanted to report a small issue where my keyboard's space bar is a bit squeaky. However, your quick-start guide was super helpful and I fixed it easily by following the lubrication tips. Just thought you might want to know!\"\n",
    "\"My new gaming keyboard stopped working within a week of purchase. None of the keys respond, and the lights don't turn on. I need a solution or a replacement as soon as possible.\"\n",
    "\"I've noticed that the letters on the keys of my gaming keyboard are starting to fade after several months of use. Is this covered by the warranty?\"\n",
    "\"I had an issue where my keyboard settings would reset every time I restarted my PC. I figured out it was due to a software conflict and resolved it by updating the firmware. Just wanted to ask if there are any new updates coming soon?\"\n",
    "\"I've been having trouble with the keyboard software not saving my configurations, and it's starting to get frustrating. What can be done to ensure my settings are saved permanently?\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412383b-079f-4b87-ad43-71becb0b0859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompts for language tasks/ translation\n",
    "\n",
    "#japanese prompt\n",
    "jpn_prompt=f\"\"\"\n",
    "\n",
    "emails={emails}\n",
    "\n",
    "Translate the following customer emails into Japanese. Your responses must be numbered, only in Japanese, and must adhere to only translating the emails.\n",
    "\n",
    "\"\"\".format(emails=emails)\n",
    "\n",
    "#korean prompt\n",
    "korean_prompt=f\"\"\"\n",
    "\n",
    "emails={emails}\n",
    "\n",
    "Translate the following customer emails into Korean. Your responses must be numbered, only in Korean, and must adhere to only translating the emails.\n",
    "\n",
    "\"\"\".format(emails=emails)\n",
    "\n",
    "#hindi prompt\n",
    "hindi_prompt=f\"\"\"\n",
    "\n",
    "emails={emails}\n",
    "\n",
    "Translate the following customer emails into Hindi. Your responses must be numbered, only in Hindi, and must adhere to only translating the emails.\n",
    "\n",
    "\"\"\".format(emails=emails)\n",
    "\n",
    "#arabic prompt\n",
    "arabic_prompt=f\"\"\"\n",
    "\n",
    "emails={emails}\n",
    "\n",
    "Translate the following customer emails into Arabic. Your responses must be numbered, only in Arabic, and must adhere to only translating the emails.\n",
    "\n",
    "\"\"\".format(emails=emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfee2cf-0312-41b2-8251-dd5162cc8e65",
   "metadata": {},
   "source": [
    "#### Mistral Large 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7273020-d76b-4e67-b1f3-11f09add565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse(prompt, inference_config):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    response = bedrock_client.converse(\n",
    "        messages=messages,\n",
    "        modelId=model_id,\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "    generated_text = response['output']['message']['content'][0]['text']\n",
    "    print(generated_text)\n",
    "    return generated_text, response\n",
    "\n",
    "inference_config = {\"temperature\": 0.0, \"maxTokens\": 4000, \"topP\": 0.1}\n",
    "\n",
    "#large 2 with hindi prompt\n",
    "generated_text, response = converse(hindi_prompt, inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75cc74-5077-40e3-a907-ce74d23387fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#arabic prompt with large 2\n",
    "generated_text, response = converse(arabic_prompt, inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78886119-ee65-4e4a-a4ce-ba2da37608e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#japanese prompt with large 2\n",
    "generated_text, response = converse(jpn_prompt, inference_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83028d47-b0f6-4fe1-a0a6-4b018dd2f6f8",
   "metadata": {},
   "source": [
    "Based on the above responses, we can see that Mistral Large 2 performs really well with a number of character-based languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910897a-9431-46f9-ae80-2601fc7d7325",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b57678-4123-4239-a1a5-89bf04f3c2e0",
   "metadata": {},
   "source": [
    "### Coding Tasks\n",
    "\n",
    "Mistral Large 2 has been trained on over 80 coding languages, including popular ones like Python, Java, C, C++, JavaScript, and Bash, as well as more specialized languages such as Swift and Fortran. This comprehensive language support empowers developers to tackle a wide range of coding tasks and projects across various domains and platforms. Whether you're working on web development, mobile applications, scientific computing, or system programming, Mistral 2 can assist you with code generation, debugging, refactoring, and other coding-related tasks.\n",
    "\n",
    "<mark><b>Sample coding tasks:</b></mark>\n",
    "\n",
    "1. \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "2. \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "3. \"Write a Rust function that implements the Aho-Corasick string matching algorithm for efficient multi-pattern searching. Optimize it for memory usage and include comprehensive error handling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282ba6a-21df-46ad-97c5-07c1f8e1b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Write a Python function called palindrome_prime_finder that finds all prime numbers within a given range that are also palindromes when written in base 10 (decimal system).\n",
    "\"\"\"\n",
    "\n",
    "inference_config = {\"temperature\": 0.0, \"maxTokens\": 1000, \"topP\": 0.1}\n",
    "\n",
    "response = converse(prompt, inference_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb62a27-09cf-421c-8d9e-e9a1c99a4912",
   "metadata": {},
   "source": [
    "---\n",
    "### Math and Reasoning Tasks\n",
    "\n",
    "Mistral Large 2 excels at Math and Reasoning tasks, let's test some out:\n",
    "\n",
    "<mark>    \n",
    "<b>Sample Questions:</b></mark>\n",
    "    \n",
    "1. \"Please provide a step-by-step reasoning process to estimate the number of stars in our galaxy, the Milky Way. Break down the calculation into logical steps, explaining your thought process and any assumptions you make along the way. Use scientific notation where appropriate, and conclude with a final estimate.\"\n",
    "2. \"What were the main reasons Frank Lloyd Wright designed his Oak Park Studio with high windows placed near the ceiling, and how did this feature reflect his architectural philosophy and the needs of an architecture firm's workspace?\"\n",
    "3. \"Escape velocity from a neutron star: Given: A neutron star has a mass of 2.5 solar masses and a radius of 12 km. Task: Calculate the escape velocity from the surface of this neutron star in km/s.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99368e37-f52e-4632-93db-f1e831629d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Calculating the orbital period of an exoplanet: Given: An exoplanet orbits its star at a distance of 2.5 AU (Astronomical Units). The star has a mass of 1.2 solar masses. Task: Calculate the orbital period of the exoplanet in Earth years.\n",
    "\"\"\"\n",
    "\n",
    "inference_config = {\"temperature\": 0.0, \"maxTokens\": 1000, \"topP\": 0.1}\n",
    "\n",
    "response = converse(prompt, inference_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47cb59-31de-4bc1-b2b5-4d35d14af28a",
   "metadata": {},
   "source": [
    "---\n",
    "### Distributors\n",
    "\n",
    "- Amazon Web Services\n",
    "- Mistral AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
