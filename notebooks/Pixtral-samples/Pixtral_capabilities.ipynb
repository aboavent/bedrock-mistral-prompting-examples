{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Depth Exploration of Pixtral's Capabilities\n",
    "\n",
    "Welcome to this comprehensive notebook where we delve into the diverse capabilities of **Pixtral**, a cutting-edge multimodal language model. In this notebook, we will examine Pixtral's performance across a variety of tasks, including:\n",
    "\n",
    "- **Optical Character Recognition (OCR)**\n",
    "- **Image Classification**\n",
    "- **Object Detection**\n",
    "- **Image Captioning**\n",
    "- **Visual Question Answering (VQA)**\n",
    "- **Handwriting Recognition**\n",
    "- **And More**\n",
    "\n",
    "The objective of this notebook is to provide you with a clear understanding of where Pixtral excels and to identify areas where it may face challenges. While these insights are based on our observations, your experiences and results may vary depending on the datasets and use cases you explore.\n",
    "\n",
    "### Pixtral 12B in Short\n",
    "\n",
    "- **Natively multimodal:** Trained with interleaved image and text data.\n",
    "- **Strong performance on multimodal tasks:** Excels in instruction following.\n",
    "- **State-of-the-art text-only benchmarks:** Maintains top performance in text-based evaluations.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- **Vision Encoder:** New 400M parameter encoder trained from scratch.\n",
    "- **Multimodal Decoder:** 12B parameter decoder based on Mistral Nemo.\n",
    "- **Flexible Image Support:** Handles variable image sizes and aspect ratios.\n",
    "- **Long Context Window:** Supports multiple images within a 128k token context.\n",
    "\n",
    "### Use\n",
    "\n",
    "- **License:** Apache 2.0\n",
    "\n",
    "Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks.\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; justify-content: center; align-items: center;\">\n",
    "  <img src=\"https://mistral.ai/images/news/pixtral-12b/pixtral-benchmarks.png\" alt=\"Benchmarks\" style=\"width: 45%; height: auto;\"/>\n",
    "  <img src=\"https://mistral.ai/images/news/pixtral-12b/pixtral-comparison.png\" alt=\"Evals\" style=\"width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "[The instructions for how to get started using this notebook can be found in the Pixtral LMI notebook](https://github.com/aws-samples/mistral-on-aws/blob/59ab4ab9736122200a2d284039cb4557782e4a20/notebooks/Pixtral-samples/Pixtral-12b-LMI-SageMaker-realtime-inference.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html\" target=\"_blank\">Amazon SageMaker Notebook Instance</a>, select Kernel \"<span style=\"color:green;\">conda_pytorch_p310</span>\".\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mistral_common[opencv] mistral_common==\"v1.4.4\" numpy==1.26.4 --force --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.djl_inference import DJLModel\n",
    "\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    "    ImageURLChunk,\n",
    ")\n",
    "\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session() # sagemaker session for interacting with different AWS APIs\n",
    "\n",
    "sagemaker_session_bucket = None # bucket to house artifacts\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role() # execution role for the endpoint\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri =f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124\" \n",
    "\n",
    "# You can also obtain the image_uri programatically as follows.\n",
    "# image_uri = image_uris.retrieve(framework=\"djl-lmi\", version=\"0.30.0\", region=\"us-west-2\")\n",
    "\n",
    "model = DJLModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"mistralai/Pixtral-12B-2409\",\n",
    "        \"HF_TOKEN\": \"<REPLACE_WITH_YOUR_HF_TOKEN>\", #since the model \"mistralai/Pixtral-12B-2409\" is gated model, you need a HF_TOKEN & go to https://huggingface.co/mistralai/Pixtral-12B-2409 to be granted access\n",
    "        \"OPTION_ENGINE\": \"Python\",\n",
    "        \"OPTION_MPI_MODE\": \"true\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"8192\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_TOKENIZER_MODE\": \"mistral\",\n",
    "        \"OPTION_ENTRYPOINT\": \"djl_python.huggingface\",\n",
    "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=4\", # this can be tuned to control how many images per prompt are allowed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "If you want to use the Pixtral 12B model with its full capabilities—including the maximum context window of 128k tokens and support for multiple images—you should look to use a ml.p5.48xlarge or ml.p4d.24xlarge instance. These instances provides the necessary GPU memory and computational power to ensure optimal performance.\n",
    "\n",
    "If you prefer a balance between performance and cost, using the Pixtral 12B model at half precision on an ml.g5.12xlarge instance is a great choice. This setup handles context windows up to 8192 tokens efficiently and supports multiple images per prompt.\n",
    "\n",
    "This is the author's back-of-the-napkin math - feel free to experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(instance_type=\"ml.g5.24xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many tokens make up an image?\n",
    "\n",
    "Pixtral tokenizes images by dividing them into small square patches of 16×16 pixels. Each patch is converted into an image token, creating a sequential representation of the entire image. To help the model understand the spatial structure and aspect ratio of the image, Pixtral inserts special [IMG BREAK] tokens after each row of patches, except after the last row. This way, the model can distinguish between different rows and better comprehend the image's layout. At the end of the sequence, an [IMG END] token is added to signify the end of the image input. This method of tokenization allows Pixtral to effectively capture the dimensions and spatial relationships within the image, enhancing its ability to interpret and reason about visual content.\n",
    "\n",
    "Example with a 512×512 Image:\n",
    "\n",
    "For instance, consider a 512×512 pixel image. When divided into 16×16 pixel patches, the image yields 32 patches along the width and 32 patches along the height, resulting in a total of 1,024 image tokens (since 32 × 32 = 1,024). Pixtral inserts [IMG BREAK] tokens after each row of patches to indicate the end of a row, adding 31 [IMG BREAK] tokens (one after each of the first 31 rows). Finally, an [IMG END] token is added at the end of the sequence to signal the end of the image input. This brings the total number of tokens for the 512×512 image to 1,056 tokens:\n",
    "\n",
    "- 1,024 image tokens\n",
    "- 31 [IMG BREAK] tokens\n",
    "- 1 [IMG END] token\n",
    "\n",
    "This example illustrates how Pixtral processes images to maintain spatial awareness, enabling the model to effectively interpret visual information embedded within the token sequence.\n",
    "\n",
    "[More information can be found in the Pixtral blogpost](https://mistral.ai/news/pixtral-12b/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Use Cases\n",
    "\n",
    "encode_image_to_data_url(image_path): Converts an image file into a base64-encoded data URL.\n",
    "\n",
    "send_images_to_model(predictor, prompt, image_paths): Sends a text prompt and images (as data URLs) to a model and returns the response text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokens(\n",
    "    messages: List[UserMessage],\n",
    "    model: str = \"pixtral\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the total number of tokens from text and images in the messages.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of UserMessage objects containing text and image chunks\n",
    "        model: Model name to use for tokenization (default: \"pixtral\")\n",
    "    \n",
    "    Returns:\n",
    "        TokenizationResult object containing token counts and statistics\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = MistralTokenizer.from_model(model)\n",
    "    \n",
    "    # Tokenize the messages\n",
    "    tokenized = tokenizer.encode_chat_completion(\n",
    "        ChatCompletionRequest(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Extract tokens\n",
    "    tokens = tokenized.tokens\n",
    "    \n",
    "    return len(tokens)\n",
    "\n",
    "def encode_image_to_data_url(image_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a local file path and encodes it to a data URL.\n",
    "    \"\"\"\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    # Determine the image MIME type (e.g., image/jpeg, image/png)\n",
    "    mime_type = Image.open(image_path).get_format_mimetype()\n",
    "    data_url = f\"data:{mime_type};base64,{base64_encoded}\"\n",
    "    return data_url\n",
    "\n",
    "def send_images_to_model(predictor, prompt, image_paths):\n",
    "    \"\"\"\n",
    "    Sends images and a prompt to the model and returns the response in plain text.\n",
    "    \"\"\"\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    \n",
    "    content_list = [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "    }]\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Encode image to data URL\n",
    "        data_url = encode_image_to_data_url(image_path)\n",
    "        \n",
    "        \n",
    "        content_list.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": data_url\n",
    "            }\n",
    "            \n",
    "        })\n",
    "        \n",
    "        messages = [\n",
    "            UserMessage(\n",
    "                content=[\n",
    "                    ImageURLChunk(image_url=data_url),\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        total_tokens = calculate_tokens(messages)\n",
    "    \n",
    "        print(f\"\\nTokens per image: {total_tokens}\")\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content_list\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    response = predictor.predict(payload)\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Extract and transcribe all text visible in the image, preserving its exact formatting, layout, and any special characters. Include line breaks and maintain the original capitalization and punctuation.\"\n",
    "image_path = \"Pixtral_data/amazon_s1_2.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legal and financial terminology was accurately recognized, which is crucial for documents like registration statements. The model effectively captured sections, such as headers and subheaders (e.g., \"### CALCULATION OF REGISTRATION FEE\"), indicating a good understanding of hierarchical text structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the attached image of an earnings report.\n",
    "\n",
    "Extract Key Data: Identify and summarize main financial metrics:\n",
    "\n",
    "Title\n",
    "\n",
    "Revenue\n",
    "Net income or loss\n",
    "Earnings per share (EPS)\n",
    "Operating expenses\n",
    "Significant one-time items or adjustments\n",
    "Diluted earnings per share\n",
    "Insights:\n",
    "\n",
    "Evaluate overall financial health based on profitability, revenue growth, or cost management.\n",
    "Note any risks or positive signals impacting future performance.\n",
    "Conclusion: Provide a brief summary of the company’s performance this quarter, highlighting potential growth areas or concerns for investors. If specific data isn't present, then leave blank.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/AMZN-Q2-2024-Earnings-Release.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Pixtral is provided with a low-resolution image, the model may hallucinate or misinterpret the image data. In this instance, Pixtral incorrectly extracted the dates from our earnings report due to the poor image quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the attached image of an earnings report.\n",
    "\n",
    "Extract Key Data: Identify and summarize main financial metrics:\n",
    "\n",
    "Title\n",
    "\n",
    "Revenue\n",
    "Net income or loss\n",
    "Earnings per share (EPS)\n",
    "Operating expenses\n",
    "Significant one-time items or adjustments\n",
    "Diluted earnings per share\n",
    "Insights:\n",
    "\n",
    "Evaluate overall financial health based on profitability, revenue growth, or cost management.\n",
    "Note any risks or positive signals impacting future performance.\n",
    "Conclusion: Provide a brief summary of the company’s performance this quarter, highlighting potential growth areas or concerns for investors. If specific data isn't present, then leave blank.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/AMZN-Q2-2024-Earning-High-Quality.png\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, when we use the same image at a higher resolution, Pixtral generates the correct completion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwriting Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Analyze the image and transcribe any handwritten text present. Convert the handwriting into a single, continuous string of text. Maintain the original spelling, punctuation, and capitalization as written. Ignore any printed text, drawings, or other non-handwritten elements in the image.\"\n",
    "image_path = \"Pixtral_data/a01-082u-01.png\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html(text):\n",
    "    pattern = r'```html\\s*(.*?)\\s*```'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "prompt = \"Create HTML and CSS code for a minimalist and futuristic portfolio website. Use the following image as template to create your own design.\"\n",
    "\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "image_path = \"Pixtral_data/portfolio-website.png\"  # Replace with your local image path\n",
    "\n",
    "Image.open(image_path).show()\n",
    "\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "html_code = extract_html(response)\n",
    "print(html_code)\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sql(text):\n",
    "    pattern = r'```sql\\s*(.*?)\\s*```'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "prompt= \"\"\"\n",
    "Generate SQL code based on the following database schema image. Please follow these steps:\n",
    "\n",
    "1. SCHEMA ANALYSIS\n",
    "- List all tables visible in the image\n",
    "- For each table, identify:\n",
    "  * Table name\n",
    "  * All columns with their data types\n",
    "  * Primary key fields (usually indicated with a key symbol or PK notation)\n",
    "  * Foreign key relationships (usually shown with connection lines between tables)\n",
    "  * Any indexes or constraints shown\n",
    "\n",
    "2. SQL GENERATION REQUIREMENTS\n",
    "Generate SQL code that:\n",
    "- Uses standard SQL syntax\n",
    "- Creates tables in the correct order (referenced tables before tables with foreign keys)\n",
    "- Implements proper primary key constraints\n",
    "- Sets up all foreign key relationships\n",
    "- Includes appropriate indexes\n",
    "- Uses consistent naming conventions\n",
    "- Includes comments explaining each major section\n",
    "\n",
    "3. FORMAT SPECIFICATIONS\n",
    "- Each CREATE TABLE statement should be clearly separated\n",
    "- Include proper indentation for readability\n",
    "- End each statement with a semicolon\n",
    "- Add relevant indexes after table creation\n",
    "- Include DROP TABLE statements (if needed) in reverse order of dependencies\n",
    "\n",
    "4. ADDITIONAL CONSIDERATIONS\n",
    "- If any field allows NULL values, specify explicitly\n",
    "- Include any visible default values\n",
    "- Add appropriate data type lengths for VARCHAR fields\n",
    "- Consider adding relevant indexes for frequently queried fields\n",
    "\n",
    "Please provide the complete SQL code that would recreate this database schema exactly as shown in the image.\n",
    "\"\"\"\n",
    "print('Input Image:\\n\\n')\n",
    "image_path = \"Pixtral_data/database-schema.png\"  # Replace with your local image path\n",
    "\n",
    "Image.open(image_path).show()\n",
    "\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "sql_code = extract_sql(response)\n",
    "print(sql_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"\"\"\n",
    "\n",
    "Analyze the attached image of the chart or graph. Your tasks are to:\n",
    "\n",
    "Identify the type of chart or graph (e.g., bar chart, line graph, pie chart, etc.).\n",
    "Extract the key data points, including labels, values, and any relevant scales or units.\n",
    "Identify and describe the main trends, patterns, or significant observations presented in the chart.\n",
    "Generate a clear and concise paragraph summarizing the extracted data and insights. The summary should highlight the most important information and provide an overview that would help someone understand the chart without seeing it.\n",
    "Ensure that your summary is well-structured, accurately reflects the data, and is written in a professional tone.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/Amazon_Chart.png\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and provide a detailed description of what you see. Include:\n",
    "\n",
    "1. The main subject or focus of the image\n",
    "2. Key elements or objects present\n",
    "3. Colors, lighting, and overall mood\n",
    "4. Spatial arrangement and composition\n",
    "5. Any text or symbols visible\n",
    "6. Actions or events taking place, if applicable\n",
    "7. Background and setting details\n",
    "8. Distinctive features or unusual aspects\n",
    "9. Estimated time of day or season, if relevant\n",
    "10. Overall context or type of scene (e.g., natural landscape, urban setting, indoor space)\n",
    "\n",
    "Describe the image as if explaining it to someone who cannot see it. Be thorough but concise, focusing on the most important and interesting aspects of the image.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/3a1SR_oZI0-dCEvLG7US5g.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and identify all distinct objects present. For each object detected:\n",
    "\n",
    "1. Name the object\n",
    "2. Specify its approximate location in the image (e.g., top-left, center, bottom-right)\n",
    "3. Estimate its size relative to the image (e.g., small, medium, large)\n",
    "4. Note any relevant characteristics (color, shape, condition)\n",
    "5. Identify if it's partially obscured or fully visible\n",
    "\n",
    "List all objects detected, including people, animals, vehicles, furniture, buildings, natural elements, and any other identifiable items. If multiple instances of the same object type are present, count and report them separately. Ignore very small or indistinct elements that can't be clearly identified. If applicable, note any obvious interactions or relationships between objects.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/dresser.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data Extraction\n",
    "\n",
    "Extracting structured data from product images is essential for efficient inventory management, e-commerce listings, and data analysis. Pixtral's multimodal capabilities enable the accurate transformation of visual information into a standardized JSON format, facilitating seamless integration with databases and applications.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The following code demonstrates how to utilize Pixtral to analyze product images and output the information in a predefined JSON structure. This ensures consistency and accuracy in capturing essential product details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a product analyst your job is to analyze the images provided and output the information in the exact JSON structure specified below. Ensure that you populate each field accurately based on the visible details in the image. If any information is not available or cannot be determined, use 'Unknown' for string fields and an empty array [] for lists.\n",
    "\n",
    "Use the format shown exactly, ensuring all fields and values align with the JSON schema requirements.\n",
    "\n",
    "Use this JSON schema:\n",
    "\n",
    "{\n",
    "  \"title\": \"string\",\n",
    "  \"description\": \"string\",\n",
    "  \"category\": {\n",
    "    \"type\": \"string\",\n",
    "    \"enum\": [\"Electronics\", \"Furniture\", \"Clothing\", \"Appliances\", \"Toys\", \"Books\", \"Tools\", \"Other\"]\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"color\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": { \"type\": \"string\" }\n",
    "    },\n",
    "    \"shape\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"Round\", \"Square\", \"Rectangular\", \"Irregular\", \"Other\"]\n",
    "    },\n",
    "    \"condition\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"New\", \"Like New\", \"Good\", \"Fair\", \"Poor\", \"Unknown\"]\n",
    "    },\n",
    "    \"material\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": { \"type\": \"string\" }\n",
    "    },\n",
    "    \"brand\": { \"type\": \"string\" }\n",
    "  },\n",
    "  \"image_quality\": {\n",
    "    \"type\": \"string\",\n",
    "    \"enum\": [\"High\", \"Medium\", \"Low\"]\n",
    "  },\n",
    "  \"background\": \"string\",\n",
    "  \"additional_features\": {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": { \"type\": \"string\" }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "image_paths = [\n",
    "    \"Pixtral_data/luggage.jpg\",\n",
    "    \"Pixtral_data/dresser.jpg\",\n",
    "    \"Pixtral_data/dog_bag.jpg\",\n",
    "]  # Replace with your actual image paths\n",
    "\n",
    "# Send to model\n",
    "response = send_images_to_model(predictor, prompt, image_paths)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Q&A\n",
    "\n",
    "Visual Question and Answering (Visual Q&A) is a powerful feature of Pixtral that allows users to interact with images through natural language queries. By enabling multi-turn conversations, Pixtral can provide detailed and contextually relevant answers based on the visual content of the images. This capability is invaluable for applications such as customer support, educational tools, and interactive data analysis.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The following code demonstrates how to utilize Pixtral's Visual Q&A functionality. Users can pass their own images or use images from the Pixtral data folder. Additionally, the max_turns parameter can be adjusted to allow for more extended conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as IPythonImage\n",
    "\n",
    "def visual_qa(predictor, image_paths, max_turns=2):\n",
    "    \"\"\"\n",
    "    Performs visual Q&A with multiple images and multi-turn conversation.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor: The SageMaker Predictor object.\n",
    "    - image_paths: A list of local image file paths.\n",
    "    - max_turns: The maximum number of conversational turns.\n",
    "\n",
    "    Returns:\n",
    "    - None. Outputs are printed directly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Display images\n",
    "    print(\"Here are the images for this conversation:\")\n",
    "    for image_path in image_paths:\n",
    "        display(IPythonImage(filename=image_path))\n",
    "        \n",
    "    # Encode images to data URLs\n",
    "    data_urls = [encode_image_to_data_url(image_path) for image_path in image_paths]\n",
    "\n",
    "    # Initialize conversation messages\n",
    "    messages = []\n",
    "\n",
    "    # Define the initial prompt within the function\n",
    "    initial_prompt = (\"You're an extremely friendly helper. Help the user answer questions about the images shown to you. \"\n",
    "                      \"If the answer isn't in the image, say 'I'm sorry, the answer is not in the provided image.'\")\n",
    "\n",
    "    # Start the conversation loop\n",
    "    for turn in range(max_turns):\n",
    "        # Get user input\n",
    "        user_question = input(\"\\nYou: \")\n",
    "        if user_question.strip() == '':\n",
    "            print(\"Please enter a question.\")\n",
    "            continue\n",
    "\n",
    "        # Build user's message content\n",
    "        if turn == 0:\n",
    "            # Include initial prompt and images in the first message\n",
    "            user_content = [{\"type\": \"text\", \"text\": initial_prompt + \" \" + user_question}]\n",
    "            for data_url in data_urls:\n",
    "                user_content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": data_url}\n",
    "                })\n",
    "        else:\n",
    "            user_content = user_question\n",
    "\n",
    "        # Append user's message to messages\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content\n",
    "        })\n",
    "\n",
    "        # Construct the payload\n",
    "        payload = {\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 3000,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "\n",
    "        # Send payload to model and get assistant's response\n",
    "        response = predictor.predict(payload)\n",
    "        assistant_response = response['choices'][0]['message']['content']\n",
    "\n",
    "        # Append assistant's response to messages\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_response\n",
    "        })\n",
    "\n",
    "        print(\"\\nAssistant:\", assistant_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image paths\n",
    "image_paths = [\n",
    "    \"Pixtral_data/trimmed_green_beans.jpg\",\n",
    "    \"Pixtral_data/amazon_gloves.jpg\",\n",
    "    \"Pixtral_data/cleaner.jpg\"\n",
    "]\n",
    "\n",
    "# Run the visual Q&A function\n",
    "visual_qa(predictor, image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Damage Assessment\n",
    "\n",
    "Insurance agents need to assess damage to the vehicle by assessing images taken at the time of issuing policy and during claim processing. Pixtral's vision capabilities can be used to assess damages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a helpful ai assistant for an insurance agent. Insurance agent has received a claim for a vehicle damage. This claim includes two images. One of the image was taken before the incident and another was taken after the incident.\n",
    "Analyse these images and answer below questions:\n",
    "1. describe if there is any damage to the vehicle\n",
    "2. should insurance agent accept or reject the claim\n",
    "\n",
    "\"\"\"\n",
    "image_paths = [\n",
    "    \"Pixtral_data/car_image_before.png\",\n",
    "    \"Pixtral_data/car_image_after.png\"\n",
    "]  \n",
    "\n",
    "# Send to model\n",
    "response = send_images_to_model(predictor, prompt, image_paths)\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_paths[0]).show()\n",
    "Image.open(image_paths[1]).show()\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that Pixtral has successfully identified the damage to the vehicle, which can be valuable for streamlining the insurance claim process. In a similar manner, Pixtral can be applied to other use cases within insurance claim verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up resources\n",
    "predictor.delete_endpoint()\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution for street sign images\n",
    "This image is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license.\n",
    "\n",
    "Creator: Unknown\n",
    "Source: https://www.mapillary.com/dataset/trafficsign\n",
    "License: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
    "Modifications: none\n",
    "\n",
    "U. Marti and H. Bunke. The IAM-database: An English Sentence Database for Off-line Handwriting Recognition. Int. Journal on Document Analysis and Recognition, Volume 5, pages 39 - 46, 2002.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
