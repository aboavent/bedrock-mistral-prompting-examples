{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edcbf738",
   "metadata": {},
   "source": [
    "## Deploy Pixtral 12b on SageMaker using LMI v12 for Realtime inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ae92d",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to deploy the [Pixtral-12b](https://huggingface.co/mistralai/Pixtral-12B-2409), gated model using the LMI v12 container. The deployment method uses [DJL serving](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-djl-serving.html) and the LMI v12 (0.30.0) or latest.\n",
    "\n",
    "The model is deployed on SageMaker Endpoints for Realtime inferencing on ml.g5.12xlarge instance.\n",
    "\n",
    "Note: This model is not stored in the typical HuggingFace pretrained format, so more configurations are required to deploy this successfully. While there are community versions of this model that have been converted into the HuggingFace pretrained format, those models are not compatible with LMI v12 as they are not compatible with vLLM. \n",
    "\n",
    "If you have fine-tuned this model and saved the artifacts in the HuggingFace pretrained format, you will need to convert the artifacts back into the mistral format. You can read more about that process in this discussion: https://huggingface.co/mistral-community/pixtral-12b/discussions/4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b26e58",
   "metadata": {},
   "source": [
    "### Install Required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d69b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8969a",
   "metadata": {},
   "source": [
    "## Create the SageMaker model object\n",
    "\n",
    "We will initialize necessary variables that could be used throught the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6039e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session() # sagemaker session for interacting with different AWS APIs\n",
    "\n",
    "sagemaker_session_bucket = None # bucket to house artifacts\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role() # execution role for the endpoint\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54ea48",
   "metadata": {},
   "source": [
    "The LMI container for djl-inference supports deploying popular vision language models like Pixtral with minimal set up. You can refer to the documentation for [Vision Language Models in LMI](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/user_guides/vision_language_models.md) for more details. The `mistralai/Pixtral-12B-2409` model will be downloaded from HuggingFace and it's a gated model. \n",
    "\n",
    "To access gated models from HuggingFace, you need token that needs to be set to the env variable `HF_TOKEN` in the following code. You can refer [Accessing Private/Gated Models](https://huggingface.co/docs/transformers.js/en/guides/private) to obtain the access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62631e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference import DJLModel\n",
    "\n",
    "image_uri =f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124\" \n",
    "\n",
    "# You can also obtain the image_uri programatically as follows.\n",
    "# image_uri = image_uris.retrieve(framework=\"djl-lmi\", version=\"0.30.0\", region=\"us-west-2\")\n",
    "\n",
    "model = DJLModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"mistralai/Pixtral-12B-2409\",\n",
    "        \"HF_TOKEN\": \"<REPLACE_WITH_YOUR_HF_TOKEN>\", #since the model \"mistralai/Pixtral-12B-2409\" is gated model, you need HF_TOKEN\n",
    "        \"OPTION_ENGINE\": \"Python\",\n",
    "        \"OPTION_MPI_MODE\": \"true\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"8192\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_TOKENIZER_MODE\": \"mistral\",\n",
    "        \"OPTION_ENTRYPOINT\": \"djl_python.huggingface\",\n",
    "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=4\", # this can be tuned to control how many images per prompt are allowed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5e907",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "We will deploy the model by providing the necessary arguments. You can refer to list of parameters to deploy the model on SageMaker here https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(instance_type=\"ml.g5.12xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a366fe",
   "metadata": {},
   "source": [
    "## Test prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bc8cb",
   "metadata": {},
   "source": [
    "The following prompts demonstrate how to use the `mistralai/Pixtral-12B-2409` model for:\n",
    "- Text only inference\n",
    "- Single image inference\n",
    "- Multi image inference\n",
    "- Local image inference\n",
    "\n",
    "For the multi image inference use-case, we use two images. However, the model is configured to accept up to 4 images in a single prompt. This setting can be tuned with the `OPTION_LIMIT_MM_PER_PROMPT` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2260ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_1_KITTEN = \"https://resources.djl.ai/images/kitten.jpg\"\n",
    "IMAGE_2_TRUCK = \"https://resources.djl.ai/images/truck.jpg\"\n",
    "\n",
    "text_only_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I would like to get better at basketball. Can you provide me a 3 month plan to improve my skills?\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "single_image_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Can you describe the following image and tell me what it contains?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": IMAGE_1_KITTEN\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "multi_image_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Can you describe the following images and tell me what they have in common? If they have nothing in common, please explain why.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": IMAGE_1_KITTEN\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": IMAGE_2_TRUCK\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485e48f",
   "metadata": {},
   "source": [
    "# Text Only Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prompt is:\\n {text_only_payload['messages'][0]['content']}\")\n",
    "text_only_output = predictor.predict(text_only_payload)\n",
    "print(\"Response is:\\n\")\n",
    "print(text_only_output['choices'][0]['message']['content'])\n",
    "print('----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb094d99",
   "metadata": {},
   "source": [
    "# Single Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response_kitten = requests.get(IMAGE_1_KITTEN)\n",
    "img_kitten = Image.open(BytesIO(response_kitten.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5230292",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the image provided to the model\")\n",
    "img_kitten.show()\n",
    "single_image_output = predictor.predict(single_image_payload)\n",
    "print(single_image_output['choices'][0]['message']['content'])\n",
    "print('----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc171f5",
   "metadata": {},
   "source": [
    "# Multi Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12250682",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_truck = requests.get(IMAGE_2_TRUCK)\n",
    "img_truck = Image.open(BytesIO(response_truck.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These are the images provided to the model\")\n",
    "img_kitten.show()\n",
    "img_truck.show()\n",
    "multi_image_output = predictor.predict(multi_image_payload)\n",
    "print(multi_image_output['choices'][0]['message']['content'])\n",
    "print('----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc9960",
   "metadata": {},
   "source": [
    "# Local image inference\n",
    "\n",
    "This example will demo sending the local file, as a part of input payload to the Pixtral model for inference. The input image has Earnings release for Amazon Q2 2024 and we will extract this financial data from the image in the format of choice, such as json, csv or markdown table. For this example, we will be asking Pixtral to generate markdown table representing as it is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd15017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "local_file_path = 'Pixtral_data/AMZN-Q2-2024-Earnings-Release.jpg'\n",
    "with open(local_file_path, \"rb\") as image_file:\n",
    "    base64_encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "local_image_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Extract the text from the provided image. Represent in a single markdown table as the source image. Ensure you verify the values before responding.\",\n",
    "                    #\"text\": \"Extract the text from the provided image. Try building a json response. Ensure you verify the values before responding.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpg;base64,{base64_encoded}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_image_output = predictor.predict(local_image_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input image to the model')\n",
    "Image.open(local_file_path).show()\n",
    "print('----------------------------')\n",
    "print('Response from the model\\n\\n')\n",
    "print(local_image_output['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up resources\n",
    "predictor.delete_endpoint()\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
