{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752d00e3-639c-42d1-9554-ada532792c6b",
   "metadata": {},
   "source": [
    "# Pixtral vLLM SageMaker Deployment guide - v1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2db89-62e8-4fee-88be-29d626280b58",
   "metadata": {},
   "source": [
    "In this notebook, we provide to you a guide to performing a simple deployment of Pixtral with vLLM. Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0ddd4-04e4-4b7d-9bb0-108448ade664",
   "metadata": {},
   "source": [
    "Currently, Pixtral models on HuggingFace are in the original consolidated format of:\n",
    "    => params.json and consolidated.safetensors \n",
    "    and not the standard  HF format:\n",
    "    => model-00001-of-00003.safetensors and config.json\n",
    "    \n",
    "We are not able to deploy the model with TGI at the moment but Mistral has upstreamed changes in `v0.6.1` of vLLM to allow users to deploy Pixtral with this 'Mistral' format.\n",
    "The djl-lmi container for vLLM is currently being worked on by us to allow deployment of the model to a SM endpoint and easy inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83d931-c66b-4e51-8a40-940199c4fb67",
   "metadata": {},
   "source": [
    "#### Note: spin up your sagemaker notebook instance with a g5.12xlarge to follow along with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c37c1-c9fb-4427-b62d-0484cae06f77",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0400d-184b-4866-86dd-0b7480158637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "gradio\n",
    "vllm==0.6.1.post2\n",
    "ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d55ac-7432-4e52-8682-09618c21596c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91714c9c-c1bf-4808-ac81-6326c709f29f",
   "metadata": {},
   "source": [
    "---\n",
    "A prerequisite for this notebook is to have a huggingface token with read access set up to be able to access the gated model from HuggingFace.\n",
    "\n",
    "=>Follow the steps here for getting access to a [HF token](https://huggingface.co/docs/hub/en/security-tokens)\n",
    "\n",
    "Once you have your access token and have allowed access to the model on HuggingFace, you can proceed to the login below. If you run into dependency errors with the cell below, please login to Hugging Face via your CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50171c89-5241-4570-850c-f1626403b813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75058966-a943-4f01-8cd3-601b1fb0785b",
   "metadata": {},
   "source": [
    "For the purpose of this notebook, you need access to an instance with at least 24 gb of GPU memory to load the model in. \n",
    "\n",
    "In order to utilize the complete context window of the model, you need a larger instance size since the model supports up to 128k tokens, \n",
    "since we are only able to store upto 102096 tokens in the kv cache with a g5.12xlarge\n",
    "\n",
    "In this example we limit the `max_model_len` param in the instance of the vLLM LLM class to 20k for demonstration purposes. Ensure you have the GPU capacity if you would like to utilize the complete context window.\n",
    "\n",
    "We also set `tensor_parallel_size` to 4 since we are using a g5.12xlarge with 4x Nvidia A10g GPUs. Change this according to the instance you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fce1d0-6464-40ed-98d7-5f6aa97ccd72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#using a g5.12xlarge\n",
    "import gradio as gr\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "import torch.multiprocessing as mp\n",
    "# Set the multiprocessing start method early in the script, to not fork the process\n",
    "mp.set_start_method('spawn', force=True)\n",
    "# Define the model and LLM object globally to avoid reloading for every request\n",
    "model_name = \"mistralai/Pixtral-12B-2409\"\n",
    "llm = LLM(model=model_name, tokenizer_mode=\"mistral\", tensor_parallel_size=4, max_model_len=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4d58c-3c1b-48ab-b0a2-59800cca5c4c",
   "metadata": {},
   "source": [
    "### Creating our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a52b3-02d0-41d4-9956-e55a37a50269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for a simple demo\n",
    "def simple_response(prompt, image_url):\n",
    "    sampling_params = SamplingParams(max_tokens=8192)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    outputs = llm.chat(messages, sampling_params=sampling_params)\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "# Define the Gradio interface\n",
    "simple_demo_interface = gr.Interface(\n",
    "    fn=simple_response, \n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"), \n",
    "        gr.Textbox(label=\"Image URL\")\n",
    "    ], \n",
    "    outputs=\"text\",\n",
    "    title=\"Pixtral Image Description\",\n",
    "    description=\"Provide a prompt and an image URL to get a description.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d28441-281f-4945-a8b1-fa19135166bd",
   "metadata": {},
   "source": [
    "### Gradio interface with Pixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f889c-a5f2-4bb8-9c8d-26ebe411c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.TabbedInterface([simple_demo_interface], [\"Simple Pixtral Demo\"])\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdb121-3f72-4160-951b-08619808f5e3",
   "metadata": {},
   "source": [
    "---\n",
    "#### Examples\n",
    "bounding box example: \"https://huggingface.co/datasets/nithiyn/bounding-box/resolve/main/bounding-box-ppl.jpg\"\n",
    "prompt: describe in detail, the first three objects within bounding boxes\n",
    "\n",
    "mykonos: \"https://huggingface.co/datasets/nithiyn/bounding-box/resolve/main/mykonos-2.jpeg\"\n",
    "prompt: Describe and identify the location in this image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3138472-bf60-46cd-b4a9-fe1fd5c149a7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we loaded in Mistral's Pixtral model with vLLM and created a simple Gradio interface to inference with the model.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe6266-7094-48fd-b710-52ab29dd73cd",
   "metadata": {},
   "source": [
    "### Distributors\n",
    "\n",
    "- Mistral AI\n",
    "- AWS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
