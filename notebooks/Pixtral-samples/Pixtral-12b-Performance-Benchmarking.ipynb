{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035cb23f",
   "metadata": {},
   "source": [
    "# Run Instance Performance Benchmarking for Pixtral-12B-2409\n",
    "\n",
    "This Jupyter notebook is designed to benchmark the performance of the Pixtral 12B model on Amazon SageMaker across multiple instance types. The primary goal is to evaluate how different instance configurations impact the model’s inference time, throughput, and efficiency. The notebook includes detailed steps for loading the Pixtral 12B model, running inference tasks, and collecting performance metrics on a variety of SageMaker instance types. By comparing these metrics, users can gain insights into the optimal instance choice based on their specific workload requirements, whether they prioritize speed, scalability, or cost-effectiveness.\n",
    "\n",
    "### awscurl\n",
    "\n",
    "To generate load on the SageMaker endpoints during benchmarking, this notebook utilizes the [awscurl](https://github.com/okigan/awscurl) tool. awscurl is a command-line utility that simplifies making authenticated HTTP requests to AWS services, including SageMaker endpoints. By using awscurl, we can simulate high traffic and stress test the endpoints, enabling us to measure the model’s performance under varying levels of load.\n",
    "\n",
    "For more information on running Pixtral-12B-2409, please see the [Pixtral-12b-LMI-SageMaker-realtime-inference.ipynb] (https://github.com/aws-samples/mistral-on-aws/blob/main/notebooks/Pixtral-samples/Pixtral-12b-LMI-SageMaker-realtime-inference.ipynb) notebook.\n",
    "\n",
    "If you're interested in learning more about Pixtral capabilities, please see the [pixtral_capabilities.ipynb] (https://github.com/aws-samples/mistral-on-aws/blob/main/notebooks/Pixtral-samples/Pixtral_capabilities.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe7cd8-b361-4879-9ebe-fe9baf98ccf0",
   "metadata": {},
   "source": [
    "### Install Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52fe3a-aabc-495d-86b7-31026f4168f2",
   "metadata": {},
   "source": [
    "Setup tools and python packages used in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a59b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd39362-1a0d-4373-9103-0d7888efdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e87d4-b3bb-47c0-a0fc-c5dbcb1d8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update -qq > /dev/null\n",
    "!sudo apt-get install -y default-jre wget > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d211b39-6248-43bd-a67c-c085d5fd1235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --quiet https://www.github.com/frankfliu/junkyard/releases/download/v0.3.1/awscurl\n",
    "!chmod +x awscurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011de61-53ee-4c71-bbbd-dce349742686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "import re\n",
    "import base64\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "from IPython.display import display, HTML\n",
    "from sagemaker.djl_inference import DJLModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbcd43-4df6-4be7-a03e-a81a9dbd19c4",
   "metadata": {},
   "source": [
    "Capture sagemaker role and session information to be used later in the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b658fea-0c0a-4997-a824-73a21f69a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "region = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797eac8-a045-4c30-8dd2-9f9c9a4172a4",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "In this section, we prepare a dataset to generate load against the SageMaker endpoint by encoding images as part of the inference prompts. Although we’re using a specific set of images here, feel free to substitute others that better match your own use case. Note that each image, once encoded, may contribute roughly ~1,500–2,500 tokens to the input payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a2ee1-87e0-442e-a060-ba9ef3847716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function reads image file and base64 encodes \n",
    "def read_and_encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a local file path and encodes it to a data URL.\n",
    "    \"\"\"\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    # Determine the image MIME type (e.g., image/jpeg, image/png)\n",
    "    mime_type = Image.open(image_path).get_format_mimetype()\n",
    "    image_content = f\"data:{mime_type};base64,{base64_encoded}\"\n",
    "    return image_content\n",
    "    \n",
    "def prepare_prompt_file(prompt, image_path, file_name):\n",
    "    \"\"\"\n",
    "    Generates a prompt file\n",
    "    \"\"\"\n",
    "\n",
    "    data_url = read_and_encode_image(image_path)\n",
    "    content_list = [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "    }]\n",
    "    content_list.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": data_url\n",
    "            }\n",
    "            \n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content_list\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "\n",
    "    file_path = f'{local_dataset_path}{file_name}'\n",
    "    \n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(payload, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216adb9-1ecf-43ab-bc50-cb10175721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset is stored in json files \n",
    "\n",
    "local_dataset_path=\"./Pixtral_benchmarking_data/\"\n",
    "\n",
    "prepare_prompt_file(prompt='extract product information from the image', \n",
    "                    image_path='Pixtral_data/cleaner.jpg', \n",
    "                    file_name='prompt1.json')\n",
    "\n",
    "\n",
    "\n",
    "prepare_prompt_file(prompt='Analyze the image and transcribe any handwritten text present. Convert the handwriting into a single, continuous string of text. Maintain the original spelling, punctuation, and capitalization as written. Ignore any printed text, drawings, or other non-handwritten elements in the image.', \n",
    "                    image_path='Pixtral_data/a01-082u-01.png', \n",
    "                    file_name='prompt2.json')\n",
    "\n",
    "\n",
    "prepare_prompt_file(prompt='As an interior designer, provide your comments on the aesthetics', \n",
    "                    image_path='Pixtral_data/dresser.jpg', \n",
    "                    file_name='prompt3.json')\n",
    "\n",
    "\n",
    "\n",
    "prepare_prompt_file(prompt='for an e-commerce catalog, generate product description for the product in the image', \n",
    "                    image_path='Pixtral_data/luggage.jpg', \n",
    "                    file_name='prompt4.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490741c",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoints\n",
    "\n",
    "In this section, we create multiple SageMaker endpoints. Each endpoint runs same model with different instance types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278b087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri =f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124\" \n",
    "\n",
    "# You can also obtain the image_uri programatically as follows.\n",
    "# image_uri = image_uris.retrieve(framework=\"djl-lmi\", version=\"0.30.0\", region=\"us-west-2\")\n",
    "\n",
    "model = DJLModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"mistralai/Pixtral-12B-2409\",\n",
    "        \"HF_TOKEN\": \"<HF_Token>\", #since the model \"mistralai/Pixtral-12B-2409\" is gated model, you need a HF_TOKEN & go to https://huggingface.co/mistralai/Pixtral-12B-2409 to be granted access\n",
    "        \"OPTION_ENGINE\": \"Python\",\n",
    "        \"OPTION_MPI_MODE\": \"true\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"8192\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_TOKENIZER_MODE\": \"mistral\",\n",
    "        \"OPTION_ENTRYPOINT\": \"djl_python.huggingface\",\n",
    "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=4\", # this can be tuned to control how many images per prompt are allowed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b91051-04f2-4a7d-8c07-09643a5839b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy endpoint\n",
    "\n",
    "endpoint_name = 'pixtral12b-on-ml-g5-12xlarge'\n",
    "predictor12xlarge = model.deploy(instance_type=\"ml.g5.12xlarge\", initial_instance_count=1, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316c234-792d-4102-bfeb-7c8f33ab96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy endpoint\n",
    "\n",
    "endpoint_name = 'pixtral12b-on-ml-g5-24xlarge'\n",
    "predictor24xlarge = model.deploy(instance_type=\"ml.g5.24xlarge\", initial_instance_count=1, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24078275-1ba9-4be2-b933-028cddf47799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy endpoint\n",
    "\n",
    "endpoint_name = 'pixtral12b-on-ml-g5-48xlarge'\n",
    "predictor48xlarge = model.deploy(instance_type=\"ml.g5.48xlarge\", initial_instance_count=1, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379675e9-6ce7-4d7e-b0ac-1207c4ed1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy endpoint\n",
    "\n",
    "endpoint_name = 'pixtral12b-on-ml-p4d-24xlarge'\n",
    "predictorp4d24xlarge = model.deploy(instance_type=\"ml.p4d.24xlarge\", initial_instance_count=1, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f944e9e-88ae-4a81-8dde-2a7dc26c3871",
   "metadata": {},
   "source": [
    "## Run Benchmarks\n",
    "\n",
    "In this section, we benchmark each endpoint with same dataset, concurrency and iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64385449-b468-4b76-bb8c-863ca33ee2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heloer function to set aws credentials with temporary token\n",
    "\n",
    "def set_credentials():\n",
    "    sts_client = boto3.client('sts')\n",
    "    credentials = sts_client._get_credentials()\n",
    "    \n",
    "    # Set environment variables\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = credentials.access_key\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = credentials.secret_key\n",
    "    os.environ[\"AWS_SESSION_TOKEN\"] = credentials.token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40ffa3-8e36-4d6b-bb52-9749f89cb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to extract values from the output file\n",
    "\n",
    "def extract_values_from_benchmark_output(output_data: str, field_name: str):\n",
    "    \n",
    "    pattern = r\"{field_name}:\\s([\\d\\.]+)\\s\"\n",
    "    pattern = pattern.replace('{field_name}', field_name)\n",
    "\n",
    "    # Search for the pattern in the list\n",
    "    for line in output_data:\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            value = float(match.group(1))\n",
    "            break\n",
    "    \n",
    "    return value\n",
    "\n",
    "\n",
    "# Helper function to extract instance type from the endpoint name\n",
    "def get_instance_type_from_endpoint_name(endpoint_name):\n",
    "    # Create a SageMaker client\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "    # Describe the endpoint to get the endpoint configuration name\n",
    "    response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    \n",
    "    # Extract the endpoint configuration name\n",
    "    endpoint_config_name = response['EndpointConfigName']\n",
    "    \n",
    "    # Describe the endpoint configuration to get the instance type\n",
    "    config_response = sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    \n",
    "    # Extract the instance types from the configuration\n",
    "    instance_types = []\n",
    "    for variant in config_response['ProductionVariants']:\n",
    "        instance_types.append(variant['InstanceType'])\n",
    "\n",
    "    return instance_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8de0be-cb8f-4391-a2bb-87d95e0f9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run benchmarks on provided endpoint\n",
    "\n",
    "def run_benchmark(endpoint_name):\n",
    "    endpoint_url = f'https://runtime.sagemaker.{region}.amazonaws.com/endpoints/{endpoint_name}/invocations'\n",
    "    print(endpoint_url)\n",
    "    os.environ[\"ENDPOINT_URL\"] = endpoint_url\n",
    "    \n",
    "    set_credentials()\n",
    "    print('Starting Benchmarking...')\n",
    "    output = !./awscurl -c 5 -N 50 -X POST $ENDPOINT_URL \\\n",
    "    --connect-timeout 120   -H \"Content-Type: application/json\" --dataset Pixtral_benchmarking_data -t -n sagemaker \n",
    "\n",
    "    print('Finished Benchmarking')\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c868b60-c942-47bb-ad22-0b996d2c53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain list of results to display in graph \n",
    "\n",
    "benchmark_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35d55f-9d76-448c-b79e-44f88308964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmarks on ml.g5.12xlarge instance\n",
    "\n",
    "endpoint_name = predictor12xlarge.endpoint_name\n",
    "\n",
    "output = run_benchmark(endpoint_name)\n",
    "\n",
    "instance_type = get_instance_type_from_endpoint_name(endpoint_name)\n",
    "avg_latency = extract_values_from_benchmark_output(output, 'Average Latency')\n",
    "\n",
    "benchmark_results.append({'InstanceType': instance_type, 'AverageLatency': avg_latency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841b3ce-e767-4029-a85f-d7a34f7f98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmarks on ml.g5.24xlarge instance\n",
    "\n",
    "endpoint_name = predictor24xlarge.endpoint_name\n",
    "\n",
    "output = run_benchmark(endpoint_name)\n",
    "\n",
    "instance_type = get_instance_type_from_endpoint_name(endpoint_name)\n",
    "avg_latency = extract_values_from_benchmark_output(output, 'Average Latency')\n",
    "\n",
    "benchmark_results.append({'InstanceType': instance_type, 'AverageLatency': avg_latency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e079b4-caf4-49b1-ab81-7b0cb39c8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmarks on ml.g5.48xlarge instance\n",
    "\n",
    "endpoint_name = predictor48xlarge.endpoint_name\n",
    "\n",
    "output = run_benchmark(endpoint_name)\n",
    "\n",
    "instance_type = get_instance_type_from_endpoint_name(endpoint_name)\n",
    "avg_latency = extract_values_from_benchmark_output(output, 'Average Latency')\n",
    "\n",
    "benchmark_results.append({'InstanceType': instance_type, 'AverageLatency': avg_latency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5435e41-7920-4703-98c9-280574ccba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmarks on ml.p4d.24xlarge instance\n",
    "\n",
    "endpoint_name = predictorp4d24xlarge.endpoint_name\n",
    "\n",
    "output = run_benchmark(endpoint_name)\n",
    "\n",
    "instance_type = get_instance_type_from_endpoint_name(endpoint_name)\n",
    "avg_latency = extract_values_from_benchmark_output(output, 'Average Latency')\n",
    "\n",
    "benchmark_results.append({'InstanceType': instance_type, 'AverageLatency': avg_latency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6b2e6-37de-4f1a-924a-c0219002f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results list in a dataframe and display as a bar chart\n",
    "\n",
    "def display_results(results: list):\n",
    "\n",
    "    # Create a dataframe from the list\n",
    "    df = pd.DataFrame(results)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot a bar chart using the DataFrame\n",
    "    plt.bar(df['InstanceType'], df['AverageLatency'], color='skyblue')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title('Average Latency by Instance Type', fontsize=14)\n",
    "    plt.xlabel('Instance Type', fontsize=12)\n",
    "    plt.ylabel('Average Latency (ms)', fontsize=12)\n",
    "    \n",
    "    # Display the graph\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0326a7e-2d8a-475c-b7ee-53256e73174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "\n",
    "display_results(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de74988",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The preliminary results suggest that a larger instance may not necessarily yield lower latency under the current test conditions. We hypothesize that additional overhead from managing more GPUs, less-than-optimal parallelization parameters, and insufficient concurrency levels to fully leverage the available hardware may be contributing factors. Further tests and tuning are needed to confirm these suspicions and adjust parameters accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5371ebd-a27b-4477-80e1-46f7d19b604b",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Do not forget to cleanup your resources to avoid SageMaker endpoint costs in your account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba0e95-6d89-4c3a-a16d-06fc010c5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoints, model config\n",
    "predictor12xlarge.delete_endpoint()\n",
    "predictor24xlarge.delete_endpoint()\n",
    "predictor48xlarge.delete_endpoint()\n",
    "predictorp4d24xlarge.delete_endpoint()\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
