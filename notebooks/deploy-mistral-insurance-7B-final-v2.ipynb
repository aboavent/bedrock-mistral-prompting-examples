{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530c6e00-d006-4aa9-a27d-eadb083eae97",
   "metadata": {},
   "source": [
    "# Mistral Model Deployment on Inferentia2 using Amazon Sagemaker and Amazon Bedrock\n",
    "\n",
    "In this notebook, we’ll walk through the complete process of deploying a fine-tuned Mistral model for the insurance domain. The model will be optimized and deployed on Inferentia 2 instances using AWS Neuron with Amazon SageMaker. Additionally, we’ll convert the model to Hugging Face’s `safetensors` format, enabling seamless import into Amazon Bedrock for usage with the Converse API.\n",
    "\n",
    "By the end of this notebook, you’ll have both models accessible through a custom chatbot application built in Streamlit. This application allows you to interact with the models deployed on both Amazon SageMaker and Amazon Bedrock, providing flexibility and hands-on experience with two deployment environments.\n",
    "\n",
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import necessary libraries to facilitate model conversion, deployment, and API interactions across AWS services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabc627-53f9-4f16-9c6c-17f0f629403e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers sagemaker boto3 tiktoken torch blobfile sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de4538-d595-48bd-b113-abf0df631068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92884ef1-402b-4a43-b25c-1a4f799ee113",
   "metadata": {},
   "source": [
    "## Section 2: Export Variables for SageMaker Deployment\n",
    "\n",
    "Define essential variables for model deployment, including model details, batch size, sequence length, and AWS-specific configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cbc89-59d2-4dcd-a335-dcc67f3d569d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"bitext/Mistral-7B-Insurance\"\n",
    "BATCH_SIZE = 4\n",
    "SEQUENCE_LENGTH = 2048\n",
    "MAX_TOTAL_TOKENS = 2048  # Set independently for the total token limit\n",
    "NUM_CORES = 2\n",
    "HF_MODEL_ID_TO_PUSH = \"aboavent/Mistral-7B-Insurance-neuron\"\n",
    "HF_TOKEN = \"[ADD YOUR HF TOKEN HERE]\"\n",
    "PRECISION = \"fp16\"\n",
    "MODEL_OUTPUT_NAME = \"Mistral-7B-Insurance-neuron\"\n",
    "COMPILED_MODEL_OUTPUT_PATH = f\"./{MODEL_OUTPUT_NAME}\"  # Concatenated path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fa2d7-d3a9-462d-a97a-9d611bffac2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 3: Hugging Face Authentication\n",
    "\n",
    "Authenticate with Hugging Face using the provided token to access and upload model resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4e3f6-b38c-4609-8312-689e2714f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d6563-cd4e-4158-b133-ee2d12a879c3",
   "metadata": {},
   "source": [
    "## Section 4: Model Compilation with Optimum CLI\n",
    "\n",
    "Use `optimum-cli` to export the model to Neuron-compatible format, specifying batch size, sequence length, precision, and number of cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b1e32-f550-4aee-98aa-ae3ef276f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export neuron \\\n",
    "    -m $MODEL_ID \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --sequence_length $SEQUENCE_LENGTH \\\n",
    "    --num_cores $NUM_CORES \\\n",
    "    --auto_cast_type $PRECISION \\\n",
    "    --trust-remote-code \\\n",
    "    $COMPILED_MODEL_OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a34b98-90ff-483d-af6f-fb024f4be308",
   "metadata": {},
   "source": [
    "## Section 5: Upload Compiled Model to Hugging Face\n",
    "\n",
    "Create a new repository on Hugging Face (if necessary) and upload the compiled model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f73fe-d18a-4ba6-be91-d29e26f2f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli repo create $MODEL_OUTPUT_NAME\n",
    "\n",
    "!huggingface-cli upload $HF_MODEL_ID_TO_PUSH $COMPILED_MODEL_OUTPUT_PATH ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ebde0-1a86-42b8-b3ae-e3c2e5a0cddf",
   "metadata": {},
   "source": [
    "## Section 6: Define SageMaker Role and Model Configuration\n",
    "\n",
    "Define the SageMaker role and configure the environment variables for deploying the model on SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972c4b0-94c6-48ee-a440-d58f53cdbea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_region = \"us-east-2\"  # Region for SageMaker endpoint\n",
    "bedrock_region = \"us-west-2\"    # Region for Bedrock model\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "    \"HF_MODEL_ID\": HF_MODEL_ID_TO_PUSH,\n",
    "    \"HF_NUM_CORES\": str(NUM_CORES),\n",
    "    \"HF_SEQUENCE_LENGTH\": str(SEQUENCE_LENGTH),\n",
    "    \"HF_AUTO_CAST_TYPE\": PRECISION,\n",
    "    \"MAX_BATCH_SIZE\": str(BATCH_SIZE),\n",
    "    \"MAX_INPUT_TOKENS\": \"1800\",\n",
    "    \"MAX_TOTAL_TOKENS\": str(MAX_TOTAL_TOKENS),\n",
    "    \"HF_TOKEN\": HF_TOKEN,\n",
    "    \"MESSAGES_API_ENABLED\": \"true\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7424ec-946f-4582-94aa-afe433ca6815",
   "metadata": {},
   "source": [
    "## Section 7: Deploy Model to SageMaker Inference\n",
    "\n",
    "Deploy the compiled model on a `ml.inf2.xlarge` instance in SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5def9-1a33-4012-a0fa-5bc0f0acdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface-neuronx\", version=\"0.0.24\"),\n",
    "    env=hub,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "huggingface_model._is_compiled_model = True\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    container_startup_health_check_timeout=2400,\n",
    "    volume_size=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cd895-2a2f-4bd8-a2ad-c1347d3128a6",
   "metadata": {},
   "source": [
    "## Section 8: Test SageMaker Endpoint\n",
    "\n",
    "Send a sample request to the SageMaker endpoint to verify that the model is deployed and functioning correctly using the HuggingFace API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a59ef-4170-4c88-a16f-763b2f116a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_request(system_prompt, user_query):\n",
    "    \"\"\"\n",
    "    Creates a sample request structure for the predictor based on the given system prompt and user query.\n",
    "\n",
    "    Parameters:\n",
    "        system_prompt (str): The initial system prompt to set the model's role.\n",
    "        user_query (str): The user's query for the insurance model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A structured request for the SageMaker predictor.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "    }\n",
    "\n",
    "# List of different system prompts and user queries to test various scenarios\n",
    "system_user_queries = [\n",
    "    (\"You are an expert in health insurance policies.\", \"What benefits do I get with my current health plan?\"),\n",
    "    (\"You are an insurance advisor.\", \"How can I reduce my monthly insurance premium?\"),\n",
    "    (\"You are an expert in auto insurance policies.\", \"What happens if my car is totaled?\"),\n",
    "    (\"You are an expert in life insurance.\", \"Can you explain the difference between term and whole life insurance?\"),\n",
    "    (\"You are an insurance claims specialist.\", \"What documents are needed to file a claim for home insurance?\"),\n",
    "    (\"You are a customer service representative for health insurance.\", \"Can I add my spouse to my health insurance policy?\"),\n",
    "    (\"You are an expert in travel insurance policies.\", \"What coverage do I have if my flight is canceled?\"),\n",
    "    (\"You are a specialist in pet insurance.\", \"Does my policy cover emergency vet visits?\"),\n",
    "    (\"You are an insurance fraud investigator.\", \"What are some common signs of insurance fraud?\"),\n",
    "    (\"You are an advisor on property insurance.\", \"How do I increase the coverage for natural disasters?\")\n",
    "]\n",
    "\n",
    "# Loop through each system prompt and user query, create a request, and get a response from the predictor\n",
    "for i, (system_prompt, user_query) in enumerate(system_user_queries, start=1):\n",
    "    print(f\"--- Sample Request {i} ---\")\n",
    "    request = create_sample_request(system_prompt, user_query)\n",
    "    response = predictor.predict(request)\n",
    "    print(\"System Prompt:\", system_prompt)\n",
    "    print(\"User Query:\", user_query)\n",
    "    print(\"Model Response:\", response['choices'][0]['message']['content'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01069ed1-96db-4230-a8ba-47f3fc87e9d7",
   "metadata": {},
   "source": [
    "### Send a sample request to the model using the SageMaker API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cef969-b503-4e40-9ed4-a78926654711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\", \n",
    "                                region_name=sagemaker_region)\n",
    "\n",
    "# Function to query the model on SageMaker\n",
    "def query_sagemaker_model(endpoint_name, query):\n",
    "    payload = {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,  # Updated model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in customer support for Insurance.\"},\n",
    "            {\"role\": \"user\", \"content\": query}  # Send the user query as a string\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 4096,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.90,\n",
    "            \"max_length\": 4096,\n",
    "            \"stop\": None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the request to SageMaker endpoint\n",
    "        response = sagemaker_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        result = json.loads(response['Body'].read())\n",
    "        print(result)\n",
    "        return result['choices'][0]['message']['content']\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred with SageMaker: {e.response['Error']['Message']}\")\n",
    "        return None\n",
    "    \n",
    "sagemaker_endpoint_name = \"huggingface-pytorch-tgi-inference-ml-in-2024-11-05-03-57-02-330\"  # SageMaker endpoint name    \n",
    "model_response = query_sagemaker_model(sagemaker_endpoint_name, \n",
    "                                       \"How can I reduce my monthly insurance premium?\")\n",
    "print(model_response)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec6f0b-4abf-4b50-a870-c680462633dd",
   "metadata": {},
   "source": [
    "## Section 9: Convert Model to Safetensors Format for Bedrock\n",
    "\n",
    "Define a function to convert the model to `safetensors` format, which is required for Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3ec75-63a9-448f-8d2d-523d20cd988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def convert_to_safetensors(model_name, save_directory):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face model to safetensors format for Amazon Bedrock compatibility.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the model to convert.\n",
    "        save_directory (str): Directory to save the converted model and tokenizer.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    print(f\"Converting and saving model to {save_directory} in safetensors format...\")\n",
    "    model.save_pretrained(save_directory, safe=True)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(\"Conversion complete!\")\n",
    "\n",
    "# Specify the directory and model name\n",
    "save_directory = os.path.expanduser(\"~/Mistral-7B-Insurance\")\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "convert_to_safetensors(MODEL_ID, save_directory)\n",
    "\n",
    "# List the contents of the save directory to verify the conversion\n",
    "os.listdir(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2d894-1183-437d-b300-ec341d2e4739",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 10: Upload Converted Model to S3\n",
    "\n",
    "Upload the `safetensors` formatted model files to an S3 bucket, making them accessible to Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbaef30-ef65-4368-b3fc-51adb50c0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Define S3 and local directory configurations\n",
    "s3_client = boto3.client(\"s3\", region_name=sagemaker_region)\n",
    "s3_bucket_name = \"mistral-7b-insurance-bedrock-import\"\n",
    "s3_model_directory = \"safetensors\"\n",
    "local_model_directory = save_directory  # Use save_directory from previous step\n",
    "\n",
    "def create_bucket_if_not_exists(bucket_name, region=\"us-west-2\"):\n",
    "    \"\"\"\n",
    "    Creates the S3 bucket if it does not exist.\n",
    "    \n",
    "    Parameters:\n",
    "        bucket_name (str): The name of the bucket to create.\n",
    "        region (str): The AWS region for the bucket.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"Bucket '{bucket_name}' does not exist. Creating bucket...\")\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "            print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "create_bucket_if_not_exists(s3_bucket_name, bedrock_region)\n",
    "\n",
    "def upload_to_s3(local_directory, bucket, s3_directory):\n",
    "    \"\"\"\n",
    "    Uploads all files from a local directory to the specified S3 bucket and directory.\n",
    "\n",
    "    Parameters:\n",
    "        local_directory (str): Path to the local directory containing files to upload.\n",
    "        bucket (str): Name of the S3 bucket.\n",
    "        s3_directory (str): Directory path within the S3 bucket to store the files.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(local_directory) if os.path.isfile(os.path.join(local_directory, f))]\n",
    "    \n",
    "    # Progress bar for uploads\n",
    "    for filename in tqdm(files, desc=\"Uploading files to S3\"):\n",
    "        file_path = os.path.join(local_directory, filename)\n",
    "        s3_path = f\"{s3_directory}/{filename}\"\n",
    "        print(f\"Uploading {filename} to s3://{bucket}/{s3_path}...\")\n",
    "        s3_client.upload_file(file_path, bucket, s3_path)\n",
    "        print(f\"{filename} uploaded successfully.\")\n",
    "\n",
    "# Run the upload function\n",
    "upload_to_s3(local_model_directory, s3_bucket_name, s3_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddc69d-5133-402c-961a-d3ff4bb6326a",
   "metadata": {},
   "source": [
    "## Section 11: Import Model into Amazon Bedrock\n",
    "\n",
    "Create an IAM Execution Role for Bedrock with Parameters to be used by\n",
    "a model import job in Amazon Bedrock using the files uploaded to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b27007-7f38-4c52-afda-845c2488e0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "# Parameters for source account\n",
    "source_account = \"603555443475\"  # Replace with your AWS account ID\n",
    "\n",
    "# IAM client and role/policy details\n",
    "iam_client = boto3.client('iam')\n",
    "role_name = \"BedrockModelImportExecutionRole\"\n",
    "policy_name = \"BedrockModelImportPolicy\"\n",
    "s3_bucket_name = \"mistral-7b-insurance-bedrock-import\"  # Replace with your actual bucket name\n",
    "\n",
    "# Define the trust policy to allow Bedrock to assume this role with specific conditions\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": source_account  \n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{bedrock_region}:{source_account}:model-import-job/*\"  \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define the permissions policy for S3 and Bedrock access\n",
    "permissions_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}\",\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModel\",\n",
    "                \"bedrock:GetModel\",\n",
    "                \"bedrock:ListModels\",\n",
    "                \"bedrock:CreateModelImportJob\",\n",
    "                \"bedrock:GetModelImportJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    print(\"Creating IAM Role...\")\n",
    "    role_response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=\"Role for Amazon Bedrock model import job with S3 access\"\n",
    "    )\n",
    "    role_arn = role_response['Role']['Arn']\n",
    "    print(f\"IAM Role created with ARN: {role_arn}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(f\"Role '{role_name}' already exists.\")\n",
    "        role_arn = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Attach the permissions policy to the role\n",
    "try:\n",
    "    print(\"Attaching policy to IAM Role...\")\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=policy_name,\n",
    "        PolicyDocument=json.dumps(permissions_policy)\n",
    "    )\n",
    "    print(\"Policy attached successfully.\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error attaching policy: {e}\")\n",
    "    raise\n",
    "\n",
    "# The role ARN will be used in the next cell to create the model import job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79f7da-29c8-4c50-baca-69c0a92625fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "bedrock_client = boto3.client('bedrock', region_name=bedrock_region)  \n",
    "s3_model_uri = f\"s3://{s3_bucket_name}/{s3_model_directory}/\"  \n",
    "imported_model_name = \"Mistral-7B-Insurance-Model\"\n",
    "\n",
    "# Use the IAM role ARN created in the previous cell\n",
    "job_name = f\"mistral-7b-insurance-import-job-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock_client.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,  # Use the ARN from the IAM role created in the previous cell\n",
    "    modelDataSource={'s3DataSource': {'s3Uri': s3_model_uri}}\n",
    ")\n",
    "\n",
    "print(\"Model import job created:\", response)\n",
    "print(json.dumps(response, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259592e-43c2-4598-8536-9478e583920b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Use the job name from the response of create_model_import_job to track the job\n",
    "polling_interval = 30  # Time in seconds between each status check\n",
    "\n",
    "def check_job_status(job_name):\n",
    "    \"\"\"\n",
    "    Checks the status of the model import job and returns the current status, failure message, and imported model ARN if available.\n",
    "\n",
    "    Parameters:\n",
    "        job_name (str): The name of the model import job to check.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the status, failure message, and imported model ARN if the job is completed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        status_response = bedrock_client.get_model_import_job(jobIdentifier=job_name)\n",
    "        return {\n",
    "            \"status\": status_response[\"status\"],\n",
    "            \"failureMessage\": status_response.get(\"failureMessage\", \"\"),\n",
    "            \"importedModelArn\": status_response.get(\"importedModelArn\", None)\n",
    "        }\n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop to check the job status periodically\n",
    "print(f\"Checking status for job {job_name} every {polling_interval} seconds...\")\n",
    "imported_model_arn = None\n",
    "while True:\n",
    "    result = check_job_status(job_name)\n",
    "    if result is None:\n",
    "        print(\"Unable to retrieve job status. Exiting.\")\n",
    "        break\n",
    "\n",
    "    status = result[\"status\"]\n",
    "    failure_message = result[\"failureMessage\"]\n",
    "    imported_model_arn = result[\"importedModelArn\"]\n",
    "    print(f\"Current status: {status}\")\n",
    "\n",
    "    # Check if the job has reached a final state\n",
    "    if status in [\"Completed\", \"Failed\"]:\n",
    "        if status == \"Failed\" and failure_message:\n",
    "            print(f\"Job failed with message: {failure_message}\")\n",
    "            imported_model_arn = None  # Clear the ARN if the job failed\n",
    "        else:\n",
    "            print(f\"Job {job_name} finished with status: {status}\")\n",
    "            print(f\"Imported Model ARN: {imported_model_arn}\")\n",
    "        break\n",
    "\n",
    "    # Wait before the next status check\n",
    "    time.sleep(polling_interval)\n",
    "\n",
    "# Set the model ID to the imported model ARN if the job was successful\n",
    "if imported_model_arn:\n",
    "    imported_model_id = imported_model_arn  # Assign the model ARN to model_id for further use\n",
    "    print(f\"Model ID (ARN) for further use: {model_id}\")\n",
    "else:\n",
    "    print(\"Model import job did not complete successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac39fe-7012-449b-95dc-faf60c391443",
   "metadata": {},
   "source": [
    "## Section 12: Call Imported Model Using Amazon Bedrock Converse API\n",
    "\n",
    "Send a test request to the imported model on Amazon Bedrock using the Converse API to verify its functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a297cea-86d0-45bd-84bf-2b8d25376fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=bedrock_region)  \n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "# imported_model_id should be the imported model's ARN (importedModelArn)\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define the conversation messages, with user role correctly structured\n",
    "# Add the system-like instruction as part of the initial user message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"You are an expert in customer support for insurance. Please help me understand my health insurance benefits.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the converse function\n",
    "def converse(messages):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock converse API without a system message.\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): List of conversation messages.\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response from Bedrock.\n",
    "    \"\"\"\n",
    "    # Configure the conversation payload\n",
    "    converse_config = {\n",
    "        \"modelId\": imported_model_id,  # Use the imported model ARN as the model ID\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": 0.5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nConversation:\")\n",
    "    for message in messages:\n",
    "        print(f\"{message['role'].capitalize()}: {json.dumps(message['content'], indent=2)}\")\n",
    "    \n",
    "    # Call the converse API\n",
    "    try:\n",
    "        response = bedrock_runtime_client.converse(**converse_config)\n",
    "        return response\n",
    "    except ClientError as e:\n",
    "        error_message = e.response['Error']['Message']\n",
    "        print(f\"An error occurred: {error_message}\")\n",
    "        print(\"Converse config:\")\n",
    "        print(json.dumps(converse_config, indent=2))\n",
    "        return None\n",
    "\n",
    "def print_converse_response(response):\n",
    "    \"\"\"\n",
    "    Prints the conversation response in a readable format.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): The API response from Bedrock.\n",
    "    \"\"\"\n",
    "    if response:\n",
    "        print(f\"Response: {response['output']['message']['content'][0]['text']}\")\n",
    "        if 'trace' in response:\n",
    "            print(\"Trace:\")\n",
    "            print(json.dumps(response['trace'], indent=2))\n",
    "    else:\n",
    "        print(\"No response received.\")\n",
    "\n",
    "# Example usage\n",
    "# Run the converse function and print the response\n",
    "response = converse(messages)\n",
    "print_converse_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6f89e-e69d-47e2-b138-3103ccbe76dc",
   "metadata": {},
   "source": [
    "## Section 13: Call Imported Model Using Amazon Bedrock Converse Streaming API\n",
    "\n",
    "This section demonstrates how to use the Amazon Bedrock converse_stream API to handle multiple test cases in real-time. Each message in the sample set represents a unique query to simulate different customer support scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41005eb4-5186-426b-9a9d-7c94dc48ed06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=\"us-west-2\")  # Replace with your region\n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define multiple conversation samples without system prompts\n",
    "sample_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you help me understand my health insurance benefits?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"What does my policy cover if I need to see a specialist?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Are dental treatments covered in my current insurance plan?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"How do I file a claim for a recent doctor visit?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you explain what deductible means in my policy?\"}]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Inference parameters\n",
    "inference_config = {\"temperature\": 0.5}\n",
    "additional_model_fields = {\"top_k\": 200}\n",
    "\n",
    "# Define the streaming converse function\n",
    "def stream_conversation(bedrock_client, model_id, messages, inference_config, additional_model_fields):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock converse_stream API and handles streaming response.\n",
    "\n",
    "    Parameters:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (list): The messages to send.\n",
    "        inference_config (dict): The inference configuration to use.\n",
    "        additional_model_fields (dict): Additional model fields to use.\n",
    "    \"\"\"\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "# Example usage of streaming for multiple test cases\n",
    "try:\n",
    "    for i, messages in enumerate(sample_messages, 1):\n",
    "        print(\"\\n\" + \"=\"*50)  # Line separator for clarity\n",
    "        print(f\"\\nStarting streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "        stream_conversation(\n",
    "            bedrock_runtime_client,\n",
    "            imported_model_id,  # Use the imported model ARN as model ID\n",
    "            messages,\n",
    "            inference_config,\n",
    "            additional_model_fields\n",
    "        )\n",
    "        print(f\"\\nFinished streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "except ClientError as err:\n",
    "    error_message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", error_message)\n",
    "    print(\"A client error occurred: \" + format(error_message))\n",
    "else:\n",
    "    print(f\"\\nFinished streaming all test cases with model {imported_model_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0629cee-f853-4355-97c9-525507a4967a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 4.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-311-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
