{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530c6e00-d006-4aa9-a27d-eadb083eae97",
   "metadata": {},
   "source": [
    "# Mistral Model Deployment on Inferentia2 using Amazon Sagemaker and Amazon Bedrock\n",
    "\n",
    "In this notebook, we’ll walk through the complete process of deploying a fine-tuned Mistral model for the insurance domain. The model will be optimized and deployed on Inferentia 2 instances using AWS Neuron with Amazon SageMaker. Additionally, we’ll convert the model to Hugging Face’s `safetensors` format, enabling seamless import into Amazon Bedrock for usage with the Converse API.\n",
    "\n",
    "By the end of this notebook, you’ll have both models accessible through a custom chatbot application built in Streamlit. This application allows you to interact with the models deployed on both Amazon SageMaker and Amazon Bedrock, providing flexibility and hands-on experience with two deployment environments.\n",
    "\n",
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import necessary libraries to facilitate model conversion, deployment, and API interactions across AWS services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabc627-53f9-4f16-9c6c-17f0f629403e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers sagemaker boto3 tiktoken torch blobfile sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9de4538-d595-48bd-b113-abf0df631068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92884ef1-402b-4a43-b25c-1a4f799ee113",
   "metadata": {},
   "source": [
    "## Section 2: Export Variables for SageMaker Deployment\n",
    "\n",
    "Define essential variables for model deployment, including model details, batch size, sequence length, and AWS-specific configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2cbc89-59d2-4dcd-a335-dcc67f3d569d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"bitext/Mistral-7B-Insurance\"\n",
    "BATCH_SIZE = 4\n",
    "SEQUENCE_LENGTH = 2048\n",
    "MAX_TOTAL_TOKENS = 2048  # Set independently for the total token limit\n",
    "NUM_CORES = 2\n",
    "HF_MODEL_ID_TO_PUSH = \"aboavent/Mistral-7B-Insurance-neuron\"\n",
    "HF_TOKEN = \"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\"\n",
    "PRECISION = \"fp16\"\n",
    "MODEL_OUTPUT_NAME = \"Mistral-7B-Insurance-neuron\"\n",
    "COMPILED_MODEL_OUTPUT_PATH = f\"./{MODEL_OUTPUT_NAME}\"  # Concatenated path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fa2d7-d3a9-462d-a97a-9d611bffac2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 3: Hugging Face Authentication\n",
    "\n",
    "Authenticate with Hugging Face using the provided token to access and upload model resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4e3f6-b38c-4609-8312-689e2714f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d6563-cd4e-4158-b133-ee2d12a879c3",
   "metadata": {},
   "source": [
    "## Section 4: Model Compilation with Optimum CLI\n",
    "\n",
    "Use `optimum-cli` to export the model to Neuron-compatible format, specifying batch size, sequence length, precision, and number of cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b1e32-f550-4aee-98aa-ae3ef276f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export neuron \\\n",
    "    -m $MODEL_ID \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --sequence_length $SEQUENCE_LENGTH \\\n",
    "    --num_cores $NUM_CORES \\\n",
    "    --auto_cast_type $PRECISION \\\n",
    "    --trust-remote-code \\\n",
    "    $COMPILED_MODEL_OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a34b98-90ff-483d-af6f-fb024f4be308",
   "metadata": {},
   "source": [
    "## Section 5: Upload Compiled Model to Hugging Face\n",
    "\n",
    "Create a new repository on Hugging Face (if necessary) and upload the compiled model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f73fe-d18a-4ba6-be91-d29e26f2f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli repo create $MODEL_OUTPUT_NAME\n",
    "\n",
    "!huggingface-cli upload $HF_MODEL_ID_TO_PUSH $COMPILED_MODEL_OUTPUT_PATH ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ebde0-1a86-42b8-b3ae-e3c2e5a0cddf",
   "metadata": {},
   "source": [
    "## Section 6: Define SageMaker Role and Model Configuration\n",
    "\n",
    "Define the SageMaker role and configure the environment variables for deploying the model on SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972c4b0-94c6-48ee-a440-d58f53cdbea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "hub = {\n",
    "    \"HF_MODEL_ID\": HF_MODEL_ID_TO_PUSH,\n",
    "    \"HF_NUM_CORES\": str(NUM_CORES),\n",
    "    \"HF_SEQUENCE_LENGTH\": str(SEQUENCE_LENGTH),\n",
    "    \"HF_AUTO_CAST_TYPE\": PRECISION,\n",
    "    \"MAX_BATCH_SIZE\": str(BATCH_SIZE),\n",
    "    \"MAX_INPUT_TOKENS\": \"1800\",\n",
    "    \"MAX_TOTAL_TOKENS\": str(MAX_TOTAL_TOKENS),\n",
    "    \"HF_TOKEN\": HF_TOKEN,\n",
    "    \"MESSAGES_API_ENABLED\": \"true\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7424ec-946f-4582-94aa-afe433ca6815",
   "metadata": {},
   "source": [
    "## Section 7: Deploy Model to SageMaker Inference\n",
    "\n",
    "Deploy the compiled model on a `ml.inf2.xlarge` instance in SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5def9-1a33-4012-a0fa-5bc0f0acdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface-neuronx\", version=\"0.0.24\"),\n",
    "    env=hub,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "huggingface_model._is_compiled_model = True\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    container_startup_health_check_timeout=2400,\n",
    "    volume_size=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cd895-2a2f-4bd8-a2ad-c1347d3128a6",
   "metadata": {},
   "source": [
    "## Section 8: Test SageMaker Endpoint\n",
    "\n",
    "Send a sample request to the SageMaker endpoint to verify that the model is deployed and functioning correctly using the HuggingFace API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a59ef-4170-4c88-a16f-763b2f116a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_request(system_prompt, user_query):\n",
    "    \"\"\"\n",
    "    Creates a sample request structure for the predictor based on the given system prompt and user query.\n",
    "\n",
    "    Parameters:\n",
    "        system_prompt (str): The initial system prompt to set the model's role.\n",
    "        user_query (str): The user's query for the insurance model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A structured request for the SageMaker predictor.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "    }\n",
    "\n",
    "# List of different system prompts and user queries to test various scenarios\n",
    "system_user_queries = [\n",
    "    (\"You are an expert in health insurance policies.\", \"What benefits do I get with my current health plan?\"),\n",
    "    (\"You are an insurance advisor.\", \"How can I reduce my monthly insurance premium?\"),\n",
    "    (\"You are an expert in auto insurance policies.\", \"What happens if my car is totaled?\"),\n",
    "    (\"You are an expert in life insurance.\", \"Can you explain the difference between term and whole life insurance?\"),\n",
    "    (\"You are an insurance claims specialist.\", \"What documents are needed to file a claim for home insurance?\"),\n",
    "    (\"You are a customer service representative for health insurance.\", \"Can I add my spouse to my health insurance policy?\"),\n",
    "    (\"You are an expert in travel insurance policies.\", \"What coverage do I have if my flight is canceled?\"),\n",
    "    (\"You are a specialist in pet insurance.\", \"Does my policy cover emergency vet visits?\"),\n",
    "    (\"You are an insurance fraud investigator.\", \"What are some common signs of insurance fraud?\"),\n",
    "    (\"You are an advisor on property insurance.\", \"How do I increase the coverage for natural disasters?\")\n",
    "]\n",
    "\n",
    "# Loop through each system prompt and user query, create a request, and get a response from the predictor\n",
    "for i, (system_prompt, user_query) in enumerate(system_user_queries, start=1):\n",
    "    print(f\"--- Sample Request {i} ---\")\n",
    "    request = create_sample_request(system_prompt, user_query)\n",
    "    response = predictor.predict(request)\n",
    "    print(\"System Prompt:\", system_prompt)\n",
    "    print(\"User Query:\", user_query)\n",
    "    print(\"Model Response:\", response['choices'][0]['message']['content'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01069ed1-96db-4230-a8ba-47f3fc87e9d7",
   "metadata": {},
   "source": [
    "### Send a sample request to the model using the SageMaker API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8cef969-b503-4e40-9ed4-a78926654711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'chat.completion', 'id': '', 'created': 1730908518, 'model': 'aboavent/Mistral-7B-Insurance-neuron', 'system_fingerprint': '2.1.1-native', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' To effectively reduce your monthly insurance premium, please adhere to the following guidelines:\\n\\n1. Evaluate your coverage: Examine your existing insurance plan and determine if there are any unused or superfluous options that may be contributing to the overall cost.\\n2. Shop around: Compare different insurance providers and the plans they offer to determine whether there are more affordable options available in the market.\\n3. Increase your deductible: Opting for a higher deductible will'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 100, 'total_tokens': 100}}\n",
      " To effectively reduce your monthly insurance premium, please adhere to the following guidelines:\n",
      "\n",
      "1. Evaluate your coverage: Examine your existing insurance plan and determine if there are any unused or superfluous options that may be contributing to the overall cost.\n",
      "2. Shop around: Compare different insurance providers and the plans they offer to determine whether there are more affordable options available in the market.\n",
      "3. Increase your deductible: Opting for a higher deductible will\n"
     ]
    }
   ],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker-runtime\", \n",
    "                                region_name=\"us-east-2\")\n",
    "\n",
    "# Function to query the model on SageMaker\n",
    "def query_sagemaker_model(endpoint_name, query):\n",
    "    payload = {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,  # Updated model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in customer support for Insurance.\"},\n",
    "            {\"role\": \"user\", \"content\": query}  # Send the user query as a string\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 4096,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.90,\n",
    "            \"max_length\": 4096,\n",
    "            \"stop\": None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the request to SageMaker endpoint\n",
    "        response = sagemaker_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        result = json.loads(response['Body'].read())\n",
    "        print(result)\n",
    "        return result['choices'][0]['message']['content']\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred with SageMaker: {e.response['Error']['Message']}\")\n",
    "        return None\n",
    "    \n",
    "sagemaker_endpoint_name = \"huggingface-pytorch-tgi-inference-ml-in-2024-11-05-03-57-02-330\"  # SageMaker endpoint name    \n",
    "model_response = query_sagemaker_model(sagemaker_endpoint_name, \n",
    "                                       \"How can I reduce my monthly insurance premium?\")\n",
    "print(model_response)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec6f0b-4abf-4b50-a870-c680462633dd",
   "metadata": {},
   "source": [
    "## Section 9: Convert Model to Safetensors Format for Bedrock\n",
    "\n",
    "Define a function to convert the model to `safetensors` format, which is required for Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de3ec75-63a9-448f-8d2d-523d20cd988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model bitext/Mistral-7B-Insurance...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc326c200982432aad36da357b1d41b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting and saving model to ~/Mistral-7B-Insurance in safetensors format...\n",
      "Conversion complete!\n",
      "CPU times: user 16.2 s, sys: 51.4 s, total: 1min 7s\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def convert_to_safetensors(model_name, save_directory):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face model to safetensors format for Amazon Bedrock compatibility.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the model to convert.\n",
    "        save_directory (str): Directory to save the converted model and tokenizer.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    print(f\"Converting and saving model to {save_directory} in safetensors format...\")\n",
    "    model.save_pretrained(save_directory, safe=True)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(\"Conversion complete!\")\n",
    "\n",
    "# Specify the directory and model name\n",
    "save_directory = os.path.expanduser(\"~/Mistral-7B-Insurance\")\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "convert_to_safetensors(MODEL_ID, save_directory)\n",
    "\n",
    "# List the contents of the save directory to verify the conversion\n",
    "os.listdir(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2d894-1183-437d-b300-ec341d2e4739",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 10: Upload Converted Model to S3\n",
    "\n",
    "Upload the `safetensors` formatted model files to an S3 bucket, making them accessible to Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fbaef30-ef65-4368-b3fc-51adb50c0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'mistral-7b-insurance-bedrock-import' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model-00002-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import/safetensors/model-00002-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  17%|█▋        | 2/12 [00:29<02:00, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00002-of-00006.safetensors uploaded successfully.\n",
      "Uploading tokenizer_config.json to s3://mistral-7b-insurance-bedrock-import/safetensors/tokenizer_config.json...\n",
      "tokenizer_config.json uploaded successfully.\n",
      "Uploading model-00006-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import/safetensors/model-00006-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  25%|██▌       | 3/12 [00:55<02:47, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00006-of-00006.safetensors uploaded successfully.\n",
      "Uploading model-00004-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import/safetensors/model-00004-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  33%|███▎      | 4/12 [01:28<03:15, 24.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00004-of-00006.safetensors uploaded successfully.\n",
      "Uploading tokenizer.model to s3://mistral-7b-insurance-bedrock-import/safetensors/tokenizer.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  50%|█████     | 6/12 [01:29<01:02, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model uploaded successfully.\n",
      "Uploading model.safetensors.index.json to s3://mistral-7b-insurance-bedrock-import/safetensors/model.safetensors.index.json...\n",
      "model.safetensors.index.json uploaded successfully.\n",
      "Uploading config.json to s3://mistral-7b-insurance-bedrock-import/safetensors/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  58%|█████▊    | 7/12 [01:29<00:35,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json uploaded successfully.\n",
      "Uploading model-00003-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import/safetensors/model-00003-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  75%|███████▌  | 9/12 [02:03<00:32, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00003-of-00006.safetensors uploaded successfully.\n",
      "Uploading generation_config.json to s3://mistral-7b-insurance-bedrock-import/safetensors/generation_config.json...\n",
      "generation_config.json uploaded successfully.\n",
      "Uploading special_tokens_map.json to s3://mistral-7b-insurance-bedrock-import/safetensors/special_tokens_map.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  83%|████████▎ | 10/12 [02:03<00:15,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map.json uploaded successfully.\n",
      "Uploading model-00005-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import/safetensors/model-00005-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  92%|█████████▏| 11/12 [02:36<00:15, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00005-of-00006.safetensors uploaded successfully.\n",
      "Uploading model-00001-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import/safetensors/model-00001-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3: 100%|██████████| 12/12 [03:07<00:00, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00001-of-00006.safetensors uploaded successfully.\n",
      "CPU times: user 1min 19s, sys: 42.3 s, total: 2min 2s\n",
      "Wall time: 3min 7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Define S3 and local directory configurations\n",
    "s3_client = boto3.client(\"s3\", region_name=\"us-west-2\")\n",
    "s3_bucket_name = \"mistral-7b-insurance-bedrock-import\"  # Updated to lowercase and valid name\n",
    "s3_model_directory = \"safetensors\"\n",
    "local_model_directory = save_directory  # Use save_directory from Section 9\n",
    "\n",
    "def create_bucket_if_not_exists(bucket_name, region=\"us-west-2\"):\n",
    "    \"\"\"\n",
    "    Creates the S3 bucket if it does not exist.\n",
    "    \n",
    "    Parameters:\n",
    "        bucket_name (str): The name of the bucket to create.\n",
    "        region (str): The AWS region for the bucket.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"Bucket '{bucket_name}' does not exist. Creating bucket...\")\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "            print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "create_bucket_if_not_exists(s3_bucket_name)\n",
    "\n",
    "def upload_to_s3(local_directory, bucket, s3_directory):\n",
    "    \"\"\"\n",
    "    Uploads all files from a local directory to the specified S3 bucket and directory.\n",
    "\n",
    "    Parameters:\n",
    "        local_directory (str): Path to the local directory containing files to upload.\n",
    "        bucket (str): Name of the S3 bucket.\n",
    "        s3_directory (str): Directory path within the S3 bucket to store the files.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(local_directory) if os.path.isfile(os.path.join(local_directory, f))]\n",
    "    \n",
    "    # Progress bar for uploads\n",
    "    for filename in tqdm(files, desc=\"Uploading files to S3\"):\n",
    "        file_path = os.path.join(local_directory, filename)\n",
    "        s3_path = f\"{s3_directory}/{filename}\"\n",
    "        print(f\"Uploading {filename} to s3://{bucket}/{s3_path}...\")\n",
    "        s3_client.upload_file(file_path, bucket, s3_path)\n",
    "        print(f\"{filename} uploaded successfully.\")\n",
    "\n",
    "# Run the upload function\n",
    "upload_to_s3(local_model_directory, s3_bucket_name, s3_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddc69d-5133-402c-961a-d3ff4bb6326a",
   "metadata": {},
   "source": [
    "## Section 11: Import Model into Amazon Bedrock\n",
    "\n",
    "Create an IAM Execution Role for Bedrock with Parameters to be used by\n",
    "a model import job in Amazon Bedrock using the files uploaded to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b27007-7f38-4c52-afda-845c2488e0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IAM Role...\n",
      "IAM Role created with ARN: arn:aws:iam::603555443475:role/BedrockModelImportExecutionRole\n",
      "Attaching policy to IAM Role...\n",
      "Policy attached successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "# Parameters for region and source account\n",
    "region = \"us-west-2\"  # Replace with your desired AWS region\n",
    "source_account = \"603555443475\"  # Replace with your AWS account ID\n",
    "\n",
    "# IAM client and role/policy details\n",
    "iam_client = boto3.client('iam')\n",
    "role_name = \"BedrockModelImportExecutionRole\"\n",
    "policy_name = \"BedrockModelImportPolicy\"\n",
    "s3_bucket_name = \"mistral-7b-insurance-bedrock-import\"  # Replace with your actual bucket name\n",
    "\n",
    "# Define the trust policy to allow Bedrock to assume this role with specific conditions\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": source_account  # Parameterized account ID\n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{source_account}:model-import-job/*\"  # Parameterized region and account ID\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define the permissions policy for S3 and Bedrock access\n",
    "permissions_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}\",\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModel\",\n",
    "                \"bedrock:GetModel\",\n",
    "                \"bedrock:ListModels\",\n",
    "                \"bedrock:CreateModelImportJob\",\n",
    "                \"bedrock:GetModelImportJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    print(\"Creating IAM Role...\")\n",
    "    role_response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=\"Role for Amazon Bedrock model import job with S3 access\"\n",
    "    )\n",
    "    role_arn = role_response['Role']['Arn']\n",
    "    print(f\"IAM Role created with ARN: {role_arn}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(f\"Role '{role_name}' already exists.\")\n",
    "        role_arn = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Attach the permissions policy to the role\n",
    "try:\n",
    "    print(\"Attaching policy to IAM Role...\")\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=policy_name,\n",
    "        PolicyDocument=json.dumps(permissions_policy)\n",
    "    )\n",
    "    print(\"Policy attached successfully.\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error attaching policy: {e}\")\n",
    "    raise\n",
    "\n",
    "# The role ARN will be used in the next cell to create the model import job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec79f7da-29c8-4c50-baca-69c0a92625fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model import job created: {'ResponseMetadata': {'RequestId': '54057d46-a7c8-40c0-8647-e5e31cbdf7dc', 'HTTPStatusCode': 201, 'HTTPHeaders': {'date': 'Tue, 05 Nov 2024 21:49:04 GMT', 'content-type': 'application/json', 'content-length': '81', 'connection': 'keep-alive', 'x-amzn-requestid': '54057d46-a7c8-40c0-8647-e5e31cbdf7dc'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:bedrock:us-west-2:603555443475:model-import-job/01252npqffte'}\n",
      "{\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"54057d46-a7c8-40c0-8647-e5e31cbdf7dc\",\n",
      "        \"HTTPStatusCode\": 201,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Tue, 05 Nov 2024 21:49:04 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"81\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"54057d46-a7c8-40c0-8647-e5e31cbdf7dc\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"jobArn\": \"arn:aws:bedrock:us-west-2:603555443475:model-import-job/01252npqffte\"\n",
      "}\n",
      "CPU times: user 22.2 ms, sys: 0 ns, total: 22.2 ms\n",
      "Wall time: 616 ms\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "bedrock_client = boto3.client('bedrock', region_name=region)  # Use the parameterized region\n",
    "s3_model_uri = f\"s3://{s3_bucket_name}/{s3_model_directory}/\"  # Reference the bucket and directory\n",
    "imported_model_name = \"Mistral-7B-Insurance-Model\"\n",
    "\n",
    "# Use the IAM role ARN created in the previous cell\n",
    "job_name = f\"mistral-7b-insurance-import-job-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock_client.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,  # Use the ARN from the IAM role created in the previous cell\n",
    "    modelDataSource={'s3DataSource': {'s3Uri': s3_model_uri}}\n",
    ")\n",
    "\n",
    "print(\"Model import job created:\", response)\n",
    "print(json.dumps(response, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4259592e-43c2-4598-8536-9478e583920b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking status for job mistral-import-job-20241105214904 every 30 seconds...\n",
      "Current status: Completed\n",
      "Job mistral-import-job-20241105214904 finished with status: Completed\n",
      "Imported Model ARN: arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll\n",
      "Model ID (ARN) for further use: arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll\n",
      "CPU times: user 8.92 ms, sys: 5.88 ms, total: 14.8 ms\n",
      "Wall time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Use the job name from the response of create_model_import_job to track the job\n",
    "polling_interval = 30  # Time in seconds between each status check\n",
    "\n",
    "def check_job_status(job_name):\n",
    "    \"\"\"\n",
    "    Checks the status of the model import job and returns the current status, failure message, and imported model ARN if available.\n",
    "\n",
    "    Parameters:\n",
    "        job_name (str): The name of the model import job to check.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the status, failure message, and imported model ARN if the job is completed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        status_response = bedrock_client.get_model_import_job(jobIdentifier=job_name)\n",
    "        return {\n",
    "            \"status\": status_response[\"status\"],\n",
    "            \"failureMessage\": status_response.get(\"failureMessage\", \"\"),\n",
    "            \"importedModelArn\": status_response.get(\"importedModelArn\", None)\n",
    "        }\n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop to check the job status periodically\n",
    "print(f\"Checking status for job {job_name} every {polling_interval} seconds...\")\n",
    "imported_model_arn = None\n",
    "while True:\n",
    "    result = check_job_status(job_name)\n",
    "    if result is None:\n",
    "        print(\"Unable to retrieve job status. Exiting.\")\n",
    "        break\n",
    "\n",
    "    status = result[\"status\"]\n",
    "    failure_message = result[\"failureMessage\"]\n",
    "    imported_model_arn = result[\"importedModelArn\"]\n",
    "    print(f\"Current status: {status}\")\n",
    "\n",
    "    # Check if the job has reached a final state\n",
    "    if status in [\"Completed\", \"Failed\"]:\n",
    "        if status == \"Failed\" and failure_message:\n",
    "            print(f\"Job failed with message: {failure_message}\")\n",
    "            imported_model_arn = None  # Clear the ARN if the job failed\n",
    "        else:\n",
    "            print(f\"Job {job_name} finished with status: {status}\")\n",
    "            print(f\"Imported Model ARN: {imported_model_arn}\")\n",
    "        break\n",
    "\n",
    "    # Wait before the next status check\n",
    "    time.sleep(polling_interval)\n",
    "\n",
    "# Set the model ID to the imported model ARN if the job was successful\n",
    "if imported_model_arn:\n",
    "    imported_model_id = imported_model_arn  # Assign the model ARN to model_id for further use\n",
    "    print(f\"Model ID (ARN) for further use: {model_id}\")\n",
    "else:\n",
    "    print(\"Model import job did not complete successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac39fe-7012-449b-95dc-faf60c391443",
   "metadata": {},
   "source": [
    "## Section 12: Call Imported Model Using Amazon Bedrock Converse API\n",
    "\n",
    "Send a test request to the imported model on Amazon Bedrock using the Converse API to verify its functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a297cea-86d0-45bd-84bf-2b8d25376fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation:\n",
      "User: [\n",
      "  {\n",
      "    \"text\": \"You are an expert in customer support for insurance. Please help me understand my health insurance benefits.\"\n",
      "  }\n",
      "]\n",
      "Response: To effectively understand your health insurance benefits, please adhere to the following steps:\n",
      "\n",
      "1. Access our website at {{WEBSITE_URL}}.\n",
      "2. Enter your login credentials to access your account.\n",
      "3. Proceed to the {{HEALTH_INSURANCE_SECTION}} section of your account.\n",
      "4. Click on the {{VIEW_DETAILS_TAB}} tab to review your health insurance details.\n",
      "\n",
      "Should you require additional support, please reach out to our customer service team via our helpline.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=\"us-west-2\")  # Replace with your region\n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "# imported_model_id should be the imported model's ARN (importedModelArn)\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define the conversation messages, with user role correctly structured\n",
    "# Add the system-like instruction as part of the initial user message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"You are an expert in customer support for insurance. Please help me understand my health insurance benefits.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the converse function\n",
    "def converse(messages):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock converse API without a system message.\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): List of conversation messages.\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response from Bedrock.\n",
    "    \"\"\"\n",
    "    # Configure the conversation payload\n",
    "    converse_config = {\n",
    "        \"modelId\": imported_model_id,  # Use the imported model ARN as the model ID\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": 0.5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nConversation:\")\n",
    "    for message in messages:\n",
    "        print(f\"{message['role'].capitalize()}: {json.dumps(message['content'], indent=2)}\")\n",
    "    \n",
    "    # Call the converse API\n",
    "    try:\n",
    "        response = bedrock_runtime_client.converse(**converse_config)\n",
    "        return response\n",
    "    except ClientError as e:\n",
    "        error_message = e.response['Error']['Message']\n",
    "        print(f\"An error occurred: {error_message}\")\n",
    "        print(\"Converse config:\")\n",
    "        print(json.dumps(converse_config, indent=2))\n",
    "        return None\n",
    "\n",
    "def print_converse_response(response):\n",
    "    \"\"\"\n",
    "    Prints the conversation response in a readable format.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): The API response from Bedrock.\n",
    "    \"\"\"\n",
    "    if response:\n",
    "        print(f\"Response: {response['output']['message']['content'][0]['text']}\")\n",
    "        if 'trace' in response:\n",
    "            print(\"Trace:\")\n",
    "            print(json.dumps(response['trace'], indent=2))\n",
    "    else:\n",
    "        print(\"No response received.\")\n",
    "\n",
    "# Example usage\n",
    "# Run the converse function and print the response\n",
    "response = converse(messages)\n",
    "print_converse_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6f89e-e69d-47e2-b138-3103ccbe76dc",
   "metadata": {},
   "source": [
    "## Section 13: Call Imported Model Using Amazon Bedrock Converse Streaming API\n",
    "\n",
    "This section demonstrates how to use the Amazon Bedrock converse_stream API to handle multiple test cases in real-time. Each message in the sample set represents a unique query to simulate different customer support scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41005eb4-5186-426b-9a9d-7c94dc48ed06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #1: Can you help me understand my health insurance benefits?\n",
      "\n",
      "Role: assistant\n",
      "To effectively utilize your health insurance benefits, please adhere to the following guidelines:\n",
      "\n",
      "1. Access your account via {{WEBSITE_URL}}.\n",
      "2. Locate the {{HEALTH_INSURANCE_SECTION}} within your account dashboard.\n",
      "3. Select the {{CLAIM_FORM}} option to initiate the claim process.\n",
      "4. Complete the claim form with all necessary information, ensuring that you include your policy number, the details of your claim, and any relevant documentation.\n",
      "5. Double-check all entered information for accuracy.\n",
      "6. Submit your claim form by sending it to the designated claims department as indicated on the website.\n",
      "\n",
      "After you have submitted your claim, please be patient as our team processes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " your request and responds to you in a timely manner.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 19\n",
      "Output tokens: 171\n",
      "Total tokens: 190\n",
      "Latency: 2642 milliseconds\n",
      "\n",
      "Finished streaming response for sample #1: Can you help me understand my health insurance benefits?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #2: What does my policy cover if I need to see a specialist?\n",
      "\n",
      "Role: assistant\n",
      " To ascertain the details of your insurance coverage for visiting a specialist, please adhere to the following guidelines:\n",
      "\n",
      "1. Access your account at {{WEBSITE_URL}}.\n",
      "2. Locate the section pertaining to {{HEALTH_INSURANCE_SECTION}}.\n",
      "3. Select the option for {{SPECIALIST_CONSULTATIONS_OPTION}}.\n",
      "4. Analyze the specifics of your coverage as outlined in the provided information.\n",
      "\n",
      "Should you require additional clarification or assistance, please do not hesitate to reach out to our customer support team."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 22\n",
      "Output tokens: 127\n",
      "Total tokens: 149\n",
      "Latency: 1902 milliseconds\n",
      "\n",
      "Finished streaming response for sample #2: What does my policy cover if I need to see a specialist?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #3: Are dental treatments covered in my current insurance plan?\n",
      "\n",
      "Role: assistant\n",
      " To verify whether your dental treatments are covered by your insurance plan, please adhere to the following guidelines:\n",
      "\n",
      "1. Access your account at {{WEBSITE_URL}}.\n",
      "2. Proceed to the {{COVERAGE_SECTION}} section.\n",
      "3. Choose your dental insurance policy from the displayed options.\n",
      "4. Select the {{COVERAGE_DETAILS}} link to review the dental treatments that are included in your coverage.\n",
      "\n",
      "If you require additional support, please reach out to our customer service"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " team by dialing our support number.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 19\n",
      "Output tokens: 119\n",
      "Total tokens: 138\n",
      "Latency: 2017 milliseconds\n",
      "\n",
      "Finished streaming response for sample #3: Are dental treatments covered in my current insurance plan?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #4: How do I file a claim for a recent doctor visit?\n",
      "\n",
      "Role: assistant\n",
      " To initiate the claim process for your recent doctor visit, please adhere to the following guidelines:\n",
      "\n",
      "1. Access our website at {{WEBSITE_URL}}.\n",
      "2. Enter your account credentials to log in.\n",
      "3. Proceed to the {{CLAIM_SECTION}} section of the site.\n",
      "4. Choose the option for {{FILE_CLAIM_OPTION}}.\n",
      "5. Complete the claim form, ensuring all mandatory fields are filled in and necessary documents are attached.\n",
      "6. Verify your information and submit the claim form.\n",
      "\n",
      "Our claims department will examine your submission and"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " respond at the earliest opportunity.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 21\n",
      "Output tokens: 134\n",
      "Total tokens: 155\n",
      "Latency: 2158 milliseconds\n",
      "\n",
      "Finished streaming response for sample #4: How do I file a claim for a recent doctor visit?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #5: Can you explain what deductible means in my policy?\n",
      "\n",
      "Role: assistant\n",
      " A deductible is the amount of money you are required to pay out of pocket before your insurance coverage begins. This means that in the event of a claim, you will be responsible for paying the specified deductible amount before your insurance provider will cover the remaining costs. For instance, if your policy has a $500 deductible, you will need to pay $500 before your insurance provider will cover any further expenses related to your claim.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 21\n",
      "Output tokens: 95\n",
      "Total tokens: 116\n",
      "Latency: 1565 milliseconds\n",
      "\n",
      "Finished streaming response for sample #5: Can you explain what deductible means in my policy?\n",
      "\n",
      "Finished streaming all test cases with model arn:aws:bedrock:us-west-2:603555443475:imported-model/39kzslj1khll.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=\"us-west-2\")  # Replace with your region\n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define multiple conversation samples without system prompts\n",
    "sample_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you help me understand my health insurance benefits?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"What does my policy cover if I need to see a specialist?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Are dental treatments covered in my current insurance plan?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"How do I file a claim for a recent doctor visit?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you explain what deductible means in my policy?\"}]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Inference parameters\n",
    "inference_config = {\"temperature\": 0.5}\n",
    "additional_model_fields = {\"top_k\": 200}\n",
    "\n",
    "# Define the streaming converse function\n",
    "def stream_conversation(bedrock_client, model_id, messages, inference_config, additional_model_fields):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock converse_stream API and handles streaming response.\n",
    "\n",
    "    Parameters:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (list): The messages to send.\n",
    "        inference_config (dict): The inference configuration to use.\n",
    "        additional_model_fields (dict): Additional model fields to use.\n",
    "    \"\"\"\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "# Example usage of streaming for multiple test cases\n",
    "try:\n",
    "    for i, messages in enumerate(sample_messages, 1):\n",
    "        print(\"\\n\" + \"=\"*50)  # Line separator for clarity\n",
    "        print(f\"\\nStarting streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "        stream_conversation(\n",
    "            bedrock_runtime_client,\n",
    "            imported_model_id,  # Use the imported model ARN as model ID\n",
    "            messages,\n",
    "            inference_config,\n",
    "            additional_model_fields\n",
    "        )\n",
    "        print(f\"\\nFinished streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "except ClientError as err:\n",
    "    error_message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", error_message)\n",
    "    print(\"A client error occurred: \" + format(error_message))\n",
    "else:\n",
    "    print(f\"\\nFinished streaming all test cases with model {imported_model_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0629cee-f853-4355-97c9-525507a4967a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 4.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-311-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
