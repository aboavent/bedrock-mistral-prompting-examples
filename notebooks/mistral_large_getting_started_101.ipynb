{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Mistral Models on Amazon Bedrock \n",
    "\n",
    "This notebook will walk you through how to get started with Mistral Large on Bedrock and cover prompting 101.\n",
    "\n",
    "The notebook was tested using the Data Science 3.0 kernel in SageMaker Studio.\n",
    "\n",
    "# Amazon Bedrock \n",
    "[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that provides access to a wide range of powerful foundation models (FMs) through a unified API. It offers models from leading AI companies like Mistral, Anthropic, AI21 Labs, Cohere, Stability AI, and Amazon's own Titan models.\n",
    "\n",
    "## Mistral Large 2\n",
    "[Mistral Large](https://mistral.ai/news/mistral-large-2407/) is the most advanced language model developed by French AI startup Mistral AI. It also has support for function calling and JSON format.\n",
    "\n",
    "Max tokens: 128k\n",
    "\n",
    "Languages: Natively fluent in French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean\n",
    "\n",
    "Supported use cases: precise instruction following, text summarization, translation, complex multilingual reasoning tasks, math and coding tasks including code generation\n",
    "\n",
    "## Mistral 7B\n",
    "[Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) is a 7B dense Transformer, fast-deployed and easily customizable. Small, yet powerful for a variety of use cases.\n",
    "\n",
    "Max tokens: 8K\n",
    "\n",
    "Languages: English\n",
    "\n",
    "Supported use cases: Text summarization, structuration, question answering,\n",
    "and code completion\n",
    "\n",
    "## Mixtral 8x7B\n",
    "[Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/) is 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral\n",
    "7B. Uses 12B active parameters out of 45B total\n",
    "\n",
    "Max tokens: 32K\n",
    "\n",
    "Languages: English, French, German, Spanish, Italian\n",
    "\n",
    "Supported use cases: Text summarization, structuration, question answering,\n",
    "and code completion\n",
    "\n",
    "## Mistral Small\n",
    "[Mistral Small](https://mistral.ai/technology/) is a compact yet powerful language model from Mistral AI, designed for efficiency and low latency. Supports native function calling and JSON outputs.\n",
    "\n",
    "Max tokens: 32k\n",
    "\n",
    "Languages: English, French, German, Spanish, Italian\n",
    "\n",
    "Supported use cases: Text generation, Code generation, Classification, RAG, Conversation\n",
    "\n",
    "\n",
    "## Getting access to Mistral Large (or other Mistral AI models)\n",
    "In order to start using Mistral Large, make sure that you have access to it from the Bedrock console:\n",
    "\n",
    "1. Log in to the AWS Console and navigate to the Amazon Bedrock service\n",
    "\n",
    "2. From the Bedrock home page, click \"Get started\"\n",
    "\n",
    "3. On the left-hand navigation menu, click \"Model access\"\n",
    "\n",
    "4. On the right side of the page, click \"Manage model access\"\n",
    "\n",
    "5. Select the base models you would like access to from the list of available models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display_html\n",
    "import logging\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that for the majority of the motebook Mistral Large is being used. However, the prompting technqiues explained throughout the notebook also apply to other Mistral AI models. For more information, please check out the [Official Mistral AI Prompting Guide](https://docs.mistral.ai/guides/prompting_capabilities/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"mistral.mistral-large-2407-v1:0\"\n",
    "MISTRAL_7B = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "MIXTRAL = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "MISTRAL_SMALL = \"mistral.mistral-small-2402-v1:0\"\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\")\n",
    "model_id = DEFAULT_MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported parameters\n",
    "\n",
    "The Mistral AI models have the following [inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html).\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "- Temperature - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- Top P - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- Top K - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- Maximum Length - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- Stop sequences - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The Napoleonic Wars (1803-1815) were a series of global conflicts fought during the reign of Napoleon Bonaparte, Emperor of the French. Here's a brief history behind them:  1. **Rise of Napoleon**: The Napoleonic Wars are rooted in the aftermath of the French Revolution (1789-1799). Napoleon, a young and brilliant military strategist, rose to prominence during this period. He seized power in a coup d'Ã©tat in 1799, becoming First Consul and later crowning himself Emperor in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"<s>[INST]What is the history behind the Napoleonic Wars? [/INST]\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 128,\n",
    "            \"prompt\": prompt,\n",
    "            \"top_p\": 0.1,\n",
    "            \"top_k\": 2,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a prompt\n",
    "\n",
    "A prompt is a natural language instruction that guides a large language model (LLM) to perform a specific task effectively. Just like providing clear guidance to a skilled assistant, well-crafted prompts are crucial for leveraging the full potential of LLMs.\n",
    "\n",
    "An effective prompt should include clear instructions, context, examples, constraints, desired output format, and unbiased language. It may need iterative refinement based on the model's initial response. Appropriate length is also important for coherent and focused outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prompt w/o clear instructions and context</th>\n",
       "      <th>Prompt w/ clear instructions and context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The value of the cloud can be assessed through various benefits it offers to both businesses and individual users. Here are some of the key values:  1. **Cost Savings**:    - **CapEx to OpEx**: Cloud services allow businesses to shift from a capital expenditure model (buying and maintaining your own servers) to an operational expenditure model (paying for computing resources as you use them).    - **Scalability**: You can easily scale resources up or down based on demand, ensuring you only pay for what you use.  2. **Accessibility and Flexibility**:</td>\n",
       "      <td>The value of the cloud, particularly AWS, can be summarized through several key benefits that it brings to enterprises:  ### Cost Efficiency - **Pay-as-You-Go Model**: AWS allows businesses to pay only for the resources they use, eliminating the need for upfront capital expenditures. - **Economies of Scale**: AWS's massive scale allows it to achieve higher economies of scale, which translates to lower pay-as-you-go prices.  ### Scalability and Elasticity - **Auto-Scaling**: AWS services like EC2 Auto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the prompts\n",
    "query = \"What is the value of the cloud?\"\n",
    "prompt_1 = f\"<s>[INST]{query} [/INST]\"\n",
    "prompt_2 = f\"\"\"<s>[INST]You are an Amazon Web Services (AWS) Expert. \n",
    "Your role is to answer every technical question about AWS as accurately as possible.\n",
    "If you do not know the answer to a question, say I don't know. Give statistics \n",
    "around usage in the Enterprise world.{query} [/INST]\"\"\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 128,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Prompt w/o clear instructions and context\": [responses[0]],\n",
    "    \"Prompt w/ clear instructions and context\": [responses[1]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering \n",
    "Prompt engineering techniques like zero-shot, few-shot, and chain-of-thought prompting can further enhance the quality and behavior of model outputs. Zero-shot relies on the model's general knowledge, few-shot provides examples, and chain-of-thought encourages step-by-step reasoning.\n",
    "\n",
    "Let's take a look at the different techniques in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting:\n",
    "- Zero-shot prompting involves providing a pre-trained language model with a prompt it hasn't seen during training and expecting it to generate a relevant output.\n",
    "- The model relies on its general language understanding and patterns learned during pre-training to produce an output, without any additional examples or fine-tuning.\n",
    "- Zero-shot prompting leverages the model's ability to perform tasks it hasn't been explicitly trained on, using only the information learned from a diverse range of sources.\n",
    "- It is useful in scenarios where the model does not have specific training examples for the given task.\n",
    "- However, zero-shot prompting offers less control over the output compared to few-shot prompting which will be discussed next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The sentiment of the text \"The movie was terrible, I hated every minute of it!\" is negative. The words \"terrible\" and \"hated\" convey a strong dislike and dissatisfaction, indicating a negative sentiment.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"<s>[INST]Classify the sentiment of the following text: The movie was terrible, I hated every minute of it!?[/INST]\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 128,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting:\n",
    "- Few-shot prompting involves providing a language model with a small number of examples (usually 2-5) demonstrating the desired task, along with the input prompt.\n",
    "- The examples serve as a conditioning context for the model, guiding it to generate a response similar to the provided examples.\n",
    "- Few-shot prompting enables the model to quickly adapt to new tasks by leveraging the patterns and structures provided in the examples.\n",
    "- It is more effective than zero-shot prompting for complex tasks and offers better control over the model's output.\n",
    "- The performance of few-shot prompting generally improves with larger model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1. new iPhone     2. excellent camera     3. long battery life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    <s>[INST]follow these examples to extract the keywords from the text.\n",
    "\n",
    "    Text: The hotel room was spacious and clean.\n",
    "    Keywords: hotel room, spacious, clean\n",
    "    Text: The actor's performance was unconvincing and dull. \n",
    "    Keywords: actor's performance, unconvincing, dull\n",
    "    Text: The new iPhone has an excellent camera and long battery life.\n",
    "    keywords:\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 128,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting:\n",
    "- CoT prompting is a technique that encourages language models to explain their reasoning process when solving complex problems.\n",
    "- It involves providing the model with a few examples that include step-by-step reasoning, demonstrating how to break down a problem into intermediate steps.\n",
    "- The model is then expected to follow a similar \"chain of thought\" when answering the prompt, explaining its reasoning process.\n",
    "- CoT prompting is particularly effective for tasks that require arithmetic reasoning, commonsense reasoning, and symbolic reasoning.\n",
    "- It has been shown to improve the performance of large language models on complex reasoning tasks compared to standard prompting.\n",
    "- CoT prompting is an emergent property of model scale, meaning its benefits are more pronounced in larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Let's denote Alice's current age as \\( A \\) and Betty's current age as \\( B \\).  According to the problem, the sum of their current ages is 42: \\[ A + B = 42 \\]  The problem also states that Alice is twice as old as Betty was when Alice was as old as Betty is now. Let's break this down:  1. Let \\( x \\) be the number of years ago when Alice was as old as Betty is now. 2. At that time, Alice's age was \\( A - x \\) and Betty's age was \\( B - x \\). 3. According to the problem, Alice's age at that time was equal to Betty's current age: \\[ A - x = B \\]  4. The problem also states that Alice is currently twice as old as Betty was at that time: \\[ A = 2(B - x) \\]  Now we have two equations: 1. \\( A + B = 42 \\) 2. \\( A = 2(B - x) \\)  From the first equation, we can express \\( A \\) in terms of \\( B \\): \\[ A = 42 - B \\]  Substitute \\( A = 42 - B \\) into the second equation: \\[ 42 - B = 2(B - x) \\]  We also know from the third equation that: \\[ A - x = B \\] Substitute \\( A = 42 - B \\) into this equation: \\[ 42 - B - x = B \\] \\[ 42 - 2B = x \\]  Now substitute \\( x = 42 - 2B \\) back into the equation \\( 42 - B = 2(B - x) \\): \\[ 42 - B = 2(B - (42 - 2B)) \\] \\[ 42 - B = 2(B - 42 + 2B) \\] \\[ 42 - B = 2(3B - 42) \\] \\[ 42 - B = 6B - 84 \\] \\[ 42 + 84 = 6B + B \\] \\[ 126 = 7B \\] \\[ B = 18 \\]  Now that we have \\( B \\), we can find \\( A \\): \\[ A = 42 - B \\] \\[ A = 42 - 18 \\] \\[ A = 24 \\]  So, Alice is currently 24 years old.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\" <s>[INST]Alice is twice as old as Betty was when Alice was as old as Betty is now.\n",
    "If the sum of their current ages is 42, how old is Alice now? Let's solve this step-by-step. [/INST]\"\"\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delimiters\n",
    "- Delimiters are a useful technique in prompt engineering for large language models (LLMs) that can improve response quality and accuracy.\n",
    "\n",
    "- Mistral AI models use delimiters like `###`, `<<<>>>` to specify boundaries between different sections of text.\n",
    "\n",
    "- Delimiters help clearly separate the instructions/task description from the actual input data.\n",
    "\n",
    "- Delimiters prevent prompt injection attacks by treating anything inside the delimiters as input to be processed according to the original instructions, rather than new directives.\n",
    "\n",
    "- By strictly delimiting user input, the model will not interpret the input as new instructions, enhancing security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>charge dispute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### User question\n",
    "question = \"I am messaging to find out what this random in flight WIFI charge is. I did not purchase anything else beside my ticket with my credit card. Could you please help understand this charge?\"\n",
    "\n",
    "### Prompt template\n",
    "prompt = f\"\"\"\n",
    "        <s>[INST]You are an airline customer service bot. Your task is to assess customer intent \n",
    "        and categorize customer question after <<<>>> into one of the following predefined categories:\n",
    "        \n",
    "        flight delay\n",
    "        lost luggage\n",
    "        seat upgrade\n",
    "        modify reservation\n",
    "        cancel reservation\n",
    "        charge dispute\n",
    "        \n",
    "        If the text doesn't fit into any of the above categories, classify it as:\n",
    "        customer service\n",
    "        \n",
    "        You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes. \n",
    "        \n",
    "        ####\n",
    "        Here are some examples:\n",
    "        \n",
    "        Question:How can I track the status of my delayed or lost luggage to ensure it will be delivered to me? I'm concerned about the process and would like to know what options are available to monitor the location of my bags. Are there any tracking systems or apps I can use to see if my luggage is on its way or has been lost? Please provide details on how airlines typically handle delayed baggage delivery and the expected timeframes for receiving my suitcase.\n",
    "        Category: lost luggage\n",
    "        Question: I have a reservation for a flight and would like to know if there is any additional information I can provide to make my reservation more convenient.\n",
    "        Category: modify reservation\n",
    "        Question: When is my flight arriving?I'm trying to catch my connection which is not runing behind. Could you please provide an update on my flight?\n",
    "        Category: flight delay \n",
    "        Question: I need help with my seat upgrade. I have a seat upgrade request but I'm having trouble finding the right seat.\n",
    "        Category: seat upgrade\n",
    "        Question: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "        Category: customer service\n",
    "        ###\n",
    "    \n",
    "        <<<\n",
    "        Question: {question}\n",
    "        >>>\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 512,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"Classification\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction Templates and Stop Sequences\n",
    " You can also add an instruction template that further guides the model to generate the response that you're looking for. Instruction templates are extra helpful when dealing with chatbot systems. For the  Mistral models, you can use the following instruction template: \n",
    "\n",
    "`<s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]`\n",
    "\n",
    "The model uses the `[INST]` and `[/INST]` 'tags' to identify the instructions. It's important to note that there should be a space after the closing [/INST] tag. If you don't include this space, the model will likely generate a space at the beginning of its response.\n",
    "\n",
    "If you want to deep dive into the special tokes, please check out the [Mistral documentation](https://docs.mistral.ai/guides/tokenization/#what-is-tokenization). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response w/o instruction template (1)</th>\n",
       "      <th>Response w/ instruction template and w/ space (2)</th>\n",
       "      <th>Response w/ instruction template and w/o space (3)</th>\n",
       "      <th>Response w/ instruction template, w/ space, and w/ follow up instructions (4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2. The sky is not blue.  Comment: @SandeepGusain, I'm not sure what you're trying to say.  Comment: @SandeepGusain I think you are trying to say that the first statement is not necessarily true, so the second statement is not necessarily false. I would say that the first statement is necessarily true, and the second statement is necessarily false.  Comment: @SandeepGusain, I'm not sure what you are trying to say.  ## Answer (1)  The first statement is necessarily true, and the second statement is necessarily false.  Comment: I don't know why you were downvoted. This is the correct answer.  Comment: @JoeZeng, I don't know either.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted because of the comment that I made.  Comment: @JoeZeng, I was downvoted</td>\n",
       "      <td>ð That's correct! The sky appears blue due to a process called Rayleigh scattering, where the Earth's atmosphere scatters more blue sunlight towards our eyes than any other color. Isn't nature fascinating?</td>\n",
       "      <td>1. The problem states: \"The sky is blue.\" 2. This is a declarative statement that asserts a fact. 3. The statement does not contain any mathematical content to solve or prove. 4. Therefore, there is no mathematical solution to provide.  Conclusion: The statement \"The sky is blue\" is a factual statement about the color of the sky.</td>\n",
       "      <td>ð The color of the sky can vary:  * Daytime: Blue (due to a process called Rayleigh scattering) * Sunrise/Sunset: Shades of red, orange, or pink * Nighttime: Black or deep blue (with stars, planets, and other celestial bodies visible)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"The sky is blue.\"\n",
    "prompt_1 = f\"\"\" {query} \"\"\"\n",
    "prompt_2 = f\"\"\" <s>[INST] {query} [/INST] \"\"\"\n",
    "prompt_3 = f\"\"\" <s>[INST] {query}[/INST] \"\"\"\n",
    "prompt_4 = f\"\"\"<s>[INST]{query} [/INST] No, it is red.</s> [INST] What color is the sky? [/INST] \"\"\"\n",
    "prompts = [prompt_1, prompt_2,prompt_3,prompt_4]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 512,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response w/o instruction template (1)\": [responses[0]],\n",
    "    \"Response w/ instruction template and w/ space (2)\": [responses[1]],\n",
    "    \"Response w/ instruction template and w/o space (3)\": [responses[2]],\n",
    "    \"Response w/ instruction template, w/ space, and w/ follow up instructions (4)\": [responses[3]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responses in column #1 suggest that the model may have struggled to understand the intended task or instruction, resulting in a response that, while potentially honest and harmless, lacked helpfulness or relevance. This could be due to the model's limitations in recognizing when to stop generating output.\n",
    "\n",
    "In contrast, the response in column #2 appears to be helpful, harmless, and honest. The model seems to have correctly interpreted the statement as an instruction, prompting it to provide a justification for the given statement.\n",
    "\n",
    "The difference in column #3 is primarily in the formatting, as the response includes an empty space at the beginning, which is an artifact of the model's output.\n",
    "\n",
    "Lastly, in column #4, the model appears to have struggled with the instruction and response, along with the follow-up. As a result, it grounded its knowledge by referencing a source labeled as \"Common Knowledge,\" potentially to provide additional context or support for its response.\n",
    "\n",
    "These examples illustrate how the use of instruction templates and formatting can guide and customize the language model's responses. Explicit instructions can steer the model towards specific tasks, such as question-answering, while the inclusion of spaces and follow-ups can enable more natural conversational flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Prompting Issues and How to Overcome them \n",
    "When writing prompts for Large Language Models, there are common pitfalls one may encounter that can impact the quality and accuracy of the model's outputs. Effective prompt engineering is crucial to mitigate these issues and unlock the full potential of LLMs. Some of the most prevalent challenges include:\n",
    "\n",
    "Hallucination and factual inaccuracies: LLMs may generate outputs that contain false or made-up information, particularly when asked about topics outside their training data.\n",
    "\n",
    "Lack of coherence and logical reasoning: The generated text may suffer from logical inconsistencies, contradictions, or lack a coherent structure, especially in complex, multi-step tasks.\n",
    "\n",
    "Difficulty with complex, multi-step tasks: LLMs may struggle to maintain context and generate consistent outputs when faced with intricate, multi-step prompts that require reasoning and problem-solving abilities.\n",
    "\n",
    "Misunderstanding user intent: The model may misinterpret the user's intended meaning or goal, leading to irrelevant or off-topic responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hallucination and factual inaccuracies\n",
    "\n",
    "Hallucination and factual inaccuracies are an issue that can arise when LLM produce text outputs. Since these models are trained on vast amounts of data from the internet and other sources, their knowledge can sometimes be incomplete, biased, or simply incorrect.\n",
    "\n",
    "Solution: Provide clear instructions in the prompt and add any additional context.  In the example below, the context variable acts as the factual information to ground the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response w/o context</th>\n",
       "      <th>Response with context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>\"Project Cobra\" can refer to several different initiatives depending on the context. Here are a few possibilities:  1. **Military Operations**: In military contexts, \"Project Cobra\" might refer to specific operations or initiatives. For example, Operation Cobra was a World War II offensive by the United States during the Normandy campaign.  2. **Technology Projects**: In the tech industry, \"Project Cobra\" could be a codename for a new product, software, or research initiative. Companies often use codenames to keep projects confidential until they are ready for public announcement.</td>\n",
       "      <td>Project Cobra is an ambitious endeavor focused on developing a method of interstellar travel by creating and manipulating closed timelike curves (CTCs). These CTCs are paths through spacetime that loop back on themselves, enabling travel to the past. The project aims to achieve this by generating traversable wormholes stabilized with exotic matter and then accelerating one mouth of the wormhole to near the speed of light to induce CTCs inside. This would allow a spacecraft to travel through the wormhole and emerge in its own past, effectively appearing to an outside observer to have moved faster than light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the prompts\n",
    "query = \"What is project Cobra?\"\n",
    "context = \"\"\"\n",
    "Project Cobra aims to develop a method of interstellar travel by creating and manipulating closed timelike curves (CTCs) - paths through spacetime that loop back on themselves, allowing travel to the past. The theoretical basis comes from solutions to Einstein's equations of general relativity that permit CTCs in unusual spacetime geometries like wormholes.\n",
    "\n",
    "To achieve this, Project Cobra plans to generate traversable wormholes stabilized with exotic matter, then accelerate one wormhole mouth to near lightspeed to induce CTCs inside. This would allow a spacecraft to travel through the wormhole and exit in its own past, appearing to an outside observer to have moved faster than light.\n",
    "\n",
    "Major challenges include creating and stabilizing the wormholes, accelerating them to form the time machine, navigating through them precisely, and avoiding paradoxes from changing the past.\n",
    "\n",
    "If successful, it could revolutionize interstellar travel by providing shortcuts through spacetime. However, it also carries huge risks of misuse or unintended consequences that would require stringent protocols and safety measures.\n",
    "\n",
    "In essence, it is an extremely ambitious endeavor aiming to translate predictions of general relativity into functional time travel technology for reaching distant stars, albeit with immense theoretical and engineering hurdles to overcome.\"\"\"\n",
    "\n",
    "prompt_1 = f\"<s>[INST]{query} [/INST]\"\n",
    "prompt_2 = f\"\"\"<s>[INST]Using the conext given below, answer the question: {query}\n",
    "                Provided context: {context} [/INST]\"\"\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 128,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response w/o context\": [responses[0]],\n",
    "    \"Response with context\": [responses[1]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How to improve coherence in responses \n",
    "Sometimes the response the model gives doesn't make sense or follow a clear logic. This happens because the model is not told to explain its thinking step-by-step. When asked a simple question, the model tries to give a direct answer without showing how it got there. This can lead to confusing or illogical responses.\n",
    "\n",
    "Solution: Use chain-of-thought prompting, where the model is prompted to break down its reasoning into a series of logical steps before providing a final answer. This can significantly improve performance on tasks requiring multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response w/o CoT</th>\n",
       "      <th>Response w/ CoT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Let's denote the number of blue chairs as B and the number of red chairs as R. According to the problem, the ratio of blue chairs to red chairs is 2:3. This means that for every 2 blue chairs, there are 3 red chairs.  We can express this relationship as: B/R = 2/3  We also know that the total number of chairs is 30: B + R = 30  Now, we can solve these equations to find the number of blue chairs.  From the ratio B/R = 2/3, we can express B in terms of R: B = (2/3) * R  Substitute this expression for B into the total number of chairs equation: (2/3) * R + R = 30  To combine the terms, we need a common denominator, which is 3: (2/3) * R + (3/3) * R = 30 (2R + 3R) / 3 = 30 5R / 3 = 30  Now, multiply both sides by 3 to get rid of the denominator: 5R = 90  Divide both sides by 5 to solve for R: R = 90 / 5 R = 18  Now that we know there are 18 red chairs, we can find the number of blue chairs using the ratio B/R = 2/3: B = (2/3) * R B = (2/3) * 18 B = 12  So, there are 12 blue chairs in the classroom.</td>\n",
       "      <td>Let's think through this step-by-step:  1. We know that for every group of 5 chairs (2 blue + 3 red), the ratio of blue to red chairs is maintained. 2. We have a total of 30 chairs in the classroom. 3. To find out how many groups of 5 chairs there are, we divide the total number of chairs by the size of one group: 30 chairs / 5 chairs/group = 6 groups. 4. Since there are 2 blue chairs in each group, we can find the total number of blue chairs by multiplying the number of groups by the number of blue chairs per group: 6 groups * 2 blue chairs/group = 12 blue chairs.  So, there are 12 blue chairs in the classroom.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the prompts\n",
    "query = \"\"\"A classroom has two blue chairs for every three red chairs.\n",
    " If there are a total of 30 chairs in the classroom, how many blue chairs are there?\"\"\"\n",
    "\n",
    "prompt_1 = f\"<s>[INST]{query} [/INST]\"\n",
    "prompt_2 = f\"\"\"<s>[INST]Think-step-by-step and answer the following question: {query} [/INST]\"\"\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 512,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response w/o CoT\": [responses[0]],\n",
    "    \"Response w/ CoT\": [responses[1]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How to tackle complex, multi-step tasks\n",
    "When faced with complex, multi-step tasks, prompting can be a significant challenge. The issue lies in the model's ability to comprehend and tackle tasks that require multiple steps, conditional logic, and critical thinking. Without proper guidance, the model may struggle to identify the necessary steps, prioritize tasks, or even understand the context of the problem. This can lead to incomplete, inaccurate, or irrelevant responses. Furthermore, complex tasks often require a deep understanding of the problem domain, making it difficult for the model to generate a coherent and logical solution.\n",
    "\n",
    "Solution: Decompose the complex task into a sequence of simpler sub-tasks in the prompt. Guide the model to solve each sub-task step-by-step. Least-to-most prompting, where sub-problems are solved in order of increasing difficulty, can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>### Step 1: Choose Recipes  **Appetizer:** Caprese Skewers - Ingredients: Cherry tomatoes, fresh mozzarella balls, fresh basil leaves, balsamic glaze, olive oil, salt, pepper  **Main Course:** Chicken Parmesan - Ingredients: 6 boneless, skinless chicken breasts, 2 cups breadcrumbs, 1 cup grated Parmesan cheese, 2 eggs, 1 cup all-purpose flour, 1 cup marinara sauce, 2 cups shredded mozzarella cheese, 1/2 cup olive oil, salt, pepper, garlic powder  **Dessert:** Tiramisu - Ingredients: 2 cups mascarpone cheese, 1/2 cup granulated sugar, 1 tsp vanilla extract, 1 cup heavy cream, 2 cups strong brewed coffee, 1/4 cup rum, 24 ladyfingers, cocoa powder for dusting  ### Step 2: Grocery List  **Produce:** - Cherry tomatoes - Fresh basil leaves - Garlic (for garlic powder)  **Dairy:** - Fresh mozzarella balls - Mascarpone cheese - Heavy cream - Eggs  **Pantry:** - Balsamic glaze - Olive oil - Salt - Pepper - Breadcrumbs - Grated Parmesan cheese - All-purpose flour - Marinara sauce - Shredded mozzarella cheese - Granulated sugar - Vanilla extract - Strong brewed coffee - Rum - Ladyfingers - Cocoa powder  **Meat:** - 6 boneless, skinless chicken breasts  ### Step 3: Cooking Utensils, Pots/Pans, Bakeware Needed  **Appetizer:** - Skewers - Small bowl for balsamic glaze - Cutting board - Knife  **Main Course:** - Large skillet - 3 shallow bowls (for breading station) - Baking dish - Measuring cups and spoons - Whisk  **Dessert:** - Mixing bowls - Electric mixer - 9x13-inch baking dish - Spatula - Fine-mesh sieve (for dusting cocoa powder)  ### Step 4: Timeline to Prep Each Course  **1 hour before serving:** - Prepare Caprese Skewers: Assemble skewers with cherry tomatoes, mozzarella, and basil. Drizzle with balsamic glaze and olive oil. Season with salt and pepper. Refrigerate until serving.  **1.5 hours before serving:** - Prepare Chicken Parmesan:   - Preheat oven to 450Â°F (230Â°C).   - Set up breading station with flour, beaten eggs, and breadcrumbs mixed with Parmesan cheese.   - Coat chicken breasts in flour, dip in eggs, and coat with breadcrumb mixture.   - Heat olive oil in a large skillet and cook chicken until golden brown on both sides.   - Transfer chicken to a baking dish, top with marinara sauce and shredded mozzarella.   - Bake for 20-25 minutes until chicken is cooked through and cheese is melted.  **2 hours before serving:** - Prepare Tiramisu:   - Brew strong coffee and let it cool.   - Mix mascarpone cheese, sugar, and vanilla extract until smooth.   - Whip heavy cream until stiff peaks form and fold into mascarpone mixture.   - Dip ladyfingers in coffee and rum mixture and arrange in a baking dish.   - Spread half of the mascarpone mixture over the ladyfingers.   - Repeat with another layer of ladyfingers and mascarpone mixture.   - Dust with cocoa powder and refrigerate until serving.  **Serving:** - Serve Caprese Skewers as an appetizer. - Serve Chicken Parmesan as the main course. - Serve Tiramisu for dessert.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompts\n",
    "query = \"The task is to plan a 3-course meal for 6 people.\"\n",
    "prompt = f\"\"\"<s>[INST]{query} Do not add any preamble.Using the following subtasks:\n",
    "            Step 1: Choose recipes for appetizer, main course, and dessert\n",
    "            Step 2: Make a grocery list of all required ingredients \n",
    "            Step 3: Determine cooking utensils, pots/pans, bakeware needed\n",
    "            Step 4: Come up with a timeline to prep each course [/INST]\"\"\"\n",
    "prompts = [prompt]\n",
    "\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response\": [responses[0]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Misunderstanding user intent\n",
    "\n",
    "Misunderstanding user intent is a common issue in prompting, where the model fails to grasp the user's underlying goals, needs, or requirements. This can lead to responses that are off-topic, irrelevant, or unhelpful. The root cause of this issue lies in the model's inability to understand the context and nuances of human communication, such as implied meaning, tone, and intent. Without clear guidance, the model may misinterpret the user's query, leading to a mismatch between the user's expectations and the model's response.\n",
    "\n",
    "Solution: Provide clear context and instructions in the prompt to guide the model. Use role prompting to define the model's persona and expertise to better address the user's needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>To plan a healthy 3-course meal for six people, remember these key principles: balance your macronutrients (carbohydrates, proteins, and fats) in each course, emphasize a variety of colorful fruits and vegetables for essential micronutrients, and opt for whole, unprocessed foods to maximize nutrient density. Current research supports the Mediterranean diet, which includes plenty of plant-based foods, lean proteins, and healthy fats, like olive oil and avocados. Additionally, portion control is crucial, so consider the recommended serving sizes to avoid overeating.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompts\n",
    "prompt_1 = f\"\"\"<s>[INST]Do not give a preamble.You are a nutritonist. The audience is adults looking to improve their eating habits.\n",
    " Provide a 3-4 sentence paragraph clearly explaining 2-3 key principles of healthy eating that are backed by current nutritional research. {query} [/INST]\"\"\"\n",
    "prompts = [prompt_1]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 512,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response\": [responses[0]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Mistral Large 2 vs Mixtral 8x7B vs Mistral Small\n",
    "\n",
    "## Mistral Large 2\n",
    "Mistral Large 2 is Mistral AI's latest large language model, designed for complex tasks requiring advanced reasoning, natural language understanding, and knowledge retrieval across multiple languages and domains.\n",
    "\n",
    "**Capabilities:**\n",
    "- Proficiency in 80+ coding languages, including Python, Java, C, C++, JavaScript, and Bash\n",
    "- Enhanced reasoning and problem-solving skills with reduced hallucination tendencies\n",
    "- Improved instruction-following and conversational abilities\n",
    "- Advanced function calling and retrieval skills for complex business applications\n",
    "\n",
    "**Performance:**\n",
    "- Achieves 84.0% accuracy on MMLU benchmark (pretrained version)\n",
    "- Outperforms previous Mistral Large model in code generation and reasoning tasks\n",
    "- Excels in multilingual performance, particularly in English, French, German, Spanish, Italian, Portuguese, Dutch,  Russian, Chinese, Japanese, Korean, Arabic, and Hindi\n",
    "\n",
    "**Applications:**\n",
    "- Code generation and software development\n",
    "- Complex reasoning and problem-solving tasks\n",
    "- Multilingual document processing and analysis\n",
    "\n",
    "## Mixtral 8x7B\n",
    "\n",
    "Mixtral 8x7B is a mixture-of-experts (MoE) model, designed for efficient and fast processing of specific tasks and domains.\n",
    "\n",
    "**Capabilities:**\n",
    "- Efficient and fast processing for real-time applications\n",
    "- Specialized for specific tasks and domains (e.g., customer support, language translation)\n",
    "- Handling simple to moderately complex prompts\n",
    "- Basic NLU and contextual understanding\n",
    "\n",
    "**Performance:**\n",
    "- Fast and efficient processing for real-time applications\n",
    "- Good performance in specific tasks and domains\n",
    "- Moderate accuracy in basic NLU and contextual understanding\n",
    "\n",
    "**Applications:**\n",
    "- Customer support and chatbots\n",
    "- Language translation and localization\n",
    "- Data analysis and processing\n",
    "\n",
    "## Mistral Small\n",
    "\n",
    "Mistral Small is an optimized model designed for low-latency workloads, striking a balance between performance and efficiency.\n",
    "\n",
    "**Capabilities:**\n",
    "- Efficient processing for real-time applications\n",
    "- Handling simple to moderately complex prompts\n",
    "- Basic NLU and contextual understanding\n",
    "- Suitable for high-volume, low-complexity tasks\n",
    "\n",
    "**Performance:**\n",
    "- Fast and efficient processing for real-time applications\n",
    "- Good performance in simple to moderate tasks\n",
    "- Lower accuracy compared to Mistral Large and Mixtral 8x7B\n",
    "\n",
    "**Applications:**\n",
    "- Customer support and chatbots\n",
    "- Text classification and sentiment analysis\n",
    "- Data extraction and information retrieval\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**Inference Speed:** Mixtral 8x7B and Mistral Small are significantly faster than Mistral Large during inference, making them suitable for real-time applications.\n",
    "\n",
    "**Prompting:** Mistral Large can handle more abstract and open-ended prompts, while Mixtral 8x7B and Mistral Small require more specific and guided prompting.\n",
    "\n",
    "**Error Handling:** Mistral Large is more robust to errors and can recover from mistakes more effectively.\n",
    "\n",
    "To summarize, Mistral Large is the most powerful and capable model, suitable for complex tasks and advanced applications, while Mixtral 8x7B and Mistral Small are optimized for efficient processing of specific tasks and real-time applications, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "The following prompts demonstrate the contrasting capabilities of the Mixtral 8x7b and Mistral Large models in terms of critical thinking and multilingual proficiency. While the analytical responses from Mixtral 8x7b are not inaccurate, they lack the depth and thoroughness exhibited by Mistral Large. Regarding the multilingual examples, Mistral Large's translations consider the contextual nuances rather than adhering to a literal word-for-word approach. This contextual awareness allows Mistral Large's translations to convey the intended meaning more precisely and coherently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mixtral 8x7b Multilingual Prompt</th>\n",
       "      <th>Mistral Large Multilinigual Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Spanish: \"El zorro pardo Ã¡gil salta sobre el perro perezoso.\"  French: \"Le renard brun rapide saute par-dessus le chien paresseux.\"  German: \"Der schnelle braune Fuchs springt Ã¼ber den faulen Hund.\"</td>\n",
       "      <td>Certainly! Here are the translations:  **Spanish:** \"El veloz zorro marrÃ³n salta sobre el perro perezoso.\"  **French:** \"Le rapide renard brun saute par-dessus le chien paresseux.\"  **German:** \"Der schnelle braune Fuchs springt Ã¼ber den faulen Hund.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompts\n",
    "prompt_1 = \"\"\"<s>[INST]Translate the following text to Spanish, French and German:\n",
    "             \"The quick brown fox jumps over the lazy dog. [/INST]\"\"\"\n",
    "prompts = [prompt_1]\n",
    "model_ids = [MIXTRAL,\n",
    "              DEFAULT_MODEL,\n",
    "              # MISTRAL_SMALL\n",
    "             ]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    for model_id in model_ids:\n",
    "        body = json.dumps({\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\": [\"</s>\"]\n",
    "        })\n",
    "        response = bedrock.invoke_model(\n",
    "        body = body,\n",
    "        modelId = model_id)\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Mixtral 8x7b Multilingual Prompt\": [responses[0]],\n",
    "    \"Mistral Large Multilinigual Prompt\": [responses[1]],\n",
    "    #\"Mistral Small Multilinigual Prompt\": [responses[1]],\n",
    "\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mixtral 8x7b Analytical Prompt</th>\n",
       "      <th>Mistral Large Analytical Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Correctness: The function is not correct because it does not handle the case when n is 2, returning 0 instead of 1. Also, for large values of n, the function will exceed the maximum recursion depth due to multiple nested calls.  Efficiency: The function's efficiency is very poor due to the repeated computation of Fibonacci numbers. For example, when calculating fibonacci(5), the function calls fibonacci(4) and fibonacci(3), which in turn call fibonacci(3) and fibonacci(2), and so on. This leads to an exponential time complexity.  To improve the efficiency, we can use dynamic programming to store and reuse the calculated Fibonacci numbers. Here's an optimized version of the function:  def fibonacci(n, memo={}): if n &lt;= 0: return 0 elif n == 1: return 1 elif n in memo: return memo[n] else: result = fibonacci(n-1, memo) + fibonacci(n-2, memo) memo[n] = result return result  This version has a time complexity of O(n) and a space complexity of O(n), making it much more efficient than the original implementation.</td>\n",
       "      <td>The function is correct in that it calculates the Fibonacci sequence. However, it's not efficient because it uses recursion to calculate each number in the sequence, which results in a lot of redundant calculations. For example, to calculate the 5th number in the sequence, it first calculates the 4th and 3rd numbers, but to calculate the 4th number, it already calculated the 3rd number. This redundancy leads to an exponential time complexity of O(2^n). A more efficient approach would be to use dynamic programming or iteration to calculate each number in the sequence only once.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompts\n",
    "\n",
    "prompt_1 = \"\"\"<s>[INST]Evaluate this Python function for correctness and efficiency:\n",
    "            def fibonacci(n):\n",
    "            if n <= 0:\n",
    "            return 0\n",
    "            elif n == 1:\n",
    "            return 1\n",
    "            else:\n",
    "            return fibonacci(n-1) + fibonacci(n-2) [/INST]\"\"\"\n",
    "prompts = [prompt_1]\n",
    "model_ids = [MIXTRAL,\n",
    "            DEFAULT_MODEL,\n",
    "            # MISTRAL_SMALL\n",
    "                ]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    for model_id in model_ids:\n",
    "        body = json.dumps({\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 514,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\": [\"</s>\"]\n",
    "        })\n",
    "        response = bedrock.invoke_model(\n",
    "        body = body,\n",
    "        modelId = model_id)\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Mixtral 8x7b Analytical Prompt\": [responses[0]],\n",
    "    \"Mistral Large Analytical Prompt\": [responses[1]],\n",
    "    # \"Mistral Small Analytical Prompt\": [responses[2]]\n",
    "    \n",
    "\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling with Mistral AI Models and the Converse API\n",
    "\n",
    "In this section, we will explore how to leverage the powerful function calling capability of Mistral AI models using Amazon Bedrock's Converse API. Function calling allows you to integrate external tools and APIs with the AI models, enabling them to perform a wide variety of tasks and access real-time information.\n",
    "\n",
    "By combining the advanced natural language understanding of Mistral AI models with the ability to invoke custom functions, you can build intelligent applications that can handle complex queries and provide accurate responses. The [Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) provides a consistent and simplified interface to interact with Mistral models, making it easier to manage conversations and function calls.\n",
    "We will dive into practical code examples that demonstrate how to set up function calling with Mistral models using the Converse API. You will learn how to define custom functions, register them with the AI models, and handle the function invocation flow seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StudioNotFoundError(Exception):\n",
    "    \"\"\"Raised when a movie studio isn't found.\"\"\"\n",
    "    pass\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Creating the actual tool that returns mock data\n",
    "def get_top_movie(studio):\n",
    "    \"\"\"Returns the highest grossing movie for the requested studio.\n",
    "    Args:\n",
    "        studio (str): The name of the movie studio for which you want\n",
    "        the highest grossing movie.\n",
    "\n",
    "    Returns:\n",
    "        response (json): The highest grossing movie and its gross.\n",
    "    \"\"\"\n",
    "\n",
    "    movie = \"\"\n",
    "    gross = \"\"\n",
    "    if studio == 'Paramount':\n",
    "        movie = \"Top Gun: Maverick\"\n",
    "        gross = \"$718,732,821\"\n",
    "\n",
    "    else:\n",
    "        return \"ERROR: Studio not found.\"\n",
    "\n",
    "    return movie, gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we created a mock function that simulates the behavior of an external tool or API called `get_top_movies`. This function can be invoked by the model when it determines that it needs to retrieve information about the top movies. Note that in a real-world scenario, this tool could be implemented as a Lambda function or any other external service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to facilitate the Converse API Call\n",
    "def generate_text(bedrock_client, model_id, tool_config, input_text):\n",
    "    \"\"\"Generates text using the supplied Amazon Bedrock model. If necessary,\n",
    "    the function handles tool use requests and sends the result to the model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The Amazon Bedrock model ID.\n",
    "        tool_config (dict): The tool configuration.\n",
    "        input_text (str): The input text.\n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating text with model %s\", model_id)\n",
    "\n",
    "   # Create the initial message from the user input.\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }]\n",
    "\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        toolConfig=tool_config\n",
    "    )\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        # Tool use requested. Call the tool and send the result to the model.\n",
    "        tool_requests = response['output']['message']['content']\n",
    "        for tool_request in tool_requests:\n",
    "            if 'toolUse' in tool_request:\n",
    "                tool = tool_request['toolUse']\n",
    "                logger.info(\"Requesting tool %s. Request: %s\",\n",
    "                            tool['name'], tool['toolUseId'])\n",
    "\n",
    "                if tool['name'] == 'top_movie':\n",
    "                    tool_result = {}\n",
    "                    try:\n",
    "                        movie, gross = get_top_movie(tool['input']['studio'])\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"json\": {\"movie\": movie, \"gross\": gross}}]\n",
    "                        }\n",
    "                    except StudioNotFoundError as err:\n",
    "                        tool_result = {\n",
    "                            \"toolUseId\": tool['toolUseId'],\n",
    "                            \"content\": [{\"text\":  err.args[0]}],\n",
    "                            \"status\": 'error'\n",
    "                        }\n",
    "\n",
    "                    tool_result_message = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"toolResult\": tool_result\n",
    "\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    messages.append(tool_result_message)\n",
    "\n",
    "                    # Send the tool result to the model.\n",
    "                    response = bedrock_client.converse(\n",
    "                        modelId=model_id,\n",
    "                        messages=messages,\n",
    "                        toolConfig=tool_config\n",
    "                    )\n",
    "                    output_message = response['output']['message']\n",
    "\n",
    "    # print the final response from the model.\n",
    "    for content in output_message['content']:\n",
    "        print(f\"{content['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you'll find a section where you need to provide a detailed description of your tool. This description should include information that the model can use to determine when to invoke or call your tool. It's essential to provide clear guidelines and required inputs to help the model understand the appropriate scenarios for utilizing your tool effectively.\n",
    "\n",
    "To ensure you provide a comprehensive and accurate description, you can refer to the official documentation located at https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html. This documentation offers detailed guidance on how to describe your tool, including best practices, formatting guidelines, and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text with model mistral.mistral-large-2407-v1:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the highest grossing movie from Paramount?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Requesting tool top_movie. Request: tooluse_pqvT0rvxR06nD9sPldIueA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest grossing movie from Paramount is Top Gun: Maverick, which grossed $718,732,821.\n"
     ]
    }
   ],
   "source": [
    "# Using Mistral Large\n",
    "model_id = DEFAULT_MODEL\n",
    "input_text = \"What is the highest grossing movie from Paramount?\"\n",
    "\n",
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"top_movie\",\n",
    "                \"description\": \"Get the highest grossing movie for a given movie studio.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"studio\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The name of the movie studio for which you want the highest grossing movie. Example studios are Paramount, Warner Bros., and Universal.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"studio\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Question: {input_text}\")\n",
    "generate_text(bedrock, model_id, tool_config, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Mistral model is prompted with the question \"What is the highest grossing movie from Paramount?\", it recognizes that it needs to use the top_movie tool to answer this query.\n",
    "\n",
    "The model then invokes the top_movie tool, which queries the right function tofind the highest-grossing Paramount movie. The tool returns the result \"Top Gun: Maverick with a gross of $718,732,821\", which the Mistral model incorporates into its final response.\n",
    "\n",
    "This output demonstrates the ability of Bedrocke and the Mistral Large 2 model to leverage external tools and data sources to enhance their capabilities and provide more accurate and up-to-date information in their responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we discussed the common pitfalls that can arise when dealing with prompts and large language models (LLMs). These pitfalls include hallucination and factual inaccuracies, lack of coherence and logical reasoning in generated text, difficulty with complex multi-step tasks, and misunderstanding user intent.\n",
    "\n",
    "We then explored how to create strong prompts and the importance of prompt engineering techniques to mitigate these issues. The key prompt engineering techniques covered were zero-shot prompting, few-shot prompting, and chain-of-thought (CoT) prompting.\n",
    "\n",
    "Zero-shot prompting relies solely on the model's pre-existing knowledge without any additional examples. Few-shot prompting provides a small number of examples to guide the model's outputs. CoT prompting encourages the model to explain its reasoning process step-by-step, which is particularly useful for complex reasoning tasks.\n",
    "\n",
    "We discussed the differences between the different Mistral AI models and their respective use cases. While Mixtral 8x7b can provide analytical responses, Mistral Large 2 offers more thorough and contextually aware outputs, especially for multilingual tasks. We also introduced the Function Calling capabaility with the Response API. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
