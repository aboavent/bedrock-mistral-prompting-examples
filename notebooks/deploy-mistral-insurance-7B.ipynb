{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530c6e00-d006-4aa9-a27d-eadb083eae97",
   "metadata": {},
   "source": [
    "# End-to-End Deployment of a Fine-Tuned Mistral Model on AWS with SageMaker, Bedrock, and Neuron Integration\n",
    "\n",
    "This notebook provides a comprehensive, hands-on guide to deploying a fine-tuned Mistral model for the insurance domain on AWS using Amazon SageMaker, Amazon Bedrock, and Hugging Face. The [Mistral-7B-Insurance](https://huggingface.co/bitext/Mistral-7B-Insurance) model is optimized to perform well in customer support scenarios, particularly for answering insurance-related queries. Also, it is a fine-tuned version of the [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "By following the steps in this notebook, you will learn how to optimize, deploy, and interact with a Mistral fine-tuned LLM using AWS’s robust infrastructure alongside Hugging Face tools. This workflow demonstrates how to leverage Hugging Face’s model repository for storage and distribution, AWS Inferentia instances on SageMaker for cost-effective inference, and Amazon Bedrock for flexible and scalable model serving. Finally, you will build a Streamlit application to interact with the model in real time, showcasing how AWS, Hugging Face, and Mistral services can support real-world customer support applications.\n",
    "\n",
    "## Goals\n",
    "\n",
    "The primary goals of this notebook are:\n",
    "1. **Model Preparation with Hugging Face**: Use Hugging Face’s tools to prepare, convert, and upload the model, enabling streamlined storage and versioning of large models.\n",
    "2. **Model Optimization for AWS Neuron**: Convert the fine-tuned Mistral model to a Neuron-compatible format using Hugging Face’s `optimum-cli`, optimizing it for performance on Inferentia-based SageMaker instances, and convert it to `safetensors` format for Amazon Bedrock compatibility.\n",
    "3. **Deploying on Amazon SageMaker**: Set up a cost-effective, high-performance deployment on Amazon SageMaker using `ml.inf2.xlarge` instances, enabling fast inference at lower costs.\n",
    "4. **Model Import to Amazon Bedrock**: Import the model from Hugging Face’s storage into Amazon Bedrock, allowing for seamless integration with other AWS services and enabling the use of the Converse API for inference.\n",
    "5. **Real-Time Interaction via Streamlit**: Build an interactive Streamlit application to test and compare model performance on both SageMaker and Bedrock, providing a hands-on experience for user interaction with deployed models.\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By completing this notebook, you will achieve the following outcomes:\n",
    "- **Experience with Hugging Face Model Handling**: Learn how to use Hugging Face’s model repository for storing, versioning, and retrieving large models, simplifying the model management process.\n",
    "- **Understanding Model Compilation and Optimization**: Gain hands-on experience with model compilation for AWS Neuron, optimizing the model for deployment on Inferentia instances, and learn how to convert models to the Hugging Face `safetensors` format for efficient storage and compatibility.\n",
    "- **Proficiency in Multi-Platform Deployment**: Deploy a model on both Amazon SageMaker and Amazon Bedrock, gaining flexibility in model serving options, and understanding the integration between Hugging Face and AWS services.\n",
    "- **Real-Time Streaming Inference**: Use Amazon Bedrock’s Converse Streaming API to interact with the model in real-time, receiving streamed responses that enhance user interaction.\n",
    "- **End-to-End Application Development**: Build a Streamlit application with a user-friendly interface that allows side-by-side testing of models deployed on both SageMaker and Bedrock, demonstrating the ease of integrating Hugging Face models into AWS applications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3c55b-beb5-45dc-a8c4-c7e85bd372d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700599cd-364b-4d4f-aece-4d9e6e74ae39",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import necessary libraries to facilitate model conversion, deployment, and API interactions across AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabc627-53f9-4f16-9c6c-17f0f629403e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers \\\n",
    "                sagemaker \\\n",
    "                boto3 \\\n",
    "                tiktoken \\\n",
    "                torch \\\n",
    "                blobfile \\\n",
    "                sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9de4538-d595-48bd-b113-abf0df631068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92884ef1-402b-4a43-b25c-1a4f799ee113",
   "metadata": {},
   "source": [
    "## Section 2: Export Variables for SageMaker Deployment\n",
    "\n",
    "Define essential variables for model deployment, including model details, batch size, sequence length, and AWS-specific configurations.\n",
    "\n",
    "### Key Variables in This Notebook\n",
    "\n",
    "1. **`MODEL_ID`**: Hugging Face ID for the fine-tuned Mistral model in the insurance domain, used throughout the compilation and deployment process.\n",
    "\n",
    "2. **`BATCH_SIZE`**: Batch size for processing inputs simultaneously, optimizing performance on Inferentia.\n",
    "\n",
    "3. **`SEQUENCE_LENGTH`**: Maximum input length for each sequence, balancing memory and model capacity.\n",
    "\n",
    "4. **`MAX_TOTAL_TOKENS`**: Upper limit for tokens in each response, managing output length during inference.\n",
    "\n",
    "5. **`NUM_CORES`**: Number of Neuron cores used in parallel for efficient processing on Inferentia instances.\n",
    "\n",
    "6. **`HF_MODEL_ID_TO_PUSH`**: The Hugging Face model name(e.g., \"aboavent/Mistral-7B-Insurance-neuron\") under which the compiled model will be saved.\n",
    "\n",
    "7. **`HF_TOKEN`**: Hugging Face authentication token for model access and upload.\n",
    "\n",
    "8. **`PRECISION`**: Model precision (`fp16`), reducing memory while maintaining inference speed.\n",
    "\n",
    "9. **`MODEL_OUTPUT_NAME`**: Designated name for the compiled model, aiding in file tracking and reference.\n",
    "\n",
    "10. **`COMPILED_MODEL_OUTPUT_PATH`**: Directory path for saving the compiled model after export, used in later deployment steps.\n",
    "\n",
    "11. **`sagemaker_region`**: The region for SageMaker where the model will be deployed.\n",
    "\n",
    "12. **`bedrock_region`**: The region for Amazon Bedrock where the model will be imported later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2cbc89-59d2-4dcd-a335-dcc67f3d569d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"bitext/Mistral-7B-Insurance\"\n",
    "BATCH_SIZE = 4\n",
    "SEQUENCE_LENGTH = 4096\n",
    "MAX_TOTAL_TOKENS = 4096  # Set independently for the total token limit\n",
    "NUM_CORES = 2\n",
    "HF_MODEL_ID_TO_PUSH = \"aboavent/Mistral-7B-Insurance-neuron\"\n",
    "HF_TOKEN = \"YOUR HF TOKEN GOES HERE"\n",
    "PRECISION = \"fp16\"\n",
    "MODEL_OUTPUT_NAME = \"Mistral-7B-Insurance-neuron\"\n",
    "COMPILED_MODEL_OUTPUT_PATH = f\"./{MODEL_OUTPUT_NAME}\" \n",
    "sagemaker_region = \"us-east-2\"  # Region for SageMaker endpoint\n",
    "bedrock_region = \"us-west-2\"    # Region for Bedrock model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fa2d7-d3a9-462d-a97a-9d611bffac2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 3: Hugging Face Authentication\n",
    "\n",
    "Authenticate with Hugging Face using the provided token to access and upload model resources.\n",
    "\n",
    "In this section, we authenticate with Hugging Face to access and manage model resources on the Hugging Face Hub. This step is essential for retrieving the model files, as well as uploading and managing compiled versions of the model for our project. You will need to create an Access Token so you can properly authenticate to Hugging Face. Here is the documentation link. [Creating User Access Tokens](https://huggingface.co/docs/hub/en/security-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4e3f6-b38c-4609-8312-689e2714f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d6563-cd4e-4158-b133-ee2d12a879c3",
   "metadata": {},
   "source": [
    "## Section 4: Model Compilation with Optimum CLI\n",
    "\n",
    "In this section, we compile the Mistral model using Hugging Face’s `optimum-cli` tool, which prepares it for efficient inference on AWS Inferentia instances with AWS Neuron. Model compilation is a critical step in optimizing performance, as it converts the model into a format compatible with AWS Neuron, enabling accelerated inference on hardware optimized for deep learning.\n",
    "\n",
    "#### Important Note\n",
    ">If you are working with a larger model that requires loading into compute memory fully, consider using a high-memory instance such as **`inf2.24xlarge`**, **`inf2.48xlarge`**, or **`trn1.32xlarge`**. These instances offer substantial memory and Neuron cores, providing sufficient resources for model processing. This compilation step can be either performed on an EC2 instance provisioned with **AWS Deep Learning AMI Neuron (Ubuntu 22.04 or Amazon Linux 2)** or using AWS Deep Learning Containers with Amazon Sagemaker Studio with a [PyTorch 1.13 HuggingFace Python 3.10 Neuron Optimized](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html) image. They include the necessary libraries and environment. For more details, refer to the [AWS Deep Learning AMI Neuron release notes](https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html) and the [AWS Deep Learning Containers Developer Guide](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html)\n",
    "\n",
    "\n",
    "#### Steps in This Section\n",
    "\n",
    "1. **Configure Compilation Parameters**:\n",
    "   - We define key parameters to guide the compilation process, including:\n",
    "      - **`MODEL_ID`**: The model ID from Hugging Face, representing the pretrained Mistral model.\n",
    "      - **`BATCH_SIZE`**: The maximum number of inputs processed at once, balancing performance and memory usage.\n",
    "      - **`SEQUENCE_LENGTH`**: Maximum length of input sequences, which impacts both memory and inference time.\n",
    "      - **`NUM_CORES`**: Number of Neuron cores to utilize, allowing parallel processing.\n",
    "      - **`PRECISION`**: Set to `fp16` to reduce memory usage while maintaining inference speed and accuracy.\n",
    "\n",
    "2. **Run Optimum CLI**:\n",
    "   - We use the Hugging Face `optimum-cli` tool to compile the model for Inferentia hardware. The command specifies the model ID, batch size, sequence length, number of cores, and precision level.\n",
    "   - This command generates a Neuron-compatible version of the model, which is stored in the specified output path (`COMPILED_MODEL_OUTPUT_PATH`).\n",
    "\n",
    "#### Why Compile the Model?\n",
    "\n",
    "Compiling the model with Optimum CLI offers several benefits:\n",
    "- **Optimized for Inferentia**: Compilation enables the model to leverage the specialized capabilities of AWS Inferentia, reducing inference latency and improving cost-efficiency.\n",
    "- **Memory Efficiency**: Using `fp16` precision and Neuron-compatible optimizations helps reduce memory usage, allowing for higher throughput.\n",
    "- **Scalability**: A compiled model is more scalable and cost-effective in production, making it ideal for customer support applications.\n",
    "\n",
    "For more details on Hugging Face Optimum CLI and its features, refer to the official [Optimum Neuron CLI documentation](https://huggingface.co/docs/optimum-neuron/guides/export_model). Also, check out the [Optimum Neuron official documentation](https://huggingface.co/docs/optimum-neuron/index) with additional samples and tutorials for your reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b1e32-f550-4aee-98aa-ae3ef276f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export neuron \\\n",
    "    -m $MODEL_ID \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --sequence_length $SEQUENCE_LENGTH \\\n",
    "    --num_cores $NUM_CORES \\\n",
    "    --auto_cast_type $PRECISION \\\n",
    "    --trust-remote-code \\\n",
    "    $COMPILED_MODEL_OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a34b98-90ff-483d-af6f-fb024f4be308",
   "metadata": {},
   "source": [
    "## Section 5: Upload Compiled Model to Hugging Face\n",
    "\n",
    "Create a new repository on Hugging Face and upload the compiled model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f73fe-d18a-4ba6-be91-d29e26f2f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli repo create $MODEL_OUTPUT_NAME\n",
    "\n",
    "!huggingface-cli upload $HF_MODEL_ID_TO_PUSH $COMPILED_MODEL_OUTPUT_PATH ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7424ec-946f-4582-94aa-afe433ca6815",
   "metadata": {},
   "source": [
    "## Section 6: Deploy Model to SageMaker Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ebde0-1a86-42b8-b3ae-e3c2e5a0cddf",
   "metadata": {},
   "source": [
    "In this section, we set up the necessary configurations for deploying our fine-tuned Mistral model on Amazon SageMaker. This involves defining the IAM role for SageMaker and setting up key model parameters for optimal performance on Inferentia instances with AWS Neuron. \n",
    "\n",
    "\n",
    "### Set Model Deployment Configuration:\n",
    "   - Configure the model’s environment variables using a dictionary (`hub`). These parameters optimize the model's performance on the `ml.inf2.xlarge` instance type and control important aspects of inference to ensure that the model is optimized for Inferentia instances, balancing speed, cost-efficiency, and accuracy. Key settings include:\n",
    "      - **`HF_MODEL_ID`**: The Hugging Face model ID of the compiled Mistral model.\n",
    "      - **`HF_NUM_CORES`**: Number of Neuron cores to allocate, providing parallel processing for efficient inference.\n",
    "      - **`HF_SEQUENCE_LENGTH`**: Maximum sequence length for inputs, which impacts memory usage and latency.\n",
    "      - **`HF_AUTO_CAST_TYPE`**: Precision setting, typically `fp16` to balance performance and memory efficiency.\n",
    "      - **`MAX_BATCH_SIZE`**: Defines the maximum number of inputs processed in a single batch, balancing latency and throughput.\n",
    "      - **`MAX_INPUT_TOKENS` and `MAX_TOTAL_TOKENS`**: Specifies the maximum tokens allowed for input and total tokens per request, ensuring responses fit within the configured limits.\n",
    "      - **`HF_TOKEN`**: Authentication token for Hugging Face, enabling access to model files stored on Hugging Face.\n",
    "      - **`MESSAGES_API_ENABLED`**: Enables conversational interactions by allowing message-based API requests.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a972c4b0-94c6-48ee-a440-d58f53cdbea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "    \"HF_MODEL_ID\": HF_MODEL_ID_TO_PUSH,\n",
    "    \"HF_NUM_CORES\": str(NUM_CORES),\n",
    "    \"HF_SEQUENCE_LENGTH\": str(SEQUENCE_LENGTH),\n",
    "    \"HF_AUTO_CAST_TYPE\": PRECISION,\n",
    "    \"MAX_BATCH_SIZE\": str(BATCH_SIZE),\n",
    "    \"MAX_INPUT_TOKENS\": \"1800\",\n",
    "    \"MAX_TOTAL_TOKENS\": str(MAX_TOTAL_TOKENS),\n",
    "    \"HF_TOKEN\": HF_TOKEN,\n",
    "    \"MESSAGES_API_ENABLED\": \"true\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b485ab-8535-43e4-83cf-a5e230941614",
   "metadata": {},
   "source": [
    "### Deploy the compiled model on a `ml.inf2.xlarge` instance in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d5def9-1a33-4012-a0fa-5bc0f0acdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------!CPU times: user 606 ms, sys: 60.3 ms, total: 667 ms\n",
      "Wall time: 9min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface-neuronx\", version=\"0.0.24\"),\n",
    "    env=hub,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "# Set this flag to indicate that the model is precompiled\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# Deploy the model and get the predictor\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    container_startup_health_check_timeout=2400,\n",
    "    volume_size=512\n",
    ")\n",
    "\n",
    "sagemaker_endpoint_name = predictor.endpoint_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cd895-2a2f-4bd8-a2ad-c1347d3128a6",
   "metadata": {},
   "source": [
    "## Section 7: Test SageMaker Endpoint with both the HuggingFace's and SageMaker's APIs\n",
    "\n",
    "Send a sample request to the SageMaker endpoint to verify that the model is deployed and functioning correctly using the **HuggingFace API**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4a59ef-4170-4c88-a16f-763b2f116a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Request 1 ---\n",
      "System Prompt: You are an expert in health insurance policies.\n",
      "User Query: What benefits do I get with my current health plan?\n",
      "Model Response:  To review the specific benefits associated with your health plan, please adhere to the following instructions:\n",
      "\n",
      "1. Access the website at {{WEBSITE_URL}}.\n",
      "2. Sign in using your credentials, ensuring both your username and password are entered correctly.\n",
      "3. Locate the {{HEALTH_INSURANCE_SECTION}} within the navigation menu of the site.\n",
      "4. Choose your particular health plan from the displayed list.\n",
      "5. Examine the detailed\n",
      "\n",
      "\n",
      "--- Sample Request 2 ---\n",
      "System Prompt: You are an insurance advisor.\n",
      "User Query: How can I reduce my monthly insurance premium?\n",
      "Model Response:  To assist you in achieving a lower monthly insurance premium, please adhere to the following guidelines:\n",
      "\n",
      "1. Compare insurance providers: Begin by comparing the rates offered by various insurance providers to determine who has the most competitive rates.\n",
      "2. Choose a higher deductible: Opting for a higher deductible can result in a lower monthly premium payment.\n",
      "3. Maintain a clean driving record: A clean driving record can significantly influence your insurance rates as insurers generally offer\n",
      "\n",
      "\n",
      "--- Sample Request 3 ---\n",
      "System Prompt: You are an expert in auto insurance policies.\n",
      "User Query: What happens if my car is totaled?\n",
      "Model Response:  If your vehicle is considered a total loss, your auto insurance policy will provide you with either the actual cash value (ACV) or the replacement cost (RCV) of your vehicle, according to the specifics of your policy.\n",
      "\n",
      "1. Actual Cash Value (ACV): This is the current market value of your vehicle at the time of the loss, which includes depreciation and other wear and tear. The insurance company will pay you the ACV after you file a\n",
      "\n",
      "\n",
      "--- Sample Request 4 ---\n",
      "System Prompt: You are an expert in life insurance.\n",
      "User Query: Can you explain the difference between term and whole life insurance?\n",
      "Model Response:  To effectively understand the differences between term life and whole life insurance, consider the following points:\n",
      "\n",
      "1. Coverage Duration and Structure:\n",
      "   * Term life insurance covers you for a specific term or period, typically ranging from 1 to 30 years.\n",
      "   * Whole life insurance, on the other hand, provides coverage throughout your entire life, as long as the premiums are paid consistently.\n",
      "\n",
      "2. Cost:\n",
      "   * Term life insurance is generally\n",
      "\n",
      "\n",
      "--- Sample Request 5 ---\n",
      "System Prompt: You are an insurance claims specialist.\n",
      "User Query: What documents are needed to file a claim for home insurance?\n",
      "Model Response:  To initiate a claim for your home insurance coverage, please adhere to the following protocol for submitting the necessary documentation:\n",
      "\n",
      "1. Access our dedicated claims portal via {{WEBSITE_URL}}.\n",
      "2. Use your policyholder account details to log in.\n",
      "3. Find the {{CLAIM_SECTION}} option in the menu for claims processing.\n",
      "4. Choose the {{FILE_CLAIM_OPTION}} available.\n",
      "5. Complete the claim form\n",
      "\n",
      "\n",
      "--- Sample Request 6 ---\n",
      "System Prompt: You are a customer service representative for health insurance.\n",
      "User Query: Can I add my spouse to my health insurance policy?\n",
      "Model Response:  To add your spouse to your health insurance policy, please adhere to the following procedure:\n",
      "\n",
      "1. Access your account via {{WEBSITE_URL}}.\n",
      "2. Locate the {{SPARSE_SECTION}} tab on the dashboard.\n",
      "3. Choose the option to {{ADD_DEPENDENT_OPTION}}.\n",
      "4. Fill out the necessary information regarding your spouse and ensure it is up-to-date.\n",
      "5. Submit your application and\n",
      "\n",
      "\n",
      "--- Sample Request 7 ---\n",
      "System Prompt: You are an expert in travel insurance policies.\n",
      "User Query: What coverage do I have if my flight is canceled?\n",
      "Model Response:  In the event that your flight is canceled, it is essential to understand the coverage provided by your travel insurance policy. As a general guideline:\n",
      "\n",
      "1. Look up the specifics of your plan to determine if it includes trip interruption or cancellation insurance. This coverage is meant to compensate for any financial losses caused by unexpected cancellations, including flights.\n",
      "2. Contact your airline to verify the justification for the cancellation and to inquire about any compensation they may offer.\n",
      "\n",
      "\n",
      "--- Sample Request 8 ---\n",
      "System Prompt: You are a specialist in pet insurance.\n",
      "User Query: Does my policy cover emergency vet visits?\n",
      "Model Response:  To verify whether your pet insurance plan covers emergency veterinary visits, please adhere to these guidelines:\n",
      "\n",
      "1. Access the {{WEBSITE_URL}}.\n",
      "2. Input your account credentials to log in.\n",
      "3. Proceed to the {{COVERAGE_SECTION}} segment.\n",
      "4. Choose your particular pet insurance policy from the displayed list.\n",
      "5. Locate the section concerning {{EMERGENCY_CARE_COVERAGE}}.\n",
      "6\n",
      "\n",
      "\n",
      "--- Sample Request 9 ---\n",
      "System Prompt: You are an insurance fraud investigator.\n",
      "User Query: What are some common signs of insurance fraud?\n",
      "Model Response:  Insurance fraud is a deceptive act to obtain benefits from an insurance policy by submitting false claims or withholding pertinent information. Several warning signs may suggest the possibility of insurance fraud. These signs include:\n",
      "\n",
      "1. Claims for unusual or elaborate medical treatments or services.\n",
      "2. Excessive numbers of claims submitted by a single policyholder at once.\n",
      "3. Unusually large claim payouts for relatively low premiums.\n",
      "4. Discrepancies\n",
      "\n",
      "\n",
      "--- Sample Request 10 ---\n",
      "System Prompt: You are an advisor on property insurance.\n",
      "User Query: How do I increase the coverage for natural disasters?\n",
      "Model Response:  To enhance your coverage for natural disasters under your property insurance, please adhere to the following steps:\n",
      "\n",
      "1. Access your account by visiting {{WEBSITE_URL}}.\n",
      "2. Proceed to the {{COVERAGE_SECTION}} section.\n",
      "3. Identify the specific policy you wish to enhance, for instance, your {{POLICY_TYPE}} insurance policy.\n",
      "4. Opt for the {{UPGRADE_COVERAGE_OPTION}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_sample_request(system_prompt, user_query):\n",
    "    \"\"\"\n",
    "    Creates a sample request structure for the predictor based on the given system prompt and user query.\n",
    "\n",
    "    Parameters:\n",
    "        system_prompt (str): The initial system prompt to set the model's role.\n",
    "        user_query (str): The user's query for the insurance model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A structured request for the SageMaker predictor.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "    }\n",
    "\n",
    "# List of different system prompts and user queries to test various scenarios\n",
    "system_user_queries = [\n",
    "    (\"You are an expert in health insurance policies.\", \"What benefits do I get with my current health plan?\"),\n",
    "    (\"You are an insurance advisor.\", \"How can I reduce my monthly insurance premium?\"),\n",
    "    (\"You are an expert in auto insurance policies.\", \"What happens if my car is totaled?\"),\n",
    "    (\"You are an expert in life insurance.\", \"Can you explain the difference between term and whole life insurance?\"),\n",
    "    (\"You are an insurance claims specialist.\", \"What documents are needed to file a claim for home insurance?\"),\n",
    "    (\"You are a customer service representative for health insurance.\", \"Can I add my spouse to my health insurance policy?\"),\n",
    "    (\"You are an expert in travel insurance policies.\", \"What coverage do I have if my flight is canceled?\"),\n",
    "    (\"You are a specialist in pet insurance.\", \"Does my policy cover emergency vet visits?\"),\n",
    "    (\"You are an insurance fraud investigator.\", \"What are some common signs of insurance fraud?\"),\n",
    "    (\"You are an advisor on property insurance.\", \"How do I increase the coverage for natural disasters?\")\n",
    "]\n",
    "\n",
    "# Loop through each system prompt and user query, create a request, and get a response from the predictor\n",
    "for i, (system_prompt, user_query) in enumerate(system_user_queries, start=1):\n",
    "    print(f\"--- Sample Request {i} ---\")\n",
    "    request = create_sample_request(system_prompt, user_query)\n",
    "    response = predictor.predict(request)\n",
    "    print(\"System Prompt:\", system_prompt)\n",
    "    print(\"User Query:\", user_query)\n",
    "    print(\"Model Response:\", response['choices'][0]['message']['content'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01069ed1-96db-4230-a8ba-47f3fc87e9d7",
   "metadata": {},
   "source": [
    "### Send a sample request to the model using the SageMaker API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cef969-b503-4e40-9ed4-a78926654711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'chat.completion', 'id': '', 'created': 1730934825, 'model': 'aboavent/Mistral-7B-Insurance-neuron', 'system_fingerprint': '2.1.1-native', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' To assist you in finding ways to potentially reduce your monthly insurance premium, please follow these steps:\\n\\n1. Examine your current insurance plan for any available discounts or reductions. These may include incentives for bundling policies, setting up automated payments, or maintaining a good driving history.\\n2. Compare your current policy to other available plans offered by your insurance company. It is possible that a different plan designed for your specific needs might offer more affordable rates.\\n3. M'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 100, 'total_tokens': 100}}\n",
      " To assist you in finding ways to potentially reduce your monthly insurance premium, please follow these steps:\n",
      "\n",
      "1. Examine your current insurance plan for any available discounts or reductions. These may include incentives for bundling policies, setting up automated payments, or maintaining a good driving history.\n",
      "2. Compare your current policy to other available plans offered by your insurance company. It is possible that a different plan designed for your specific needs might offer more affordable rates.\n",
      "3. M\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\", \n",
    "                                region_name=sagemaker_region)\n",
    "\n",
    "# Function to query the model on SageMaker\n",
    "def query_sagemaker_model(endpoint_name, query):\n",
    "    payload = {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,  # Updated model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in customer support for Insurance.\"},\n",
    "            {\"role\": \"user\", \"content\": query}  # Send the user query as a string\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 4096,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.90,\n",
    "            \"max_length\": 4096,\n",
    "            \"stop\": None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the request to SageMaker endpoint\n",
    "        response = sagemaker_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        result = json.loads(response['Body'].read())\n",
    "        print(result)\n",
    "        return result['choices'][0]['message']['content']\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred with SageMaker: {e.response['Error']['Message']}\")\n",
    "        return None\n",
    "    \n",
    "if sagemaker_endpoint_name is None:\n",
    "    sagemaker_endpoint_name = \"huggingface-pytorch-tgi-inference-ml-in-2024-11-05-03-57-02-330\"  # SageMaker endpoint name    \n",
    "\n",
    "model_response = query_sagemaker_model(sagemaker_endpoint_name, \n",
    "                                       \"How can I reduce my monthly insurance premium?\")\n",
    "print(model_response)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec6f0b-4abf-4b50-a870-c680462633dd",
   "metadata": {},
   "source": [
    "## Section 8: Convert Model to Safetensors Format for Bedrock\n",
    "\n",
    "In this section, we convert the compiled Mistral model into the `safetensors` format, which is a lightweight and efficient file format designed for securely storing large model weights. This conversion is necessary to make the model compatible with Amazon Bedrock, allowing us to easily import and use the model within the Bedrock environment.\n",
    "\n",
    "#### Important Note\n",
    "> For this conversion, it is recommended to use a larger instance within SageMaker Studio with **at least 128 GB of memory**. This ensures sufficient resources for loading and processing the entire model during the conversion process. Attempting this step on smaller instances may result in memory-related errors.\n",
    "\n",
    "#### Steps in This Section\n",
    "\n",
    "1. **Define Conversion Function**:\n",
    "   - We define a `convert_to_safetensors` function that loads the model and its tokenizer, then saves them in `safetensors` format.\n",
    "   - The function uses the Hugging Face Transformers library to load the model and save it in the `safetensors` format for compatibility with Amazon Bedrock.\n",
    "\n",
    "2. **Run Conversion**:\n",
    "   - Call the `convert_to_safetensors` function, specifying the model name (`MODEL_ID`) and the directory (`save_directory`) where the converted model files will be saved.\n",
    "   - This process generates two outputs:\n",
    "      - The model in `safetensors` format.\n",
    "      - The tokenizer, saved alongside the model.\n",
    "\n",
    "3. **Verify Conversion**:\n",
    "   - After conversion, we list the contents of the target directory (`save_directory`) to confirm that the model files have been saved correctly in the desired format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de3ec75-63a9-448f-8d2d-523d20cd988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model bitext/Mistral-7B-Insurance...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3ce8df39a5406aa6ee3fd1cb79e719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting and saving model to /root/Mistral-7B-Insurance in safetensors format...\n",
      "Conversion complete!\n",
      "CPU times: user 18.8 s, sys: 49.3 s, total: 1min 8s\n",
      "Wall time: 5min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model-00002-of-00006.safetensors',\n",
       " 'tokenizer_config.json',\n",
       " 'model-00006-of-00006.safetensors',\n",
       " 'model-00004-of-00006.safetensors',\n",
       " 'tokenizer.model',\n",
       " 'model.safetensors.index.json',\n",
       " 'config.json',\n",
       " 'model-00003-of-00006.safetensors',\n",
       " 'generation_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'model-00005-of-00006.safetensors',\n",
       " 'model-00001-of-00006.safetensors']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def convert_to_safetensors(model_name, save_directory):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face model to safetensors format for Amazon Bedrock compatibility.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the model to convert.\n",
    "        save_directory (str): Directory to save the converted model and tokenizer.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    print(f\"Converting and saving model to {save_directory} in safetensors format...\")\n",
    "    model.save_pretrained(save_directory, safe=True)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(\"Conversion complete!\")\n",
    "\n",
    "# Specify the directory and model name\n",
    "save_directory = os.path.expanduser(\"~/Mistral-7B-Insurance\")\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "convert_to_safetensors(MODEL_ID, save_directory)\n",
    "\n",
    "# List the contents of the save directory to verify the conversion\n",
    "os.listdir(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2d894-1183-437d-b300-ec341d2e4739",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 9: Upload Converted Model to S3\n",
    "\n",
    "Upload the `safetensors` formatted model files to an S3 bucket, making them accessible to Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fbaef30-ef65-4368-b3fc-51adb50c0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique S3 bucket name for this execution: mistral-7b-insurance-bedrock-import-20241106232823\n",
      "Bucket 'mistral-7b-insurance-bedrock-import-20241106232823' does not exist. Creating bucket...\n",
      "Bucket 'mistral-7b-insurance-bedrock-import-20241106232823' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model-00002-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/model-00002-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  17%|█▋        | 2/12 [00:28<01:56, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00002-of-00006.safetensors uploaded successfully.\n",
      "Uploading tokenizer_config.json to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/tokenizer_config.json...\n",
      "tokenizer_config.json uploaded successfully.\n",
      "Uploading model-00006-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/model-00006-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  25%|██▌       | 3/12 [00:52<02:35, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00006-of-00006.safetensors uploaded successfully.\n",
      "Uploading model-00004-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/model-00004-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  33%|███▎      | 4/12 [01:18<02:46, 20.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00004-of-00006.safetensors uploaded successfully.\n",
      "Uploading tokenizer.model to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/tokenizer.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  50%|█████     | 6/12 [01:18<00:53,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model uploaded successfully.\n",
      "Uploading model.safetensors.index.json to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/model.safetensors.index.json...\n",
      "model.safetensors.index.json uploaded successfully.\n",
      "Uploading config.json to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  58%|█████▊    | 7/12 [01:19<00:30,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json uploaded successfully.\n",
      "Uploading model-00003-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/model-00003-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  75%|███████▌  | 9/12 [01:47<00:27,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00003-of-00006.safetensors uploaded successfully.\n",
      "Uploading generation_config.json to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/generation_config.json...\n",
      "generation_config.json uploaded successfully.\n",
      "Uploading special_tokens_map.json to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/special_tokens_map.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  83%|████████▎ | 10/12 [01:48<00:12,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map.json uploaded successfully.\n",
      "Uploading model-00005-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/model-00005-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  92%|█████████▏| 11/12 [02:14<00:12, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00005-of-00006.safetensors uploaded successfully.\n",
      "Uploading model-00001-of-00006.safetensors to s3://mistral-7b-insurance-bedrock-import-20241106232823/safetensors/model-00001-of-00006.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3: 100%|██████████| 12/12 [02:41<00:00, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00001-of-00006.safetensors uploaded successfully.\n",
      "CPU times: user 1min 18s, sys: 37.7 s, total: 1min 56s\n",
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from datetime import datetime\n",
    "\n",
    "# Define S3 and local directory configurations\n",
    "s3_client = boto3.client(\"s3\", region_name=bedrock_region)\n",
    "\n",
    "base_bucket_name = \"mistral-7b-insurance-bedrock-import\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "s3_bucket_name = f\"{base_bucket_name}-{timestamp}\"\n",
    "s3_model_directory = \"safetensors\"\n",
    "\n",
    "local_model_directory = save_directory  # Use save_directory from previous step\n",
    "\n",
    "print(f\"Unique S3 bucket name for this execution: {s3_bucket_name}\")\n",
    "\n",
    "\n",
    "def create_bucket_if_not_exists(bucket_name, region=\"us-west-2\"):\n",
    "    \"\"\"\n",
    "    Creates the S3 bucket if it does not exist.\n",
    "    \n",
    "    Parameters:\n",
    "        bucket_name (str): The name of the bucket to create.\n",
    "        region (str): The AWS region for the bucket.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"Bucket '{bucket_name}' does not exist. Creating bucket...\")\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "            print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "create_bucket_if_not_exists(s3_bucket_name, bedrock_region)\n",
    "\n",
    "def upload_to_s3(local_directory, bucket, s3_directory):\n",
    "    \"\"\"\n",
    "    Uploads all files from a local directory to the specified S3 bucket and directory.\n",
    "\n",
    "    Parameters:\n",
    "        local_directory (str): Path to the local directory containing files to upload.\n",
    "        bucket (str): Name of the S3 bucket.\n",
    "        s3_directory (str): Directory path within the S3 bucket to store the files.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(local_directory) if os.path.isfile(os.path.join(local_directory, f))]\n",
    "    \n",
    "    # Progress bar for uploads\n",
    "    for filename in tqdm(files, desc=\"Uploading files to S3\"):\n",
    "        file_path = os.path.join(local_directory, filename)\n",
    "        s3_path = f\"{s3_directory}/{filename}\"\n",
    "        print(f\"Uploading {filename} to s3://{bucket}/{s3_path}...\")\n",
    "        s3_client.upload_file(file_path, bucket, s3_path)\n",
    "        print(f\"{filename} uploaded successfully.\")\n",
    "\n",
    "# Run the upload function\n",
    "upload_to_s3(local_model_directory, s3_bucket_name, s3_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddc69d-5133-402c-961a-d3ff4bb6326a",
   "metadata": {},
   "source": [
    "## Section 10: Import Model into Amazon Bedrock\n",
    "\n",
    "Create an IAM Execution Role for Bedrock with Parameters to be used by\n",
    "a model import job in Amazon Bedrock using the Mistral 7b Insurance model files uploaded to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b27007-7f38-4c52-afda-845c2488e0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IAM Role...\n",
      "Role 'BedrockModelImportExecutionRole' already exists.\n",
      "Attaching policy to IAM Role...\n",
      "Policy attached successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "# Parameters for source account\n",
    "source_account = \"603555443475\"  # Replace with your AWS account ID\n",
    "\n",
    "# IAM client and role/policy details\n",
    "iam_client = boto3.client('iam')\n",
    "role_name = \"BedrockModelImportExecutionRole\"\n",
    "policy_name = \"BedrockModelImportPolicy\"\n",
    "s3_bucket_name = \"mistral-7b-insurance-bedrock-import\"  # Replace with your actual bucket name\n",
    "\n",
    "# Define the trust policy to allow Bedrock to assume this role with specific conditions\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": source_account  \n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{bedrock_region}:{source_account}:model-import-job/*\"  \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define the permissions policy for S3 and Bedrock access\n",
    "permissions_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}\",\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModel\",\n",
    "                \"bedrock:GetModel\",\n",
    "                \"bedrock:ListModels\",\n",
    "                \"bedrock:CreateModelImportJob\",\n",
    "                \"bedrock:GetModelImportJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    print(\"Creating IAM Role...\")\n",
    "    role_response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=\"Role for Amazon Bedrock model import job with S3 access\"\n",
    "    )\n",
    "    role_arn = role_response['Role']['Arn']\n",
    "    print(f\"IAM Role created with ARN: {role_arn}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(f\"Role '{role_name}' already exists.\")\n",
    "        role_arn = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Attach the permissions policy to the role\n",
    "try:\n",
    "    print(\"Attaching policy to IAM Role...\")\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=policy_name,\n",
    "        PolicyDocument=json.dumps(permissions_policy)\n",
    "    )\n",
    "    print(\"Policy attached successfully.\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error attaching policy: {e}\")\n",
    "    raise\n",
    "\n",
    "# The role ARN will be used in the next cell to create the model import job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576a8cb-f4f8-4429-9c0d-5fc6743a8e6b",
   "metadata": {},
   "source": [
    "## Section 11: Create and Submit Model Import Job to Amazon Bedrock\n",
    "\n",
    "In this section, we set up and submit a model import job to Amazon Bedrock, which imports our converted model files from Amazon S3 into Bedrock for deployment and inference. This process imports the model to Bedrock, preparing it for deployment and usage with the Converse API.\n",
    "\n",
    "\n",
    "1. **Initialize Variables**: \n",
    "   - **`bedrock_client`**: Establishes a Bedrock client connection in the specified `bedrock_region`.\n",
    "   - **`s3_model_uri`**: Constructs the S3 URI path to the directory where our model files are stored.\n",
    "   - **`imported_model_name`**: Provides a user-friendly name for the model once it’s imported to Amazon Bedrock.\n",
    "   - **`job_name`**: Generates a unique name for the model import job by appending a timestamp, ensuring that each job has a unique identifier.\n",
    "\n",
    "2. **Submit the Model Import Job**:\n",
    "   - We use the `create_model_import_job` method of the `bedrock_client` to initiate the import job.\n",
    "   - This method requires:\n",
    "      - **`jobName`**: The unique job identifier (`job_name`).\n",
    "      - **`importedModelName`**: The name under which the model will be imported to Bedrock.\n",
    "      - **`roleArn`**: The ARN of the IAM role with permissions to access the S3 bucket and perform import operations.\n",
    "      - **`modelDataSource`**: Specifies the S3 URI containing the model files in `safetensors` format.\n",
    "      \n",
    "   - After initiating the job, the response from Bedrock includes details about the job status and configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79f7da-29c8-4c50-baca-69c0a92625fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "bedrock_client = boto3.client('bedrock', region_name=bedrock_region)  \n",
    "s3_model_uri = f\"s3://{s3_bucket_name}/{s3_model_directory}/\"  \n",
    "imported_model_name = f\"Mistral-7B-Insurance-Model-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# Use the IAM role ARN created in the previous cell\n",
    "job_name = f\"mistral-7b-insurance-import-job-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock_client.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,  # Use the ARN from the IAM role created in the previous cell\n",
    "    modelDataSource={'s3DataSource': {'s3Uri': s3_model_uri}}\n",
    ")\n",
    "\n",
    "print(\"Model import job created:\", response)\n",
    "print(json.dumps(response, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc77d7f-4778-4c48-96e3-760f9edf5c81",
   "metadata": {},
   "source": [
    "## Section 12: Monitor Bedrock Model Import Job Status\n",
    "\n",
    "In this section, we track the status of our model import job in Amazon Bedrock to ensure that the model is successfully imported and ready for deployment. Since model import jobs can take time, a periodic status check allows us to monitor the progress and handle any errors that may arise.\n",
    "\n",
    "1. **Set Polling Interval**:\n",
    "   - Define `polling_interval` to specify the time (in seconds) between each status check. Here, we set it to 30 seconds.\n",
    "\n",
    "2. **Define `check_job_status` Function**:\n",
    "   - The `check_job_status` function queries the current status of the model import job using the `get_model_import_job` API.\n",
    "   - Parameters:\n",
    "      - **`job_name`**: The unique identifier for the model import job, which we generated in the previous step.\n",
    "   - Returns:\n",
    "      - A dictionary with the job's current status (`Completed` or `Failed`), any failure message if applicable, and the `importedModelArn` if the job is successful.\n",
    "\n",
    "3. **Periodic Status Check Loop**:\n",
    "   - We initialize `imported_model_arn` as `None`.\n",
    "   - In an infinite loop, we call `check_job_status` every `polling_interval` seconds to retrieve the latest job status.\n",
    "   - The current status and any failure message are printed to provide real-time feedback.\n",
    "\n",
    "4. **Job Completion or Failure Handling**:\n",
    "   - The loop exits if the job reaches a final state (`Completed` or `Failed`).\n",
    "      - If `Completed`, the `importedModelArn` is stored for further use, and a success message is printed.\n",
    "      - If `Failed`, an error message is printed along with any specific failure message.\n",
    "\n",
    "5. **Set `imported_model_id` for Further Use**:\n",
    "   - Once the job completes successfully, `imported_model_id` is assigned the value of `importedModelArn`, which will be used to interact with the model in subsequent steps.\n",
    "\n",
    "This monitoring process allows us to seamlessly track the job’s progress and handle any issues, ensuring that the model is ready for deployment as soon as the import is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259592e-43c2-4598-8536-9478e583920b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Use the job name from the response of create_model_import_job to track the job\n",
    "polling_interval = 30  # Time in seconds between each status check\n",
    "\n",
    "def check_job_status(job_name):\n",
    "    \"\"\"\n",
    "    Checks the status of the model import job and returns the current status, failure message, and imported model ARN if available.\n",
    "\n",
    "    Parameters:\n",
    "        job_name (str): The name of the model import job to check.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the status, failure message, and imported model ARN if the job is completed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        status_response = bedrock_client.get_model_import_job(jobIdentifier=job_name)\n",
    "        return {\n",
    "            \"status\": status_response[\"status\"],\n",
    "            \"failureMessage\": status_response.get(\"failureMessage\", \"\"),\n",
    "            \"importedModelArn\": status_response.get(\"importedModelArn\", None)\n",
    "        }\n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop to check the job status periodically\n",
    "print(f\"Checking status for job {job_name} every {polling_interval} seconds...\")\n",
    "imported_model_arn = None\n",
    "while True:\n",
    "    result = check_job_status(job_name)\n",
    "    if result is None:\n",
    "        print(\"Unable to retrieve job status. Exiting.\")\n",
    "        break\n",
    "\n",
    "    status = result[\"status\"]\n",
    "    failure_message = result[\"failureMessage\"]\n",
    "    imported_model_arn = result[\"importedModelArn\"]\n",
    "    print(f\"Current status: {status}\")\n",
    "\n",
    "    # Check if the job has reached a final state\n",
    "    if status in [\"Completed\", \"Failed\"]:\n",
    "        if status == \"Failed\" and failure_message:\n",
    "            print(f\"Job failed with message: {failure_message}\")\n",
    "            imported_model_arn = None  # Clear the ARN if the job failed\n",
    "        else:\n",
    "            print(f\"Job {job_name} finished with status: {status}\")\n",
    "            print(f\"Imported Model ARN: {imported_model_arn}\")\n",
    "        break\n",
    "\n",
    "    # Wait before the next status check\n",
    "    time.sleep(polling_interval)\n",
    "\n",
    "# Set the model ID to the imported model ARN if the job was successful\n",
    "if imported_model_arn:\n",
    "    imported_model_id = imported_model_arn  # Assign the model ARN to model_id for further use\n",
    "else:\n",
    "    print(\"Model import job did not complete successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac39fe-7012-449b-95dc-faf60c391443",
   "metadata": {},
   "source": [
    "## Section 13: Call Imported Model Using Amazon Bedrock Converse API\n",
    "\n",
    "In this section, we send a test request to our imported model on Amazon Bedrock using the Converse API. Since models may take some time to become ready after import, we implement a retry mechanism with exponential backoff to handle cases where the model is temporarily unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a297cea-86d0-45bd-84bf-2b8d25376fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=bedrock_region)\n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define the conversation messages, with user role correctly structured\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"You are an expert in customer support for insurance. Please help me understand my health insurance benefits.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the converse function with retry mechanism\n",
    "def converse_with_retry(messages, max_retries=10, initial_wait=30):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock Converse API with retry logic for the 'Model is not ready' error.\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): List of conversation messages.\n",
    "        max_retries (int): Maximum number of retry attempts.\n",
    "        initial_wait (int): Initial wait time (in seconds) between retries, doubled after each attempt.\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response from Bedrock if successful, None if all retries fail.\n",
    "    \"\"\"\n",
    "    retry_attempt = 0\n",
    "    wait_time = initial_wait\n",
    "\n",
    "    while retry_attempt < max_retries:\n",
    "        # Configure the conversation payload\n",
    "        converse_config = {\n",
    "            \"modelId\": imported_model_id,  # Use the imported model ARN as the model ID\n",
    "            \"messages\": messages,\n",
    "            \"inferenceConfig\": {\n",
    "                \"temperature\": 0.5\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nAttempt {retry_attempt + 1} of {max_retries}: Sending conversation request...\")\n",
    "\n",
    "        try:\n",
    "            response = bedrock_runtime_client.converse(**converse_config)\n",
    "            return response  # Return response if successful\n",
    "        except ClientError as e:\n",
    "            error_message = e.response['Error']['Message']\n",
    "            if \"Model is not ready for inference\" in error_message:\n",
    "                print(f\"Error: {error_message}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)  # Wait before retrying\n",
    "                retry_attempt += 1\n",
    "                wait_time *= 2  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"An error occurred: {error_message}\")\n",
    "                return None  # Exit if error is not 'Model not ready'\n",
    "\n",
    "    print(\"Max retries reached. Model is still not ready.\")\n",
    "    return None\n",
    "\n",
    "# Run the conversation with retry logic\n",
    "response = converse_with_retry(messages)\n",
    "\n",
    "# Function to print the response if received\n",
    "def print_converse_response(response):\n",
    "    if response:\n",
    "        print(f\"Response: {response['output']['message']['content'][0]['text']}\")\n",
    "    else:\n",
    "        print(\"No response received.\")\n",
    "\n",
    "# Print the response\n",
    "print_converse_response(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6f89e-e69d-47e2-b138-3103ccbe76dc",
   "metadata": {},
   "source": [
    "## Section 14: Call Imported Model Using Amazon Bedrock Converse Streaming API\n",
    "\n",
    "In this section, we demonstrate how to use Amazon Bedrock's Converse Streaming API to interact with our imported model in real-time. The Converse Streaming API allows us to receive responses from the model as they are generated, providing a more interactive experience.\n",
    "\n",
    "#### Steps in This Section\n",
    "\n",
    "1. **Define Sample Messages**:\n",
    "   - We create a list of sample messages to simulate different customer support queries related to insurance. Each message serves as a prompt for the model to respond to.\n",
    "\n",
    "2. **Inference Configuration**:\n",
    "   - Set parameters such as `temperature` and `top_k` in `inference_config` and `additional_model_fields` to control the model's response style and randomness.\n",
    "\n",
    "3. **Define the `stream_conversation` Function**:\n",
    "   - The `stream_conversation` function uses the `converse_stream` method to send messages to the model and receive responses in a streamed format.\n",
    "   - Parameters:\n",
    "      - **`bedrock_client`**: The Bedrock runtime client initialized for the specified region.\n",
    "      - **`model_id`**: The ARN of the imported model, used to identify the model for inference.\n",
    "      - **`messages`**: The list of conversation messages to send.\n",
    "      - **`inference_config` and `additional_model_fields`**: Configurations that determine the response style and sampling behavior.\n",
    "   - As the response stream is received, the function processes different event types:\n",
    "      - **`messageStart`**: Indicates the start of a new message from the model.\n",
    "      - **`contentBlockDelta`**: Contains partial content from the model’s response, which is printed in real-time.\n",
    "      - **`messageStop`**: Marks the end of a message, including information on the stop reason.\n",
    "      - **`metadata`**: Provides metrics on token usage and latency.\n",
    "\n",
    "4. **Run the Streaming API for Multiple Test Cases**:\n",
    "   - We loop through each sample message in `sample_messages`, calling `stream_conversation` for each to receive and display responses from the model.\n",
    "   - This approach allows us to simulate different customer queries and observe how the model responds in real-time to each prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41005eb4-5186-426b-9a9d-7c94dc48ed06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/71w502zzlj0q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #1: Can you help me understand my health insurance benefits?\n",
      "\n",
      "Role: assistant\n",
      "To effectively understand your health insurance benefits, please adhere to the following steps:\n",
      "\n",
      "1. Access the {{WEBSITE_URL}}.\n",
      "2. Enter your login information to access your account.\n",
      "3. Find the {{HEALTH_INSURANCE_SECTION}} within the dashboard.\n",
      "4. Select the {{VIEW_DETAILS_TAB}} to review your health insurance information.\n",
      "\n",
      "If you require additional assistance, do not hesitate to reach out to our support team via the {{HE"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/71w502zzlj0q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP_SECTION}} available on our website.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 19\n",
      "Output tokens: 121\n",
      "Total tokens: 140\n",
      "Latency: 2007 milliseconds\n",
      "\n",
      "Finished streaming response for sample #1: Can you help me understand my health insurance benefits?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #2: What does my policy cover if I need to see a specialist?\n",
      "\n",
      "Role: assistant\n",
      " To determine the coverage associated with your policy for visiting a specialist, please adhere to the following guidelines:\n",
      "\n",
      "1. Access your account by visiting {{WEBSITE_URL}}.\n",
      "2. Locate the section labeled {{MY_POLICIES_SECTION}}.\n",
      "3. Choose the specific policy you wish to examine from the displayed options.\n",
      "4. Go to the {{HEALTH_COVERAGE_SECTION}} section for relevant information.\n",
      "5. Identify the section titled {{SPECIALIST_COVERAGE_INFORMATION}} for details regarding specialist visits.\n",
      "\n",
      "Should you require additional support, please reach out to our customer service team by"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/71w502zzlj0q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " calling the designated customer support number.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 22\n",
      "Output tokens: 148\n",
      "Total tokens: 170\n",
      "Latency: 2328 milliseconds\n",
      "\n",
      "Finished streaming response for sample #2: What does my policy cover if I need to see a specialist?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #3: Are dental treatments covered in my current insurance plan?\n",
      "\n",
      "Role: assistant\n",
      " To verify the coverage of dental treatments under your insurance plan, please adhere to the following procedures:\n",
      "\n",
      "1. Access the {{WEBSITE_URL}}.\n",
      "2. Enter your account credentials to log in.\n",
      "3. Go to the {{COVERAGE_SECTION}} section of the website.\n",
      "4. Choose the dental insurance plan that you want to examine.\n",
      "5. Review the available information regarding dental treatments and their coverage.\n",
      "\n",
      "If you require further clarification or assistance, please reach out to customer"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/71w502zzlj0q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " support using the contact information available on the website.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 19\n",
      "Output tokens: 119\n",
      "Total tokens: 138\n",
      "Latency: 1893 milliseconds\n",
      "\n",
      "Finished streaming response for sample #3: Are dental treatments covered in my current insurance plan?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #4: How do I file a claim for a recent doctor visit?\n",
      "\n",
      "Role: assistant\n",
      " To initiate a claim for your recent doctor visit, please adhere to the following procedure:\n",
      "\n",
      "1. Access our website at {{WEBSITE_URL}}.\n",
      "2. Sign in to your account with your registered username and password.\n",
      "3. Locate the {{CLAIM_SECTION}} on the homepage.\n",
      "4. Choose the option for {{FILE_CLAIM_OPTION}}.\n",
      "5. Complete the claim form, ensuring all mandatory fields are filled in and relevant documents are attached.\n",
      "6. Verify your details and submit the claim form.\n",
      "\n",
      "Our dedicated"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/71w502zzlj0q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " claims team will evaluate your submission and respond promptly.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 21\n",
      "Output tokens: 135\n",
      "Total tokens: 156\n",
      "Latency: 2142 milliseconds\n",
      "\n",
      "Finished streaming response for sample #4: How do I file a claim for a recent doctor visit?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #5: Can you explain what deductible means in my policy?\n",
      "\n",
      "Role: assistant\n",
      " A deductible is the amount you are required to pay out of pocket before your insurance coverage begins. It is a specified amount that you agree to pay in the event of a claim. For instance, if your policy has a $500 deductible, you will need to pay the initial $500 for any claim you file before your insurance provider covers the remaining costs. This arrangement helps to ensure that you have a vested interest in keeping your claims to a minimum and maintaining a reasonable level of risk management.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 21\n",
      "Output tokens: 108\n",
      "Total tokens: 129\n",
      "Latency: 1844 milliseconds\n",
      "\n",
      "Finished streaming response for sample #5: Can you explain what deductible means in my policy?\n",
      "\n",
      "Finished streaming all test cases with model arn:aws:bedrock:us-west-2:603555443475:imported-model/71w502zzlj0q.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=\"us-west-2\")  # Replace with your region\n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define multiple conversation samples without system prompts\n",
    "sample_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you help me understand my health insurance benefits?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"What does my policy cover if I need to see a specialist?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Are dental treatments covered in my current insurance plan?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"How do I file a claim for a recent doctor visit?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you explain what deductible means in my policy?\"}]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Inference parameters\n",
    "inference_config = {\"temperature\": 0.5}\n",
    "additional_model_fields = {\"top_k\": 200}\n",
    "\n",
    "# Define the streaming converse function\n",
    "def stream_conversation(bedrock_client, model_id, messages, inference_config, additional_model_fields):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock converse_stream API and handles streaming response.\n",
    "\n",
    "    Parameters:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (list): The messages to send.\n",
    "        inference_config (dict): The inference configuration to use.\n",
    "        additional_model_fields (dict): Additional model fields to use.\n",
    "    \"\"\"\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "# Example usage of streaming for multiple test cases\n",
    "try:\n",
    "    for i, messages in enumerate(sample_messages, 1):\n",
    "        print(\"\\n\" + \"=\"*50)  # Line separator for clarity\n",
    "        print(f\"\\nStarting streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "        stream_conversation(\n",
    "            bedrock_runtime_client,\n",
    "            imported_model_id,  # Use the imported model ARN as model ID\n",
    "            messages,\n",
    "            inference_config,\n",
    "            additional_model_fields\n",
    "        )\n",
    "        print(f\"\\nFinished streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "except ClientError as err:\n",
    "    error_message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", error_message)\n",
    "    print(\"A client error occurred: \" + format(error_message))\n",
    "else:\n",
    "    print(f\"\\nFinished streaming all test cases with model {imported_model_id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d365b-c379-4aff-8d3b-fa85640a81c7",
   "metadata": {},
   "source": [
    "## Section 15: Running the Streamlit Application for Model Interaction\n",
    "\n",
    "In this section, we’ll generate a configuration file for our Streamlit application and then run the app to interact with our deployed models on Amazon SageMaker and Amazon Bedrock.\n",
    "\n",
    "### Step 1: Generate `app-config.json`\n",
    "\n",
    "The Streamlit application (`app.py`) requires certain configuration details, such as the SageMaker region, Bedrock region, SageMaker endpoint name, Bedrock model ID, and SageMaker model ID (`sagemaker_model_id`). These parameters are essential for connecting the app to our deployed models.\n",
    "\n",
    "Run the following code to create the configuration file `app-config.json` with the necessary details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da5b3104-3278-41e0-9ccc-26d1efb037f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to app-config.json\n"
     ]
    }
   ],
   "source": [
    "# Define configuration for Streamlit app\n",
    "config_data = {\n",
    "    \"sagemaker_region\": sagemaker_region,\n",
    "    \"bedrock_region\": bedrock_region,\n",
    "    \"sagemaker_endpoint_name\": sagemaker_endpoint_name,\n",
    "    \"bedrock_model_id\": imported_model_id,  # Use the imported model ARN as the Bedrock model ID\n",
    "    \"sagemaker_model_id\": HF_MODEL_ID_TO_PUSH  # Set the SageMaker model ID from the notebook variable\n",
    "}\n",
    "\n",
    "# Write configuration to app-config.json\n",
    "with open(\"app-config.json\", \"w\") as config_file:\n",
    "    json.dump(config_data, config_file, indent=4)\n",
    "\n",
    "print(\"Configuration saved to app-config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad786c5d-c21d-4ebb-82c8-5628dd5b0c91",
   "metadata": {},
   "source": [
    "### Step 2: Start the Streamlit Application\n",
    "\n",
    "Once `app-config.json` has been generated, you can launch the Streamlit app by running the following command in your terminal:\n",
    "\n",
    ">**streamlit run app.py --server.port 8501 --server.headless true**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84129c03-fc09-4d32-8179-1bf38696cb16",
   "metadata": {},
   "source": [
    "### Step 3: Access the Application\n",
    "\n",
    "Once Streamlit is running, you can access the application in your browser.\n",
    "\n",
    "- **If running locally**: Open your browser and navigate to **[http://localhost:8501](http://localhost:8501)**.\n",
    "\n",
    "- **If running on Amazon SageMaker Studio**:\n",
    "    1. Take the base URL of your SageMaker Studio environment. This typically looks like:\n",
    "       ```\n",
    "       https://<your-host-name>/jupyter/default/lab\n",
    "       ```\n",
    "    2. Replace `/lab` at the end of the URL with `/proxy/8501/`, so the final URL becomes:\n",
    "       ```\n",
    "       https://<your-host-name>/jupyter/default/proxy/8501/\n",
    "       ```\n",
    "       This URL will direct you to the Streamlit application within SageMaker Studio.\n",
    "\n",
    "- **For other cloud or remote environments**: You may need to configure port forwarding to access the application. Check the environment’s documentation for instructions on setting up port forwarding or consult the Streamlit URL provided by the specific platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18ff5c-681f-4356-ace1-78092dc0ce26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e4a80-eada-46cf-a664-b5a712c29bfd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we walked through the end-to-end process of deploying a fine-tuned Mistral model for insurance-related customer support on two different AWS platforms: Amazon SageMaker and Amazon Bedrock. This exercise provided a comprehensive look at how to optimize, deploy, and interact with a Mistral model using AWS’s specialized infrastructure.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "By completing this notebook, you should now have an understanding of:\n",
    "- **Model Optimization and Compilation**: How to prepare a model for efficient deployment on AWS Inferentia instances using Neuron, including the compilation of a Hugging Face model to a Neuron-compatible format with `optimum-cli`.\n",
    "- **Deploying Models on Amazon SageMaker**: How to set up and deploy the model on a `ml.inf2.xlarge` instance, leveraging SageMaker’s managed inference capabilities for low-latency, cost-effective serving.\n",
    "- **Converting Models for Amazon Bedrock**: How to convert the model to Hugging Face’s `safetensors` format and upload it to Amazon S3 for import into Amazon Bedrock.\n",
    "- **Using Amazon Bedrock's Converse API**: How to use the Bedrock Converse API to query the model, including real-time streaming of responses for enhanced interactivity.\n",
    "- **Building an Interactive Application with Streamlit**: How to connect both deployed models (SageMaker and Bedrock) to a custom chatbot interface, providing a hands-on experience for interacting with the model through a simple UI.\n",
    "\n",
    "### Value Proposition\n",
    "\n",
    "The techniques demonstrated in this notebook highlight the versatility and scalability of AWS’s machine learning and AI infrastructure:\n",
    "- **Cost-Effective Inference**: With Inferentia-based SageMaker deployment, we can run high-performance inferences while minimizing costs.\n",
    "- **Flexibility Across Platforms**: Amazon Bedrock provides a flexible environment for model hosting and deployment, enabling broader access to generative AI.\n",
    "- **Seamless User Interaction**: By connecting the models to a Streamlit app, we created an accessible interface that makes it easy for end-users to interact with the model and get real-time responses.\n",
    "\n",
    "This workflow demonstrated a full deployment lifecycle, from model preparation and compilation to deployment, interaction, and user application, empowering you to build scalable AI solutions on AWS. With these skills, you’re now well-equipped to deploy similar language models across different AWS environments to meet diverse operational needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557f153-e77f-4d96-bebb-918288814918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 4.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-311-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
