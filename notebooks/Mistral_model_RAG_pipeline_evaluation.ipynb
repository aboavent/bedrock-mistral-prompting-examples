{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Pipeline for Mistral Models with Q&A Automation and Model Evaluation using LlamaIndex and Ragas\n",
    "\n",
    "> *This notebook should work well in the `Data Science 3.0` kernel on Amazon SageMaker Studio. It requires Python v3.10+*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook is designed to evaluate the performance of the Retrieval-Augmented Generation (RAG) pipeline. The RAG pipeline leverages a retriever component to identify relevant context from a knowledge base and a generator component to produce fluent and informative responses based on the retrieved context.\n",
    "\n",
    "In this notebook, we will explore the RAG pipeline using the [LlamaIndex Evaluation library](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation), which provides a comprehensive set of tools for building and evaluating question-answering systems. Additionally, we will utilize the [Ragas](https://docs.Ragas.io/en/stable/) (RAG Assessment) framework, designed specifically for assessing the performance of RAG models.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Why We Need Evaluators like LlamaIndex Evaluators and Ragas\n",
    "\n",
    "We need evaluators like LlamaIndex evaluators and Ragas in a RAG pipeline primarily because language models, including those used in the RAG pipeline, can suffer from issues like hallucination, factual inconsistencies, and biases. Evaluators help us assess the performance and reliability of the RAG pipeline, ensuring that it provides accurate, relevant, and trustworthy responses.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "1. **Mitigating Hallucination**: Language models, especially large language models used in RAG pipelines, can sometimes generate plausible-sounding but factually incorrect or made-up information, a phenomenon known as hallucination. Evaluators help identify instances of hallucination by comparing the generated responses against the ground truth or the source knowledge base.\n",
    "\n",
    "2. **Ensuring Faithfulness and Factual Correctness**: Evaluators assess the faithfulness and factual correctness of the generated responses by comparing them with the source knowledge base or reference data. This is crucial in domains where accurate and reliable information is essential, such as healthcare, finance, or legal contexts.\n",
    "\n",
    "3. **Measuring Relevance and Context Understanding**: Evaluators can measure how relevant and contextually appropriate the generated responses are, given the input query and the retrieved context from the knowledge base. This helps identify cases where the RAG pipeline fails to understand the query or retrieves irrelevant information.\n",
    "\n",
    "4. **Quantifying Performance**: Evaluators provide quantitative metrics, such as accuracy, precision, recall, and F1-score, which allow for objective comparisons of different RAG pipeline configurations, retriever-generator model combinations, or training strategies.\n",
    "\n",
    "5. **Identifying Biases and Inconsistencies**: Evaluators can help identify biases and inconsistencies in the generated responses, which may arise due to biases in the training data or the language model itself. This is important for ensuring fairness and avoiding potentially harmful biases in the RAG pipeline's outputs.\n",
    "\n",
    "6. **Tailored Evaluation**: Frameworks like Ragas provide a structured approach to creating tailored test sets and evaluating the RAG pipeline's performance on specific types of queries or domains, allowing for more targeted assessments.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The primary objectives of this notebook are:\n",
    "\n",
    "1. **Implement the RAG pipeline**: We will set up the RAG pipeline using LlamaIndex, configuring the retriever and generator components according to best practices. Mistral 7B Instruct LLM will be utilized as the generator component in this example.\n",
    "\n",
    "2. **Create and evaluate the LlamaIndex Query Engine**: We will create a LlamaIndex Query Engine to facilitate efficient retrieval and generation of answers from the knowledge base.\n",
    "\n",
    "3. **Generate test sets with LlamaIndex RagDatasetGenerator and Ragas TestsetGenerator**: To evaluate the RAG pipeline's performance, we will generate synthetic test sets using LlamaIndex [RagDatasetGenerator module](https://docs.llamaindex.ai/en/stable/examples/llama_dataset/labelled-rag-datasets/). In this example, **Mistral 7B Instruct** model will be used to generate Q&A pairs (including \"ground truth\") for the synthetic dataset. Additionally, we will leverage the Ragas [TestsetGenerator module](https://docs.Ragas.io/en/latest/getstarted/testset_generation.html) to create tailored test sets aligned with our specific needs. As for the Ragas section, only Mistral 7B Instruct will be utilized.\n",
    "\n",
    "4. **Evaluate pipeline performance using LlamaIndex evaluators and Ragas**: We will leverage both the LlamaIndex evaluators and the Ragas framework to comprehensively assess the performance of the RAG pipeline on a range of question-answering tasks. Using the generated test sets, we will analyze metrics such as faithfulness, relevancy, correctness, and other relevant measures to gain insights into the pipeline's strengths and weaknesses.\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this notebook, we expect to achieve the following outcomes:\n",
    "\n",
    "1. A functional RAG pipeline implemented using LlamaIndex, capable of answering questions based on a knowledge base.\n",
    "\n",
    "2. A LlamaIndex Query Engine for efficient retrieval and generation of answers.\n",
    "\n",
    "3. Synthetic test sets generated using LlamaIndex RagDatasetGenerator and tailored test sets created with the Ragas TestsetGenerator.\n",
    "\n",
    "4. Comprehensive performance evaluation of the RAG pipeline using both LlamaIndex evaluators and the Ragas framework, including quantitative metrics such as faithfulness, relevancy, correctness, semantic similarity, and other relevant measures, as well as qualitative analysis.\n",
    "\n",
    "5. Insights into the impact of different pipeline configurations on performance and identification of limitations and potential areas for improvement in the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Setup and Requirements\n",
    "\n",
    "To start exploring RAG patterns with a practical example, we'll first install some libraries that might not be present in the default notebook kernel image:\n",
    "\n",
    "- [Amazon Bedrock](https://docs.aws.amazon.com/pythonsdk/) AWS Python SDKs `boto3` and `botocore` to be able to call the service\n",
    "- [LlamaIndex](https://docs.llamaindex.ai/en/stable/getting_started/installation/) is an open-source framework to help integrate LLMs with trusted data sources, and measure the performance of data-connected LLM use-cases\n",
    "- [Ragas](https://docs.Ragas.io/en/stable/) is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction) is an open-source framework for orchestrating common LLM patterns. In this example, it's only used with Ragas as an optional step to generate test dataset via langchain docs. The entire RAG pipeline in this example is implemented with LlamaIndex though.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade --quiet --no-cache-dir --force-reinstall \\\n",
    "    boto3 \\\n",
    "    botocore \\\n",
    "    langchain \\\n",
    "    langchain-aws \\\n",
    "    llama-index \\\n",
    "    llama-index-embeddings-langchain \\\n",
    "    llama-index-llms-bedrock \\\n",
    "    llama-index-embeddings-bedrock \\\n",
    "    llama-index-llms-langchain \\\n",
    "    ragas \\\n",
    "    spacy \\\n",
    "    datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything installed, let's import the required libraries and do some initial setup. This will come in handy later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import os  # For dealing with folder paths\n",
    "import sys\n",
    "import textwrap\n",
    "from io import StringIO\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "\n",
    "# External Dependencies:\n",
    "import nest_asyncio  # Needed for some asyncio-based libs to work in Jupyter notebooks\n",
    "nest_asyncio.apply()  # Enable asyncio-based libs to work properly in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, **Mistral 7B Instruct** is our default model, but feel free to pick any other available Mistral model to experiment with this RAG pipeline. You just need to change the `DEFAULT_MODEL` variable. \n",
    "\n",
    "You can also choose which Titan Embeddings model to be used throughtout this notebook. Just chage the `DEFAULT_EMBEDDINGS` if needed. By default, **Amazon Titan Text Embeddings V2** is utilized.\n",
    "\n",
    "Additionally, you may want to change the AWS region as well. If so, just change the `AWS_REGION` variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_mistral7b_id=\"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id=\"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2_id=\"mistral.mistral-large-2407-v1:0\"\n",
    "titan_embeddings_g1=\"amazon.titan-embed-text-v1\"\n",
    "titan_text_embeddings_v2=\"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "DEFAULT_MODEL=instruct_mistral7b_id\n",
    "DEFAULT_EMBEDDINGS=titan_text_embeddings_v2\n",
    "AWS_REGION=\"us-west-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download and pre-process documents with Titan Text Embeddings and LlamaIndex\n",
    "\n",
    "In this example, we'll create an in-memory semantic search index using:\n",
    "\n",
    "- [Amazon Titan Embeddings v2](https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-titan-text-embeddings-v2-amazon-bedrock/) on Amazon Bedrock, as a model to convert text of documents and user queries into numerical \"embedding\" vectors.\n",
    "- LlamaIndex [VectorStoreIndex](https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores/), to index the generated document vectors in-memory and retrieve the most similar documents for incoming queries/questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the sample document: Amazon's 2023 shareholder letter.\n",
    "\n",
    "In this example, we'll just use a single document for our RAG corpus: Amazon's 2023 annual letter to shareholders. Since the document itself is long, it'll be split into multiple separate entries in the search index.\n",
    "\n",
    "First, run the cell below to download the file locally. It'll also create the /data folder if it doesn't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.87file/s]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "URL_FILENAME_MAP = {\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf\": \"Amazon-com-Inc-2023-Shareholder-Letter.pdf\"\n",
    "}\n",
    "\n",
    "# Create the local folder if it doesn't exist\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "# Download files with progress bar\n",
    "for url, filename in tqdm(URL_FILENAME_MAP.items(), unit=\"file\"):\n",
    "    urlretrieve(url, os.path.join(DATA_ROOT, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can initially read the PDF files using LlamaIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=[\"data/Amazon-com-Inc-2023-Shareholder-Letter.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and vectorize the documents\n",
    "\n",
    "Text vectorization models are machine learning models that convert text data into numerical vector representations. This process, known as vectorization, allows the text to be processed and analyzed using mathematical operations and algorithms. They typically place an upper limit on the length of text they can process as a single item. Additionally, we want each search result to be reasonably short for embedding results in the answer generation LLM prompt later.\n",
    "\n",
    "To address this, we need to **split** the source document into shorter passages for indexing. LlamaIndex's [TokenTextSplitter](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/#tokentextsplitter) offers a utility for this purpose.\n",
    "\n",
    "Choosing **chunk_size** and **chunk_overlap** values for the TokenTextSplitter involves a trade-off between computational efficiency and preserving context.\n",
    "\n",
    "Larger chunk sizes can help capture more context and meaning within each chunk, which is beneficial for tasks that require understanding the broader context of the text. However, larger chunk sizes also increase the computational requirements for processing each chunk, as more tokens need to be processed at once.\n",
    "\n",
    "The chunk overlap helps maintain continuity and context between adjacent chunks by including some overlapping text. This overlap can be particularly useful when processing text sequentially, as it provides some context from the previous chunk, which can aid in understanding the current chunk. A larger overlap can help preserve more context, but it also increases redundancy and computational overhead, as the same tokens are processed multiple times across overlapping chunks.\n",
    "\n",
    "Therefore, the chunk size and overlap must be optimized for the specific NLP task at hand(e.g., text summarization and Q&A), where preserving context is crucial,  and strike a balance between preserving context and keeping the computational requirements manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=102\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert each document chunk into a single vector, we'll use the **Amazon Titan Text Embeddings V2 model**. It's a lightweight, efficient model ideal for high accuracy retrieval tasks at different dimensions. The model supports flexible embeddings sizes (256, 512, 1,024) and prioritizes accuracy maintenance at smaller dimension sizes, helping to reduce storage costs without compromising on accuracy. When reducing from 1,024 to 512 dimensions, Titan Text Embeddings V2 retains approximately 99% retrieval accuracy, and when reducing from 1,024 to 256 dimensions, the model maintains 97% accuracy. Additionally, Titan Text Embeddings V2 includes multilingual support for 100+ languages in pre-training as well as unit vector normalization for improving accuracy of measuring vector similarity.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "embed_model = BedrockEmbedding(model=DEFAULT_EMBEDDINGS,\n",
    "                               region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After configuring the splitting and vectorization parameters, we can proceed to set up and execute LlamaIndex's [IngestionPipeline](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/) to load and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 28 chunks from 11 source docs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'Amazon-com-Inc-2023-Shareholder-Letter.pdf',\n",
       " 'file_path': 'data/Amazon-com-Inc-2023-Shareholder-Letter.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 101160,\n",
       " 'creation_date': '2024-05-15',\n",
       " 'last_modified_date': '2024-05-15'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Create an ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, embed_model])\n",
    "\n",
    "# save\n",
    "pipeline.persist(\"./pipeline_storage\")\n",
    "\n",
    "# Run the ingestion pipeline\n",
    "doc_nodes = pipeline.run(documents=docs)\n",
    "\n",
    "print(f\"Ingested {len(doc_nodes)} chunks from {len(docs)} source docs\")\n",
    "doc_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating and Evaluating the LlamaIndex Query Engine\n",
    "\n",
    "After completing the chunking and vectorization processes, we can proceed to index the data into a queryable storage system.\n",
    "As the end-to-end querying process involves not only retrieving relevant documents but also generating textual answers from those documents, we need to define the configuration for Mistral at this stage. In this example, we will use **Mistral 7B Instruct**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 30\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "import boto3  # AWS SDK for Python\n",
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "model_kwargs_mistral = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 200,\n",
    "    \"max_tokens\": 8192  # Max response length\n",
    "}\n",
    "\n",
    "# Initialize the Mistral model to formulate final answer from search results\n",
    "llm = Bedrock(\n",
    "    model=DEFAULT_MODEL,\n",
    "    streaming=True,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_kwargs_mistral,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "# Set LlamaIndex settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model \n",
    "Settings.chunk_size=512\n",
    "\n",
    "# Create a vector index from documents\n",
    "vector_index = VectorStoreIndex.from_documents(documents=docs, \n",
    "                                               doc_nodes=doc_nodes)\n",
    "print(\"Number of nodes:\", len(vector_index.docstore.docs))\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=5,  # The top k=5 search results will be fed through to the LLM prompt\n",
    ")\n",
    "\n",
    "# store the created index to the local file system in case you need to re-load it into memory\n",
    "os.makedirs(\"./indices\", exist_ok=True)\n",
    "vector_index.storage_context.persist(\"./indices/amazon-shareholder-letters-2023-mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's execute some example questions against the vector index and the Mistral model. \n",
    "The query function takes the user's query as input and generates a response based on the relevant context and prompts. The answer should be present in the [source document](data/2023-Shareholder-Letter.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt viewing function\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}\" f\"**Text:** \"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Primitives are foundational building blocks that enable rapid innovation and experimentation in\n",
      "AWS. They are discrete, indivisible units that do one thing really well and are meant to be used\n",
      "together. By building primitives, AWS provides developers with maximum freedom and flexibility to\n",
      "create new applications and services. This approach is crucial for the overall AWS generative AI\n",
      "strategy because it allows for the democratization of AI technology and empowers both internal and\n",
      "external builders to transform customer experiences and invent new ones. The use of primitives also\n",
      "enables the composition of building blocks across businesses and in new combinations, leading to new\n",
      "possibilities for customers. Additionally, this approach requires patience as the benefits of the\n",
      "first few primitive services may not be immediately apparent to customers before the full potential\n",
      "of these building blocks is realized.\n"
     ]
    }
   ],
   "source": [
    "# Defines the query or question that we want to ask the model.\n",
    "query=\"What is the importance of building primitives for innovation and experimentation in AWS? Why is this approach so important for the overall AWS generative AI strategy?\"\n",
    "\n",
    "# Retrieves the prompts that will be used to generate a response to the query.\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "\n",
    "# Displays the prompts that were generated for the given query.\n",
    "display_prompt_dict(prompts_dict)\n",
    "\n",
    "# Executes the query against the vector index and the Mistral model.\n",
    "# The query function takes the user's query as input and generates a response based on the relevant context and prompts.\n",
    "response = query_engine.query(query)\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some additional examples of in-context questions, that is, questions to which answers will be found in the source document loaded into this RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the context provided, several premium brands started listing on Amazon in 2023. Some of these\n",
      "brands include Coach, Victoria's Secret, Pit Viper, Martha Stewart, Clinique, Lancôme, and Urban\n",
      "Decay.\n",
      "---\n",
      " The context information mentions several countries as being part of Amazon's emerging geographies,\n",
      "specifically India, Brazil, Australia, Mexico, Middle East, Africa, and Thailand.\n",
      "---\n",
      " Based on the context, Amazon is building several GenAI applications for customer and seller service\n",
      "productivity. Some of these applications include those that generate, customize, and edit high-\n",
      "quality images, advertising copy, and videos, as well as customer and seller service productivity\n",
      "apps. Additionally, Amazon Q, an expert on AWS, is mentioned as a capable work assistant that\n",
      "answers questions, summarizes data, carries on coherent conversation, and takes action. It is\n",
      "optimistic that much of this world-changing AI will be built on top of AWS.\n"
     ]
    }
   ],
   "source": [
    "# In-context questions:\n",
    "print_ww(query_engine.query(\"Which premium brands started listing on Amazon in 2023? (List at least 5 brands)\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"Which countries does Amazon see meaningful progress in as emerging geographies?\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"What are some of the GenAI applications that Amazon is building for customer and seller service productivity?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of questions non-related to the source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Amazon's international expansion in the Netherlands is not explicitly mentioned in the provided\n",
      "context information. However, the text does mention that Amazon sees meaningful progress in their\n",
      "emerging geographies, including India, Brazil, Australia, Mexico, Middle East, Africa, and other\n",
      "countries. The company aims to reduce delivery times and better tailor the customer experience in\n",
      "these markets. While the Netherlands is not specifically named, it can be inferred that Amazon is\n",
      "expanding its reach and operations in various international markets.\n",
      "---\n",
      " The context does not provide the name of the new Amazon large language model (LLM) mentioned in the\n",
      "text.\n",
      "---\n",
      " The context information provided does not mention the cash and investment balances at Amazon.com at\n",
      "the end of the year 1927. The information given pertains to the years 1996, 1997, and 2023, with the\n",
      "cash and investment balances at the end of 1997 being $125 million.\n"
     ]
    }
   ],
   "source": [
    "# Out-of-context questions:\n",
    "print_ww(query_engine.query(\"Tell me more about Amazon's international expansion in the Netherlands\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"What's the name of the new Amazon's LLM?\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"What was the total cash and investment balances at Amazon.com at the end of 1927?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The revenue growth rate in 2023 was bigger than the one in 1997 in all segments: North America (12%\n",
      "vs. 838%), International (11% vs. 738%), and AWS (13% vs. 838%). Therefore, the number of times the\n",
      "revenue growth rate in 2023 is bigger than the one in 1997 is three for North America and\n",
      "International, and infinite for AWS since the growth rate in 1997 was much higher. However, it's\n",
      "important to note that the absolute growth figures are significantly larger in 2023 due to the much\n",
      "larger base revenue in that year.\n"
     ]
    }
   ],
   "source": [
    "# Cross-context question:\n",
    "print_ww(query_engine.query(\"How many times is the revenue growth rate in 2023 bigger than the one in 1997?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG Automated Pipeline evaluation with LlamaIndex evaluators\n",
    "\n",
    "In the sections below, we'll show 4 automated evaluations available throught LlamaIndex. However, there are some additional metrics out-of-the-box that can be found [here](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/):\n",
    "\n",
    "1. **Faithfulness**: This metric verifies whether the final response is in agreement with (doesn't contradict) the retrieved document snippets.\n",
    "2. **Relevancy**: This metrics checks whether the response and retrieved content were relevant to the query.\n",
    "3. **Correctness**: This metric evaluates whether the generated answer is relevant and agreeing with a reference answer.\n",
    "4. **Semantic Similarity**: Evaluates the quality of a question answering system by comparing the similarity between embeddings of the generated answer and the reference answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Faithfulness to source documents\n",
    "\n",
    "The **Faithfulness** metric evaluates the coherence between the generated response and the source document snippets retrieved during the search process. This assessment is essential for identifying any discrepancies or hallucinations introduced by the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ----------------\n",
      "What technology transformation is Andy Jassy comparing the potential impact of Generative AI to?\n",
      "\n",
      "Answer: ----------------\n",
      " Andy Jassy is comparing the potential impact of Generative AI to that of the cloud technology\n",
      "transformation.\n",
      "\n",
      "----------------\n",
      "Evaluation Result: True\n",
      "Reasoning:\n",
      " YES\n",
      "The context mentions the development and use of foundation models (FMs) and generative AI (GenAI)\n",
      "applications, as well as the importance of having access to powerful compute resources and software\n",
      "tools for building and deploying these models. The information provided aligns with the context.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "query=\"What technology transformation is Andy Jassy comparing the potential impact of Generative AI to?\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(\"Question: ----------------\")\n",
    "print_ww(query)\n",
    "print(\"\\nAnswer: ----------------\")\n",
    "print_ww(response)\n",
    "print(\"\\n----------------\")\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "eval_result = faithfulness_evaluator.evaluate_response(response=response)\n",
    "\n",
    "print_ww(\"Evaluation Result:\", eval_result.passing)\n",
    "print_ww(f\"Reasoning:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Relevancy of response + source nodes to the query\n",
    "\n",
    "The **Relevancy** metric verifies the correspondence between the response and the retrieved source documents with the user's query. This evaluation is crucial for assessing whether the response properly addresses the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: True\n",
      "Reasoning:\n",
      " YES, the response is in line with the context information provided. Andy Jassy is comparing the\n",
      "potential impact of Generative AI to that of the cloud technology transformation in the context of\n",
      "the text.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=llm)\n",
    "eval_result = relevancy_evaluator.evaluate_response(query=query, response=response)\n",
    "\n",
    "print_ww(\"Evaluation Result:\", eval_result.passing)\n",
    "print_ww(f\"Reasoning:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Exploring differences between Relevancy and Faithfulness Evaluators\n",
    "\n",
    "To illustrate the contrast between **relevancy** and **faithfulness**, let's examine the following question which isn't in the source data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Amazon 'Bedrock Studio' will be launched?\n",
      " The context information provided does not mention a specific launch date for Amazon's Bedrock\n",
      "Studio. The text describes Bedrock as a service that is off to a strong start with tens of thousands\n",
      "of active customers and that Amazon continues to iterate on, adding new models and features. It also\n",
      "mentions that the majority of GenAI applications will ultimately be built by other companies using\n",
      "the primitives that Amazon is building in AWS.\n"
     ]
    }
   ],
   "source": [
    "# Out-of-context question:\n",
    "ooc_query = \"When Amazon 'Bedrock Studio' will be launched?\"\n",
    "\n",
    "ooc_response = query_engine.query(ooc_query)\n",
    "print_ww(ooc_query)\n",
    "print_ww(ooc_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_print_response(evaluator, ooc_response, ooc_query=None):\n",
    "    \"\"\"\n",
    "    Evaluates the relevancy or faithfulness of a response to a given query using\n",
    "    a provided evaluator, and prints the evaluation result, reasoning, and contexts.\n",
    "\n",
    "    Args:\n",
    "        evaluator (Union[RelevancyEvaluator, FaithfulnessEvaluator]): The evaluator\n",
    "            to use for evaluation.\n",
    "        ooc_response (str): The response to evaluate.\n",
    "        ooc_query (str, optional): The original out-of-context query. If not provided,\n",
    "            the response will be evaluated without a specific query context.\n",
    "\n",
    "    Returns:\n",
    "        Union[RelevancyEvaluationResult, FaithfulnessEvaluationResult]: The result\n",
    "            of the evaluation.\n",
    "    \"\"\"\n",
    "    evaluation_result = evaluator.evaluate_response(query=ooc_query, response=ooc_response)\n",
    "\n",
    "    print_ww(\"Evaluation Result:\", evaluation_result.passing)\n",
    "    print_ww(f\"Reasoning:\\n{evaluation_result.feedback}\")\n",
    "    #print_ww(\"Contexts:\\n\", evaluation_result.contexts)\n",
    "    #print_ww(\"Source:\\n\", ooc_response.source_nodes)\n",
    "\n",
    "    return evaluation_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Relevancy Evaluator** module is useful to measure if the response + source nodes match the query. Therefore, it helps measuring if the query was actually answered by the response. In this example, as the context information does not provide any details about the launch date of Amazon Bedrock Studio, then the evaluation result is **FALSE**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: False\n",
      "Reasoning:\n",
      " NO. The context information does not mention a launch date for Amazon Bedrock Studio.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query=\"When Amazon 'Bedrock Studio' will be launched?\", contexts=['service. Amazon Bedrock invented this layer and provides customers with the easiest way to build and scale\\nGenAI applications with the broadest selection of first- and third-party FMs, as well as leading ease-of-usecapabilities that allow GenAI builders to get higher quality model outputs more quickly. Bedrock is off to avery strong start with tens of thousands of active customers after just a few months. The team continuesto iterate rapidly on Bedrock, recently delivering Guardrails (to safeguard what questions applications will\\nanswer), Knowledge Bases (to expand models’ knowledge base with Retrieval Augmented Generation—or\\nRAG—and real-time queries), Agents (to complete multi-step tasks), and Fine-Tuning (to keep teaching\\nand refining models), all of which improve customers’ application quality. We also just added new modelsfrom Anthropic (their newly-released Claude 3 is the best performing large language model in the world),Meta (with Llama 2), Mistral, Stability AI, Cohere, and our own Amazon Titan family of FMs. Whatcustomers have learned at this early stage of GenAI is that there’s meaningful iteration required to build aproduction GenAI application with the requisite enterprise quality at the cost and latency needed. Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business.', 'Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business. These range from Rufus (our new, AI-powered shopping assistant),to an even more intelligent and capable Alexa, to advertising capabilities (making it simple with natural\\nlanguage prompts to generate, customize, and edit high-quality images, advertising copy, and videos), tocustomer and seller service productivity apps, to dozens of others. We’re also building several apps in AWS,including arguably the most compelling early GenAI use case—a coding companion. We recently launchedAmazon Q, an expert on AWS that writes, debugs, tests, and implements code, while also doingtransformations (like moving from an old version of Java to a new one), and querying customers’ variousdata repositories (e.g. Intranets, wikis, Salesforce, Amazon S3, ServiceNow, Slack, Atlassian, etc.) to answerquestions, summarize data, carry on coherent conversation, and take action. Q is the most capable workassistant available today and evolving fast.\\nWhile we’re building a substantial number of GenAI applications ourselves, the vast majority will ultimately\\nbe built by other companies. However, what we’re building in AWS is not just a compelling app or foundationmodel. These AWS services, at all three layers of the stack, comprise a set of primitives that democratize thisnext seminal phase of AI, and will empower internal and external builders to transform virtually everycustomer experience that we know (and invent altogether new ones as well). We’re optimistic that much ofthis world-changing AI will be built on top of AWS.', 'transform the customer health experience: Acute Care (via Amazon Clinic), Primary Care (via One\\nMedical), and a Pharmacy service to buy whatever medication a patient may need. Because of our growingsuccess, Amazon customers are now asking us to help them with all kinds of wellness and nutritionopportunities—which can be partially unlocked with some of our existing grocery building blocks, includingWhole Foods Market or Amazon Fresh.\\nAs a builder, it’s hard to wait for these building blocks to be built versus just combining a bunch of\\ncomponents together to solve a specific problem. The latter can be faster, but almost always slows you downin the future. We’ve seen this temptation in our robotics efforts in our fulfillment network. There aredozens of processes we seek to automate to improve safety, productivity, and cost. Some of the biggestopportunities require invention in domains such as storage automation, manipulation, sortation, mobilityof large cages across long distances, and automatic identification of items. Many teams would skip right to thecomplex solution, baking in “just enough” of these disciplines to make a concerted solution work, butwhich doesn’t solve much more, can’t easily be evolved as new requirements emerge, and that can’t be reusedfor other initiatives needing many of the same components. However, when you think in primitives, likeour Robotics team does, you prioritize the building blocks, picking important initiatives that can benefit fromeach of these primitives, but which build the tool chest to compose more freely (and quickly) for futureand complex needs. Our Robotics team has built primitives in each of the above domains that will belynchpins in our next set of automation, which includes multi-floor storage, trailer loading and unloading,large pallet mobility, and more flexible sortation across our outbound processes (including in vehicles). Theteam is also building a set of foundation AI models to better identify products in complex environments,optimize the movement of our growing robotic fleet, and better manage the bottlenecks in our facilities.\\nSometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question.', 'Thebottom layer is for developers and companies wanting to build foundation models (“FMs”). The\\nprimary primitives are the compute required to train models and generate inferences (or predictions), andthe software that makes it easier to build these models. Starting with compute, the key is the chip inside it. Todate, virtually all the leading FMs have been trained on Nvidia chips, and we continue to offer the broadestcollection of Nvidia instances of any provider. That said, supply has been scarce and cost remains an issue ascustomers scale their models and applications. Customers have asked us to push the envelope onprice-performance for AI chips, just as we have with Graviton for generalized CPU chips. As a result, we’vebuilt custom AI training chips (named Trainium) and inference chips (named Inferentia). In 2023, weannounced second versions of our Trainium and Inferentia chips, which are both meaningfully moreprice-performant than their first versions and other alternatives. This past fall, leading FM-maker, Anthropic,announced it would use Trainium and Inferentia to build, train, and deploy its future FMs. We alreadyhave several customers using our AI chips, including Anthropic, Airbnb, Hugging Face, Qualtrics, Ricoh,and Snap.\\nCustomers building their own FM must tackle several challenges in getting a model into production.\\nGetting data organized and fine-tuned, building scalable and efficient training infrastructure, and thendeploying models at scale in a low latency, cost-efficient manner is hard. It’s why we’ve built AmazonSageMaker, a managed, end-to-end service that’s been a game changer for developers in preparing their datafor AI, managing experiments, training models faster (e.g. Perplexity AI trains models 40% faster inSageMaker), lowering inference latency (e.g. Workday has reduced inference latency by 80% with SageMaker),and improving developer productivity (e.g. NatWest reduced its time-to-value for AI from 12-18 months tounder seven months using SageMaker).\\nThemiddle layer is for customers seeking to leverage an existing FM, customize it with their own data, and\\nleverage a leading cloud provider’s security and features to build a GenAI application—all as a managed', '(More on how we’re approaching GenAI and why webelieve we’ll be successful later in the letter.)\\nWe’re also making progress on many of our newer business investments that have the potential to be\\nimportant to customers and Amazon long-term. Touching on two of them:\\nWe have increasing conviction that Prime Video can be a large and profitable business on its own. This\\nconfidence is buoyed by the continued development of compelling, exclusive content (e.g. Thursday Night\\nFootball, Lord of the Rings, Reacher, The Boys, Citadel, Road House , etc.), Prime Video customers’ engagement\\nwith this content, growth in our marketplace programs (through our third-party Channels program, aswell as the broad selection of shows and movies customers rent or buy), and the addition of advertising inPrime Video.\\nIn October, we hit a major milestone in our journey to commercialize Project Kuiper when we launched two\\nend-to-end prototype satellites into space, and successfully validated all key systems and sub-systems—\\nrare in an initial launch like this. Kuiper is our low Earth orbit satellite initiative that aims to providebroadband connectivity to the 400-500 million households who don’t have it today (as well as governmentsand enterprises seeking better connectivity and performance in more remote areas), and is a very large revenueopportunity for Amazon. We’re on track to launch our first production satellites in 2024. We’ve still got along way to go, but are encouraged by our progress.\\nOverall, 2023 was a strong year, and I’m grateful to our collective teams who delivered on behalf of\\ncustomers. These results represent a lot of invention, collaboration, discipline, execution, and reimagination'], response=\" The context information provided does not mention a specific launch date for Amazon's Bedrock Studio. The text describes Bedrock as a service that is off to a strong start with tens of thousands of active customers and that Amazon continues to iterate on, adding new models and features. It also mentions that the majority of GenAI applications will ultimately be built by other companies using the primitives that Amazon is building in AWS.\", passing=False, feedback=' NO. The context information does not mention a launch date for Amazon Bedrock Studio.', score=0.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate relevance of result to the original question:\n",
    "\n",
    "evaluate_and_print_response(RelevancyEvaluator(llm=llm),\n",
    "                            ooc_query=ooc_query, \n",
    "                            ooc_response=ooc_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Faithfulness Evaluator** module is helpful to measure if the response from a query engine matches any source nodes. The context does not provide any information regarding the launch date of Amazon Bedrock Studio. Therefore, it can't answer that question.\n",
    "\n",
    "This helps to measure if the response has been **HALLUCINATED**, which hasn't happened in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: False\n",
      "Reasoning:\n",
      " NO. The context does not mention any specific launch date for Amazon's Bedrock Studio.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query=None, contexts=['service. Amazon Bedrock invented this layer and provides customers with the easiest way to build and scale\\nGenAI applications with the broadest selection of first- and third-party FMs, as well as leading ease-of-usecapabilities that allow GenAI builders to get higher quality model outputs more quickly. Bedrock is off to avery strong start with tens of thousands of active customers after just a few months. The team continuesto iterate rapidly on Bedrock, recently delivering Guardrails (to safeguard what questions applications will\\nanswer), Knowledge Bases (to expand models’ knowledge base with Retrieval Augmented Generation—or\\nRAG—and real-time queries), Agents (to complete multi-step tasks), and Fine-Tuning (to keep teaching\\nand refining models), all of which improve customers’ application quality. We also just added new modelsfrom Anthropic (their newly-released Claude 3 is the best performing large language model in the world),Meta (with Llama 2), Mistral, Stability AI, Cohere, and our own Amazon Titan family of FMs. Whatcustomers have learned at this early stage of GenAI is that there’s meaningful iteration required to build aproduction GenAI application with the requisite enterprise quality at the cost and latency needed. Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business.', 'Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business. These range from Rufus (our new, AI-powered shopping assistant),to an even more intelligent and capable Alexa, to advertising capabilities (making it simple with natural\\nlanguage prompts to generate, customize, and edit high-quality images, advertising copy, and videos), tocustomer and seller service productivity apps, to dozens of others. We’re also building several apps in AWS,including arguably the most compelling early GenAI use case—a coding companion. We recently launchedAmazon Q, an expert on AWS that writes, debugs, tests, and implements code, while also doingtransformations (like moving from an old version of Java to a new one), and querying customers’ variousdata repositories (e.g. Intranets, wikis, Salesforce, Amazon S3, ServiceNow, Slack, Atlassian, etc.) to answerquestions, summarize data, carry on coherent conversation, and take action. Q is the most capable workassistant available today and evolving fast.\\nWhile we’re building a substantial number of GenAI applications ourselves, the vast majority will ultimately\\nbe built by other companies. However, what we’re building in AWS is not just a compelling app or foundationmodel. These AWS services, at all three layers of the stack, comprise a set of primitives that democratize thisnext seminal phase of AI, and will empower internal and external builders to transform virtually everycustomer experience that we know (and invent altogether new ones as well). We’re optimistic that much ofthis world-changing AI will be built on top of AWS.', 'transform the customer health experience: Acute Care (via Amazon Clinic), Primary Care (via One\\nMedical), and a Pharmacy service to buy whatever medication a patient may need. Because of our growingsuccess, Amazon customers are now asking us to help them with all kinds of wellness and nutritionopportunities—which can be partially unlocked with some of our existing grocery building blocks, includingWhole Foods Market or Amazon Fresh.\\nAs a builder, it’s hard to wait for these building blocks to be built versus just combining a bunch of\\ncomponents together to solve a specific problem. The latter can be faster, but almost always slows you downin the future. We’ve seen this temptation in our robotics efforts in our fulfillment network. There aredozens of processes we seek to automate to improve safety, productivity, and cost. Some of the biggestopportunities require invention in domains such as storage automation, manipulation, sortation, mobilityof large cages across long distances, and automatic identification of items. Many teams would skip right to thecomplex solution, baking in “just enough” of these disciplines to make a concerted solution work, butwhich doesn’t solve much more, can’t easily be evolved as new requirements emerge, and that can’t be reusedfor other initiatives needing many of the same components. However, when you think in primitives, likeour Robotics team does, you prioritize the building blocks, picking important initiatives that can benefit fromeach of these primitives, but which build the tool chest to compose more freely (and quickly) for futureand complex needs. Our Robotics team has built primitives in each of the above domains that will belynchpins in our next set of automation, which includes multi-floor storage, trailer loading and unloading,large pallet mobility, and more flexible sortation across our outbound processes (including in vehicles). Theteam is also building a set of foundation AI models to better identify products in complex environments,optimize the movement of our growing robotic fleet, and better manage the bottlenecks in our facilities.\\nSometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question.', 'Thebottom layer is for developers and companies wanting to build foundation models (“FMs”). The\\nprimary primitives are the compute required to train models and generate inferences (or predictions), andthe software that makes it easier to build these models. Starting with compute, the key is the chip inside it. Todate, virtually all the leading FMs have been trained on Nvidia chips, and we continue to offer the broadestcollection of Nvidia instances of any provider. That said, supply has been scarce and cost remains an issue ascustomers scale their models and applications. Customers have asked us to push the envelope onprice-performance for AI chips, just as we have with Graviton for generalized CPU chips. As a result, we’vebuilt custom AI training chips (named Trainium) and inference chips (named Inferentia). In 2023, weannounced second versions of our Trainium and Inferentia chips, which are both meaningfully moreprice-performant than their first versions and other alternatives. This past fall, leading FM-maker, Anthropic,announced it would use Trainium and Inferentia to build, train, and deploy its future FMs. We alreadyhave several customers using our AI chips, including Anthropic, Airbnb, Hugging Face, Qualtrics, Ricoh,and Snap.\\nCustomers building their own FM must tackle several challenges in getting a model into production.\\nGetting data organized and fine-tuned, building scalable and efficient training infrastructure, and thendeploying models at scale in a low latency, cost-efficient manner is hard. It’s why we’ve built AmazonSageMaker, a managed, end-to-end service that’s been a game changer for developers in preparing their datafor AI, managing experiments, training models faster (e.g. Perplexity AI trains models 40% faster inSageMaker), lowering inference latency (e.g. Workday has reduced inference latency by 80% with SageMaker),and improving developer productivity (e.g. NatWest reduced its time-to-value for AI from 12-18 months tounder seven months using SageMaker).\\nThemiddle layer is for customers seeking to leverage an existing FM, customize it with their own data, and\\nleverage a leading cloud provider’s security and features to build a GenAI application—all as a managed', '(More on how we’re approaching GenAI and why webelieve we’ll be successful later in the letter.)\\nWe’re also making progress on many of our newer business investments that have the potential to be\\nimportant to customers and Amazon long-term. Touching on two of them:\\nWe have increasing conviction that Prime Video can be a large and profitable business on its own. This\\nconfidence is buoyed by the continued development of compelling, exclusive content (e.g. Thursday Night\\nFootball, Lord of the Rings, Reacher, The Boys, Citadel, Road House , etc.), Prime Video customers’ engagement\\nwith this content, growth in our marketplace programs (through our third-party Channels program, aswell as the broad selection of shows and movies customers rent or buy), and the addition of advertising inPrime Video.\\nIn October, we hit a major milestone in our journey to commercialize Project Kuiper when we launched two\\nend-to-end prototype satellites into space, and successfully validated all key systems and sub-systems—\\nrare in an initial launch like this. Kuiper is our low Earth orbit satellite initiative that aims to providebroadband connectivity to the 400-500 million households who don’t have it today (as well as governmentsand enterprises seeking better connectivity and performance in more remote areas), and is a very large revenueopportunity for Amazon. We’re on track to launch our first production satellites in 2024. We’ve still got along way to go, but are encouraged by our progress.\\nOverall, 2023 was a strong year, and I’m grateful to our collective teams who delivered on behalf of\\ncustomers. These results represent a lot of invention, collaboration, discipline, execution, and reimagination'], response=\" The context information provided does not mention a specific launch date for Amazon's Bedrock Studio. The text describes Bedrock as a service that is off to a strong start with tens of thousands of active customers and that Amazon continues to iterate on, adding new models and features. It also mentions that the majority of GenAI applications will ultimately be built by other companies using the primitives that Amazon is building in AWS.\", passing=False, feedback=\" NO. The context does not mention any specific launch date for Amazon's Bedrock Studio.\", score=0.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate faithfulness of response to retrieved content:\n",
    "\n",
    "evaluate_and_print_response(FaithfulnessEvaluator(llm=llm), \n",
    "                            ooc_response=ooc_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Automating Q&A Generation with LllamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LllamaInex provides tools designed to automatically generate datasets when provided with a set of documents to query. In the example below, we use the **RagDatasetGenerator** class to generate evaluation questions and reference answers(ground truth) from the source documents and the specified number of questions per node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Synthetic Test Set with LlamaIndex RagDatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c1ea08f6564bed98ff110e1aaeb3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes created: 30\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents=docs,\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=2, # set the number of questions per nodes\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print(f\"Number of nodes created: {len(dataset_generator.nodes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> ⏰ **Note:** The code block below is used to generate a dataset of questions and reference answers(\"ground truth\") from the source  document and may take some time to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:30<00:00,  1.03s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.64s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.52s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.50s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.01s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.10s/it]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.37s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.72s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.81s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.13s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.57s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.50s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 29.7 ms, total: 1.13 s\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the information provided in the share...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>The total revenue for Amazon in 2023 grew 12%...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which segment of Amazon's business had the hig...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>The segment of Amazon's business that had the...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Based on the information provided, which event...</td>\n",
       "      <td>[In our Stores business, customers have enthus...</td>\n",
       "      <td>In Q4 2023, Amazon held an exclusive event fo...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what ways did Amazon improve delivery speed...</td>\n",
       "      <td>[In our Stores business, customers have enthus...</td>\n",
       "      <td>In 2023, Amazon improved delivery speeds by r...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Based on the information provided in the conte...</td>\n",
       "      <td>[One is the benefit of regionalization, where ...</td>\n",
       "      <td>Amazon's regionalization efforts contributed ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In what ways did Amazon manage to reduce their...</td>\n",
       "      <td>[One is the benefit of regionalization, where ...</td>\n",
       "      <td>In 2023, Amazon managed to reduce their cost ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What geographical locations did Amazon's inter...</td>\n",
       "      <td>[expand selection and features, and move towar...</td>\n",
       "      <td>Based on the context provided, Amazon expande...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Which new offering did Amazon's Advertising bu...</td>\n",
       "      <td>[expand selection and features, and move towar...</td>\n",
       "      <td>Amazon's Advertising business introduced a ne...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What significant advancements were made in AWS...</td>\n",
       "      <td>[This work diminished short-term revenue, but ...</td>\n",
       "      <td>During the past year, AWS announced several s...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In what ways has Amazon seen growth and succes...</td>\n",
       "      <td>[This work diminished short-term revenue, but ...</td>\n",
       "      <td>According to the shareholder letter, Amazon h...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What significant milestone was reached in Octo...</td>\n",
       "      <td>[(More on how we’re approaching GenAI and why ...</td>\n",
       "      <td>In October 2023, Amazon successfully launched...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Which exclusive content has Amazon produced fo...</td>\n",
       "      <td>[(More on how we’re approaching GenAI and why ...</td>\n",
       "      <td>Based on the context provided in the sharehol...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Based on the context, how does Amazon define t...</td>\n",
       "      <td>[across Amazon. Yet, I think every one of us a...</td>\n",
       "      <td>According to the context provided, a builder ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>According to the text, what are primitives in ...</td>\n",
       "      <td>[across Amazon. Yet, I think every one of us a...</td>\n",
       "      <td>In the context of Amazon's approach to softwa...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Based on the context, how do Amazon describe t...</td>\n",
       "      <td>[While unafraid to invent from scratch, they h...</td>\n",
       "      <td>In the context provided, Amazon describes pri...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In the context, what were the limitations of A...</td>\n",
       "      <td>[While unafraid to invent from scratch, they h...</td>\n",
       "      <td>The limitations of Amazon's core retail servi...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Based on the information provided, what challe...</td>\n",
       "      <td>[Over time, we realized we could add broader s...</td>\n",
       "      <td>Amazon faced challenges in decoupling and del...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What was the purpose of the \"NPI\" mechanism in...</td>\n",
       "      <td>[Over time, we realized we could add broader s...</td>\n",
       "      <td>The \"NPI\" (New Initiative) mechanism in Amazo...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What motivated Amazon to create AWS primitive ...</td>\n",
       "      <td>[ecommerce components into true primitive serv...</td>\n",
       "      <td>Amazon was motivated to create AWS primitive ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Which Amazon primitive service was launched fi...</td>\n",
       "      <td>[ecommerce components into true primitive serv...</td>\n",
       "      <td>The first Amazon primitive service to be laun...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Based on the information provided, which custo...</td>\n",
       "      <td>[When we launched Amazon Elastic Compute Cloud...</td>\n",
       "      <td>Based on the information provided, Amazon aim...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How did Amazon's logistics primitives, such as...</td>\n",
       "      <td>[When we launched Amazon Elastic Compute Cloud...</td>\n",
       "      <td>Amazon's logistics primitives, such as wareho...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Based on the information provided, what servic...</td>\n",
       "      <td>[So, how do you build the right set of primiti...</td>\n",
       "      <td>Amazon introduced Fulfillment by Amazon (FBA)...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>In addition to logistics, what other primitive...</td>\n",
       "      <td>[So, how do you build the right set of primiti...</td>\n",
       "      <td>Based on the context provided, Amazon develop...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Based on the context, what steps has Amazon ta...</td>\n",
       "      <td>[A couple years ago, we launched Buy with Prim...</td>\n",
       "      <td>Amazon introduced a service called Buy with P...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How does Amazon manage its inventory between i...</td>\n",
       "      <td>[A couple years ago, we launched Buy with Prim...</td>\n",
       "      <td>Amazon manages its inventory between its ship...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What logistics services has Amazon offered to ...</td>\n",
       "      <td>[warehouses. And, in the last few years, our s...</td>\n",
       "      <td>Amazon offers external sellers several logist...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How does Amazon's Multi-Channel Fulfillment se...</td>\n",
       "      <td>[warehouses. And, in the last few years, our s...</td>\n",
       "      <td>Amazon's Multi-Channel Fulfillment (MCF) serv...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How does Amazon utilize its same-day fulfillme...</td>\n",
       "      <td>[These are all primitives that we’ve exposed t...</td>\n",
       "      <td>Amazon utilizes its same-day fulfillment faci...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>In what ways does Amazon plan to leverage its ...</td>\n",
       "      <td>[These are all primitives that we’ve exposed t...</td>\n",
       "      <td>Based on the context provided, Amazon views i...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>How does Amazon plan to make perishable shoppi...</td>\n",
       "      <td>[But, how else might we use this capability if...</td>\n",
       "      <td>Amazon plans to make perishable shopping as c...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>In what ways is Amazon's approach to building ...</td>\n",
       "      <td>[But, how else might we use this capability if...</td>\n",
       "      <td>Amazon's approach to building \"primitives\" in...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Based on the information provided, which speci...</td>\n",
       "      <td>[transform the customer health experience: Acu...</td>\n",
       "      <td>Based on the context information provided, Am...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>In the context of Amazon's robotics efforts, w...</td>\n",
       "      <td>[transform the customer health experience: Acu...</td>\n",
       "      <td>In Amazon's robotics efforts, the team is foc...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>What specific primitives is Amazon's Robotics ...</td>\n",
       "      <td>[However, when you think in primitives, likeou...</td>\n",
       "      <td>Amazon's Robotics team is focusing on several...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Which chip technology does Amazon currently us...</td>\n",
       "      <td>[However, when you think in primitives, likeou...</td>\n",
       "      <td>Amazon currently uses Nvidia chips for traini...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Which chip technology does Amazon Web Services...</td>\n",
       "      <td>[Thebottom layer is for developers and compani...</td>\n",
       "      <td>Amazon Web Services (AWS) primarily uses Nvid...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>What are some benefits of using Amazon SageMak...</td>\n",
       "      <td>[Thebottom layer is for developers and compani...</td>\n",
       "      <td>Amazon SageMaker offers several benefits for ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Which companies have currently adopted Amazon'...</td>\n",
       "      <td>[service. Amazon Bedrock invented this layer a...</td>\n",
       "      <td>Based on the context provided, the following ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What new features has Amazon recently added to...</td>\n",
       "      <td>[service. Amazon Bedrock invented this layer a...</td>\n",
       "      <td>Amazon has recently added several new feature...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Which companies have already adopted Amazon's ...</td>\n",
       "      <td>[Customersdon’t want only one model. They want...</td>\n",
       "      <td>Based on the context information provided, th...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>What unique capabilities does Amazon Q, an exp...</td>\n",
       "      <td>[Customersdon’t want only one model. They want...</td>\n",
       "      <td>Amazon Q is an expert AI system built on AWS ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Based on the text, which principles does Amazo...</td>\n",
       "      <td>[Intranets, wikis, Salesforce, Amazon S3, Serv...</td>\n",
       "      <td>Amazon prioritizes the following principles t...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>According to the text, what are some areas of ...</td>\n",
       "      <td>[Intranets, wikis, Salesforce, Amazon S3, Serv...</td>\n",
       "      <td>According to the text, Amazon is currently fo...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Based on the given text, which two principles ...</td>\n",
       "      <td>[The answerlies in our discipline around deepl...</td>\n",
       "      <td>Based on the given text, Amazon prioritizes h...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>According to the text, what percentage of the ...</td>\n",
       "      <td>[The answerlies in our discipline around deepl...</td>\n",
       "      <td>According to the text, more than 80% of the w...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Based on the information provided in the share...</td>\n",
       "      <td>[on-premises. These businesses will keep shift...</td>\n",
       "      <td>Andy Jassy, the President and CEO of Amazon.c...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>In what areas is Amazon anticipating a shift t...</td>\n",
       "      <td>[on-premises. These businesses will keep shift...</td>\n",
       "      <td>According to the Amazon shareholder letter, t...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Based on the 1997 Amazon.com shareholder lette...</td>\n",
       "      <td>[1997 LETTER TO SHAREHOLDERS\\n(Reprinted from ...</td>\n",
       "      <td>In the 1997 Amazon.com shareholder letter, th...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>According to the letter, what is Amazon.com's ...</td>\n",
       "      <td>[1997 LETTER TO SHAREHOLDERS\\n(Reprinted from ...</td>\n",
       "      <td>According to the letter, Amazon.com's long-te...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Based on the information provided in the share...</td>\n",
       "      <td>[It’s All About the Long Term\\nWe believe that...</td>\n",
       "      <td>Based on the information provided in the shar...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>According to the text, how does Amazon's focus...</td>\n",
       "      <td>[It’s All About the Long Term\\nWe believe that...</td>\n",
       "      <td>According to the text, Amazon's focus on the ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Based on the excerpt from Amazon's shareholder...</td>\n",
       "      <td>[• We will make bold rather than timid investm...</td>\n",
       "      <td>Based on the excerpt from Amazon's shareholde...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>In what ways did Amazon differentiate itself f...</td>\n",
       "      <td>[• We will make bold rather than timid investm...</td>\n",
       "      <td>According to the text, Amazon differentiated ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Based on the information provided, what signif...</td>\n",
       "      <td>[With this foundation, we would like to turn t...</td>\n",
       "      <td>In 1997, Amazon.com made several significant ...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>According to the text, which strategic partner...</td>\n",
       "      <td>[With this foundation, we would like to turn t...</td>\n",
       "      <td>According to the text, Amazon.com established...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Based on the information provided, how did Ama...</td>\n",
       "      <td>[Infrastructure\\nDuring 1997, we worked hard t...</td>\n",
       "      <td>The employee base of Amazon.com grew from 158...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>According to the text, what are some of Amazon...</td>\n",
       "      <td>[Infrastructure\\nDuring 1997, we worked hard t...</td>\n",
       "      <td>According to the text, Amazon.com's goals for...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Based on the information provided in the share...</td>\n",
       "      <td>[Such things aren’t meant tobe easy. We are in...</td>\n",
       "      <td>Based on the information provided in the shar...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>According to Jeff Bezos, what were some of the...</td>\n",
       "      <td>[Such things aren’t meant tobe easy. We are in...</td>\n",
       "      <td>According to the context provided, Jeff Bezos...</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "      <td>ai (mistral.mistral-7b-instruct-v0:2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   Based on the information provided in the share...   \n",
       "1   Which segment of Amazon's business had the hig...   \n",
       "2   Based on the information provided, which event...   \n",
       "3   In what ways did Amazon improve delivery speed...   \n",
       "4   Based on the information provided in the conte...   \n",
       "5   In what ways did Amazon manage to reduce their...   \n",
       "6   What geographical locations did Amazon's inter...   \n",
       "7   Which new offering did Amazon's Advertising bu...   \n",
       "8   What significant advancements were made in AWS...   \n",
       "9   In what ways has Amazon seen growth and succes...   \n",
       "10  What significant milestone was reached in Octo...   \n",
       "11  Which exclusive content has Amazon produced fo...   \n",
       "12  Based on the context, how does Amazon define t...   \n",
       "13  According to the text, what are primitives in ...   \n",
       "14  Based on the context, how do Amazon describe t...   \n",
       "15  In the context, what were the limitations of A...   \n",
       "16  Based on the information provided, what challe...   \n",
       "17  What was the purpose of the \"NPI\" mechanism in...   \n",
       "18  What motivated Amazon to create AWS primitive ...   \n",
       "19  Which Amazon primitive service was launched fi...   \n",
       "20  Based on the information provided, which custo...   \n",
       "21  How did Amazon's logistics primitives, such as...   \n",
       "22  Based on the information provided, what servic...   \n",
       "23  In addition to logistics, what other primitive...   \n",
       "24  Based on the context, what steps has Amazon ta...   \n",
       "25  How does Amazon manage its inventory between i...   \n",
       "26  What logistics services has Amazon offered to ...   \n",
       "27  How does Amazon's Multi-Channel Fulfillment se...   \n",
       "28  How does Amazon utilize its same-day fulfillme...   \n",
       "29  In what ways does Amazon plan to leverage its ...   \n",
       "30  How does Amazon plan to make perishable shoppi...   \n",
       "31  In what ways is Amazon's approach to building ...   \n",
       "32  Based on the information provided, which speci...   \n",
       "33  In the context of Amazon's robotics efforts, w...   \n",
       "34  What specific primitives is Amazon's Robotics ...   \n",
       "35  Which chip technology does Amazon currently us...   \n",
       "36  Which chip technology does Amazon Web Services...   \n",
       "37  What are some benefits of using Amazon SageMak...   \n",
       "38  Which companies have currently adopted Amazon'...   \n",
       "39  What new features has Amazon recently added to...   \n",
       "40  Which companies have already adopted Amazon's ...   \n",
       "41  What unique capabilities does Amazon Q, an exp...   \n",
       "42  Based on the text, which principles does Amazo...   \n",
       "43  According to the text, what are some areas of ...   \n",
       "44  Based on the given text, which two principles ...   \n",
       "45  According to the text, what percentage of the ...   \n",
       "46  Based on the information provided in the share...   \n",
       "47  In what areas is Amazon anticipating a shift t...   \n",
       "48  Based on the 1997 Amazon.com shareholder lette...   \n",
       "49  According to the letter, what is Amazon.com's ...   \n",
       "50  Based on the information provided in the share...   \n",
       "51  According to the text, how does Amazon's focus...   \n",
       "52  Based on the excerpt from Amazon's shareholder...   \n",
       "53  In what ways did Amazon differentiate itself f...   \n",
       "54  Based on the information provided, what signif...   \n",
       "55  According to the text, which strategic partner...   \n",
       "56  Based on the information provided, how did Ama...   \n",
       "57  According to the text, what are some of Amazon...   \n",
       "58  Based on the information provided in the share...   \n",
       "59  According to Jeff Bezos, what were some of the...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [Dear Shareholders:\\nLast year at this time, I...   \n",
       "1   [Dear Shareholders:\\nLast year at this time, I...   \n",
       "2   [In our Stores business, customers have enthus...   \n",
       "3   [In our Stores business, customers have enthus...   \n",
       "4   [One is the benefit of regionalization, where ...   \n",
       "5   [One is the benefit of regionalization, where ...   \n",
       "6   [expand selection and features, and move towar...   \n",
       "7   [expand selection and features, and move towar...   \n",
       "8   [This work diminished short-term revenue, but ...   \n",
       "9   [This work diminished short-term revenue, but ...   \n",
       "10  [(More on how we’re approaching GenAI and why ...   \n",
       "11  [(More on how we’re approaching GenAI and why ...   \n",
       "12  [across Amazon. Yet, I think every one of us a...   \n",
       "13  [across Amazon. Yet, I think every one of us a...   \n",
       "14  [While unafraid to invent from scratch, they h...   \n",
       "15  [While unafraid to invent from scratch, they h...   \n",
       "16  [Over time, we realized we could add broader s...   \n",
       "17  [Over time, we realized we could add broader s...   \n",
       "18  [ecommerce components into true primitive serv...   \n",
       "19  [ecommerce components into true primitive serv...   \n",
       "20  [When we launched Amazon Elastic Compute Cloud...   \n",
       "21  [When we launched Amazon Elastic Compute Cloud...   \n",
       "22  [So, how do you build the right set of primiti...   \n",
       "23  [So, how do you build the right set of primiti...   \n",
       "24  [A couple years ago, we launched Buy with Prim...   \n",
       "25  [A couple years ago, we launched Buy with Prim...   \n",
       "26  [warehouses. And, in the last few years, our s...   \n",
       "27  [warehouses. And, in the last few years, our s...   \n",
       "28  [These are all primitives that we’ve exposed t...   \n",
       "29  [These are all primitives that we’ve exposed t...   \n",
       "30  [But, how else might we use this capability if...   \n",
       "31  [But, how else might we use this capability if...   \n",
       "32  [transform the customer health experience: Acu...   \n",
       "33  [transform the customer health experience: Acu...   \n",
       "34  [However, when you think in primitives, likeou...   \n",
       "35  [However, when you think in primitives, likeou...   \n",
       "36  [Thebottom layer is for developers and compani...   \n",
       "37  [Thebottom layer is for developers and compani...   \n",
       "38  [service. Amazon Bedrock invented this layer a...   \n",
       "39  [service. Amazon Bedrock invented this layer a...   \n",
       "40  [Customersdon’t want only one model. They want...   \n",
       "41  [Customersdon’t want only one model. They want...   \n",
       "42  [Intranets, wikis, Salesforce, Amazon S3, Serv...   \n",
       "43  [Intranets, wikis, Salesforce, Amazon S3, Serv...   \n",
       "44  [The answerlies in our discipline around deepl...   \n",
       "45  [The answerlies in our discipline around deepl...   \n",
       "46  [on-premises. These businesses will keep shift...   \n",
       "47  [on-premises. These businesses will keep shift...   \n",
       "48  [1997 LETTER TO SHAREHOLDERS\\n(Reprinted from ...   \n",
       "49  [1997 LETTER TO SHAREHOLDERS\\n(Reprinted from ...   \n",
       "50  [It’s All About the Long Term\\nWe believe that...   \n",
       "51  [It’s All About the Long Term\\nWe believe that...   \n",
       "52  [• We will make bold rather than timid investm...   \n",
       "53  [• We will make bold rather than timid investm...   \n",
       "54  [With this foundation, we would like to turn t...   \n",
       "55  [With this foundation, we would like to turn t...   \n",
       "56  [Infrastructure\\nDuring 1997, we worked hard t...   \n",
       "57  [Infrastructure\\nDuring 1997, we worked hard t...   \n",
       "58  [Such things aren’t meant tobe easy. We are in...   \n",
       "59  [Such things aren’t meant tobe easy. We are in...   \n",
       "\n",
       "                                     reference_answer  \\\n",
       "0    The total revenue for Amazon in 2023 grew 12%...   \n",
       "1    The segment of Amazon's business that had the...   \n",
       "2    In Q4 2023, Amazon held an exclusive event fo...   \n",
       "3    In 2023, Amazon improved delivery speeds by r...   \n",
       "4    Amazon's regionalization efforts contributed ...   \n",
       "5    In 2023, Amazon managed to reduce their cost ...   \n",
       "6    Based on the context provided, Amazon expande...   \n",
       "7    Amazon's Advertising business introduced a ne...   \n",
       "8    During the past year, AWS announced several s...   \n",
       "9    According to the shareholder letter, Amazon h...   \n",
       "10   In October 2023, Amazon successfully launched...   \n",
       "11   Based on the context provided in the sharehol...   \n",
       "12   According to the context provided, a builder ...   \n",
       "13   In the context of Amazon's approach to softwa...   \n",
       "14   In the context provided, Amazon describes pri...   \n",
       "15   The limitations of Amazon's core retail servi...   \n",
       "16   Amazon faced challenges in decoupling and del...   \n",
       "17   The \"NPI\" (New Initiative) mechanism in Amazo...   \n",
       "18   Amazon was motivated to create AWS primitive ...   \n",
       "19   The first Amazon primitive service to be laun...   \n",
       "20   Based on the information provided, Amazon aim...   \n",
       "21   Amazon's logistics primitives, such as wareho...   \n",
       "22   Amazon introduced Fulfillment by Amazon (FBA)...   \n",
       "23   Based on the context provided, Amazon develop...   \n",
       "24   Amazon introduced a service called Buy with P...   \n",
       "25   Amazon manages its inventory between its ship...   \n",
       "26   Amazon offers external sellers several logist...   \n",
       "27   Amazon's Multi-Channel Fulfillment (MCF) serv...   \n",
       "28   Amazon utilizes its same-day fulfillment faci...   \n",
       "29   Based on the context provided, Amazon views i...   \n",
       "30   Amazon plans to make perishable shopping as c...   \n",
       "31   Amazon's approach to building \"primitives\" in...   \n",
       "32   Based on the context information provided, Am...   \n",
       "33   In Amazon's robotics efforts, the team is foc...   \n",
       "34   Amazon's Robotics team is focusing on several...   \n",
       "35   Amazon currently uses Nvidia chips for traini...   \n",
       "36   Amazon Web Services (AWS) primarily uses Nvid...   \n",
       "37   Amazon SageMaker offers several benefits for ...   \n",
       "38   Based on the context provided, the following ...   \n",
       "39   Amazon has recently added several new feature...   \n",
       "40   Based on the context information provided, th...   \n",
       "41   Amazon Q is an expert AI system built on AWS ...   \n",
       "42   Amazon prioritizes the following principles t...   \n",
       "43   According to the text, Amazon is currently fo...   \n",
       "44   Based on the given text, Amazon prioritizes h...   \n",
       "45   According to the text, more than 80% of the w...   \n",
       "46   Andy Jassy, the President and CEO of Amazon.c...   \n",
       "47   According to the Amazon shareholder letter, t...   \n",
       "48   In the 1997 Amazon.com shareholder letter, th...   \n",
       "49   According to the letter, Amazon.com's long-te...   \n",
       "50   Based on the information provided in the shar...   \n",
       "51   According to the text, Amazon's focus on the ...   \n",
       "52   Based on the excerpt from Amazon's shareholde...   \n",
       "53   According to the text, Amazon differentiated ...   \n",
       "54   In 1997, Amazon.com made several significant ...   \n",
       "55   According to the text, Amazon.com established...   \n",
       "56   The employee base of Amazon.com grew from 158...   \n",
       "57   According to the text, Amazon.com's goals for...   \n",
       "58   Based on the information provided in the shar...   \n",
       "59   According to the context provided, Jeff Bezos...   \n",
       "\n",
       "                      reference_answer_by  \\\n",
       "0   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "1   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "2   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "3   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "4   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "5   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "6   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "7   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "8   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "9   ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "10  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "11  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "12  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "13  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "14  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "15  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "16  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "17  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "18  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "19  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "20  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "21  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "22  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "23  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "24  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "25  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "26  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "27  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "28  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "29  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "30  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "31  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "32  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "33  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "34  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "35  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "36  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "37  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "38  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "39  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "40  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "41  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "42  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "43  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "44  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "45  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "46  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "47  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "48  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "49  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "50  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "51  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "52  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "53  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "54  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "55  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "56  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "57  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "58  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "59  ai (mistral.mistral-7b-instruct-v0:2)   \n",
       "\n",
       "                                 query_by  \n",
       "0   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "1   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "2   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "3   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "4   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "5   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "6   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "7   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "8   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "9   ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "10  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "11  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "12  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "13  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "14  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "15  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "16  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "17  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "18  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "19  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "20  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "21  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "22  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "23  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "24  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "25  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "26  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "27  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "28  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "29  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "30  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "31  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "32  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "33  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "34  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "35  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "36  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "37  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "38  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "39  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "40  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "41  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "42  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "43  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "44  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "45  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "46  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "47  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "48  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "49  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "50  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "51  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "52  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "53  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "54  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "55  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "56  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "57  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "58  ai (mistral.mistral-7b-instruct-v0:2)  \n",
       "59  ai (mistral.mistral-7b-instruct-v0:2)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Since there are 30 nodes generated from the source document, there should be a total of 60 questions in the generated dataset\n",
    "eval_questions = dataset_generator.generate_dataset_from_nodes()\n",
    "eval_questions.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run the evaluation on the dataset and visualize the results in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Response\n",
    "import pandas as pd\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
    "\n",
    "    eval_df = pd.DataFrame(columns=['Query', 'Response', 'Source', 'Evaluation Result'])\n",
    "        \n",
    "    new_record = {\n",
    "                    \"Query\": query,\n",
    "                    \"Response\": str(response),\n",
    "                    \"Source\": (\n",
    "                        response.source_nodes[0].node.get_content()[:100] + \"...\"\n",
    "                    ),\n",
    "                    \"Evaluation Result\": eval_result,\n",
    "                }\n",
    "    eval_df = eval_df._append(new_record, ignore_index=True)\n",
    "\n",
    "\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the first generated evaluation question with the **RelevancyEvaluator** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- Question ---------\n",
      "Based on the information provided in the shareholder letter, what was the percentage increase in\n",
      "total revenue for Amazon in 2023 compared to the previous year?\n",
      "\n",
      "--------- Response ---------\n",
      " The total revenue for Amazon in 2023 grew by 12% compared to the previous year.\n",
      "\n",
      "--------- Passed ---------\n",
      "True\n",
      "\n",
      "--------- Feedback ---------\n",
      " YES, the response is in line with the context information provided as the shareholder letter states\n",
      "that Amazon's total revenue grew 12% year-over-year in 2023.\n",
      "\n",
      "--------- Source ---------\n",
      "on-premises. These businesses will keep shifting online and into the cloud. In Media and\n",
      "Advertising,\n",
      "content will continue to migrate from linear formats to streaming. Globally, hundreds of millions of\n",
      "peoplewho don’t have adequate broadband access will gain that connectivity in the next few years.\n",
      "Last butcertainly not least, Generative AI may be the largest technology transformation since the\n",
      "cloud (whichitself, is still in the early stages), and perhaps since the Internet. Unlike the mass\n",
      "modernization of on-premises infrastructure to the cloud, where there’s work required to migrate,\n",
      "this GenAI revolution will bebuilt from the start on top of the cloud. The amount of societal and\n",
      "business benefit from the solutions thatwill be possible will astound us all.\n",
      "There has never been a time in Amazon’s history where we’ve felt there is so much opportunity to\n",
      "make our\n",
      "customers’ lives better and easier. We’re incredibly excited about what’s possible, focused on\n",
      "inventing thefuture, and look forward to working together to make it so.\n",
      "Sincerely,\n",
      "Andy Jassy\n",
      "President and Chief Executive OfficerAmazon.com, Inc.\n",
      "P .S. As we have always done, our original 1997 Shareholder Letter follows. What’s written there is\n",
      "as true\n",
      "today as it was in 1997.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_11c57_row0_col1, #T_11c57_row0_col2 {\n",
       "  inline-size: 600px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_11c57\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_11c57_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_11c57_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
       "      <th id=\"T_11c57_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
       "      <th id=\"T_11c57_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_11c57_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_11c57_row0_col0\" class=\"data row0 col0\" >Based on the information provided in the shareholder letter, what was the percentage increase in total revenue for Amazon in 2023 compared to the previous year?</td>\n",
       "      <td id=\"T_11c57_row0_col1\" class=\"data row0 col1\" > The total revenue for Amazon in 2023 grew by 12% compared to the previous year.</td>\n",
       "      <td id=\"T_11c57_row0_col2\" class=\"data row0 col2\" >Dear Shareholders:\n",
       "Last year at this time, I shared my enthusiasm and optimism for Amazon’s future. ...</td>\n",
       "      <td id=\"T_11c57_row0_col3\" class=\"data row0 col3\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f310be76e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator_mistral = RelevancyEvaluator(llm=llm)\n",
    "\n",
    "# pick the first question from the generated evaluation dataset\n",
    "eval_questions_df = eval_questions.to_pandas()\n",
    "eval_question = eval_questions_df.iloc[0,0] \n",
    "response_vector = query_engine.query(eval_question)\n",
    "\n",
    "eval_result = evaluator_mistral.evaluate_response(\n",
    "    query=eval_question, response=response_vector\n",
    ")\n",
    "\n",
    "# print results\n",
    "print(\"\\n--------- Question ---------\")\n",
    "print_ww(eval_question)\n",
    "print(\"\\n--------- Response ---------\")\n",
    "print_ww(str(response_vector))\n",
    "print(\"\\n--------- Passed ---------\")\n",
    "print_ww(str(eval_result.passing))\n",
    "print(\"\\n--------- Feedback ---------\")\n",
    "print_ww(str(eval_result.feedback))\n",
    "print(\"\\n--------- Source ---------\")\n",
    "print_ww(response.source_nodes[0].node.get_content())\n",
    "\n",
    "# show a DataFrame\n",
    "display_eval_df(eval_question, response_vector, eval_result.passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Run LlamaIndex Evaluations for Faithfulness, Relevancy, and Correctness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Correctness** metric checks the correctness of a question answering system, relying on a provided reference answer(\"ground truth\"), query, and response. It assigns a score from 1 to 5 (with higher values indicating better quality) alongside an explanation for the rating. Conversely, both the Relevancy and Faithfulness evaluators return a score between 0 and 1, with higher values indicating better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "\n",
    "def run_evaluations(evaluation_dataset: pd.DataFrame, query_engine, language_model):\n",
    "    \"\"\"Run a batch evaluation on a list of questions and reference answers using a provided query engine.\n",
    "\n",
    "    Args:\n",
    "        evaluation_dataset (DataFrame): A list of questions and reference_answers(ground truth) to evaluate.\n",
    "        query_engine (BaseQueryEngine): The query engine to use for answering the questions.\n",
    "        language_model (LLM): The language model to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the evaluation results, including the query,\n",
    "            generated answer, faithfulness evaluation, and relevancy evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    results_list = []\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(llm=language_model)\n",
    "    relevancy_evaluator = RelevancyEvaluator(llm=language_model)\n",
    "    correctness_evaluator = CorrectnessEvaluator(llm=language_model)\n",
    "\n",
    "    #for question, ground_truth in zip(evaluation_questions, evaluation_ground_truth):\n",
    "    for index, row in evaluation_dataset.iterrows():\n",
    "\n",
    "        question = row['query']  \n",
    "        ground_truth = row['reference_answer']  \n",
    "        \n",
    "        response = query_engine.query(question)\n",
    "        generated_answer = str(response)\n",
    "\n",
    "        # Faithfulness evaluator\n",
    "        faithfulness_results = faithfulness_evaluator.evaluate_response(response=response)\n",
    "        \n",
    "        # RelevancyEvaluator evaluator\n",
    "        relevancy_results = relevancy_evaluator.evaluate_response(query=question, response=response)\n",
    "        \n",
    "        # CorrectnessEvaluator evaluator\n",
    "        correctness_results = correctness_evaluator.evaluate(\n",
    "            query=question,\n",
    "            response=generated_answer,\n",
    "            reference=ground_truth\n",
    "        )\n",
    "\n",
    "        current_evaluation = {\n",
    "            \"query\": question,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"faithfulness\": faithfulness_results.passing,\n",
    "            \"faithfulness_feedback\": faithfulness_results.feedback,\n",
    "            \"faithfulness_score\": faithfulness_results.score,\n",
    "            \"relevancy\": relevancy_results.passing,\n",
    "            \"relevancy_feedback\": relevancy_results.feedback,\n",
    "            \"relevancy_score\": relevancy_results.score,\n",
    "            \"correctness\": correctness_results.passing,\n",
    "            \"correctness_feedback\": correctness_results.feedback,\n",
    "            \"correctness_score\": correctness_results.score,\n",
    "        }\n",
    "        results_list.append(current_evaluation)\n",
    "\n",
    "    evaluations_df = pd.DataFrame(results_list)\n",
    "    return evaluations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 344 ms, sys: 6.61 ms, total: 350 ms\n",
      "Wall time: 25.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>faithfulness_feedback</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>relevancy_feedback</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correctness_feedback</th>\n",
       "      <th>correctness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the information provided in the share...</td>\n",
       "      <td>The total revenue for Amazon in 2023 grew by ...</td>\n",
       "      <td>The total revenue for Amazon in 2023 grew 12%...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES\\nThe context mentions that Amazon's total...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Both the generated and reference answers are i...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which segment of Amazon's business had the hig...</td>\n",
       "      <td>The segment of Amazon's business that experie...</td>\n",
       "      <td>The segment of Amazon's business that had the...</td>\n",
       "      <td>False</td>\n",
       "      <td>The context does not directly support or cont...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer correctly identifies the ...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Based on the information provided, which event...</td>\n",
       "      <td>In Q4 2023, Amazon held an exclusive event fo...</td>\n",
       "      <td>In Q4 2023, Amazon held an exclusive event fo...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES (The context mentions the growth and expa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is fully relevant and cor...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what ways did Amazon improve delivery speed...</td>\n",
       "      <td>In 2023, Amazon made significant strides in i...</td>\n",
       "      <td>In 2023, Amazon improved delivery speeds by r...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the context supports the information tha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES. The response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is fully relevant to the ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Based on the information provided in the conte...</td>\n",
       "      <td>Amazon's regionalization efforts led to a sig...</td>\n",
       "      <td>Amazon's regionalization efforts contributed ...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the context supports the information abo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES. The response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is fully relevant and cor...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Based on the information provided in the share...   \n",
       "1  Which segment of Amazon's business had the hig...   \n",
       "2  Based on the information provided, which event...   \n",
       "3  In what ways did Amazon improve delivery speed...   \n",
       "4  Based on the information provided in the conte...   \n",
       "\n",
       "                                    generated_answer  \\\n",
       "0   The total revenue for Amazon in 2023 grew by ...   \n",
       "1   The segment of Amazon's business that experie...   \n",
       "2   In Q4 2023, Amazon held an exclusive event fo...   \n",
       "3   In 2023, Amazon made significant strides in i...   \n",
       "4   Amazon's regionalization efforts led to a sig...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0   The total revenue for Amazon in 2023 grew 12%...          True   \n",
       "1   The segment of Amazon's business that had the...         False   \n",
       "2   In Q4 2023, Amazon held an exclusive event fo...          True   \n",
       "3   In 2023, Amazon improved delivery speeds by r...          True   \n",
       "4   Amazon's regionalization efforts contributed ...          True   \n",
       "\n",
       "                               faithfulness_feedback  faithfulness_score  \\\n",
       "0   YES\\nThe context mentions that Amazon's total...                 1.0   \n",
       "1   The context does not directly support or cont...                 0.0   \n",
       "2   YES (The context mentions the growth and expa...                 1.0   \n",
       "3   YES, the context supports the information tha...                 1.0   \n",
       "4   YES, the context supports the information abo...                 1.0   \n",
       "\n",
       "   relevancy                                 relevancy_feedback  \\\n",
       "0       True   YES, the response is in line with the context...   \n",
       "1       True   YES, the response is in line with the context...   \n",
       "2       True   YES, the response is in line with the context...   \n",
       "3       True   YES. The response is in line with the context...   \n",
       "4       True   YES. The response is in line with the context...   \n",
       "\n",
       "   relevancy_score  correctness  \\\n",
       "0              1.0         True   \n",
       "1              1.0         True   \n",
       "2              1.0         True   \n",
       "3              1.0         True   \n",
       "4              1.0         True   \n",
       "\n",
       "                                correctness_feedback  correctness_score  \n",
       "0  Both the generated and reference answers are i...                5.0  \n",
       "1  The generated answer correctly identifies the ...                4.5  \n",
       "2  The generated answer is fully relevant and cor...                5.0  \n",
       "3  The generated answer is fully relevant to the ...                5.0  \n",
       "4  The generated answer is fully relevant and cor...                5.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Run evaluations for the first 5 rows only\n",
    "evaluation_results_df = run_evaluations(eval_questions_df.head(5), query_engine, llm)\n",
    "evaluation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query\n",
      "Which segment of Amazon's business had the highest percentage increase in revenue in 2023 compared\n",
      "to the previous year, and what was the exact dollar amount of revenue generated in each year?\n",
      "Additionally, what major events contributed to the revenue growth in this segment during the holiday\n",
      "season?\n",
      "\n",
      "--- Generated Answer\n",
      " The segment of Amazon's business that experienced the highest percentage increase in revenue year-\n",
      "over-year (YoY) in 2023 was the AWS segment. The revenue for AWS grew by 13% YoY from $80B in 2022\n",
      "to $91B in 2023.\n",
      "\n",
      "In the Stores business, which includes Amazon's retail operations, revenue also grew significantly.\n",
      "North America revenue increased by 12% YoY, International revenue grew by 11%, and the total retail\n",
      "revenue grew from $514B in 2022 to $575B in 2023.\n",
      "\n",
      "During the holiday season in Q4 2023, Amazon held Prime Big Deal Days, an exclusive event for Prime\n",
      "members, followed by an extended Black Friday and Cyber Monday shopping event open to all customers.\n",
      "These events became Amazon's largest revenue event ever, with customers saving nearly $24B across\n",
      "millions of deals and coupons, almost 70% more than the prior year. The focus on selection, price,\n",
      "and convenience, along with the early start to the holiday shopping season, contributed to the\n",
      "revenue growth in the Stores segment during this period.\n",
      "\n",
      "--- Ground Truth\n",
      " The segment of Amazon's business that had the highest percentage increase in revenue in 2023\n",
      "compared to the previous year was AWS (Amazon Web Services). AWS revenue increased 13% year-over-\n",
      "year from $80B in 2022 to $91B in 2023.\n",
      "\n",
      "During the holiday season in 2023, Amazon held two major events that contributed significantly to\n",
      "the revenue growth in the AWS segment. Prime Big Deal Days, an exclusive event for Prime members,\n",
      "and the extended Black Friday and Cyber Monday shopping event, open to all customers, both became\n",
      "Amazon's largest revenue events ever in this segment. However, the text does not provide specific\n",
      "revenue figures for these events.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a single row\n",
    "row=1\n",
    "print(\"\\n--- Query\")\n",
    "print_ww(evaluation_results_df.iloc[row,0])\n",
    "print(\"\\n--- Generated Answer\")\n",
    "print_ww(evaluation_results_df.iloc[row,1])\n",
    "print(\"\\n--- Ground Truth\")\n",
    "print_ww(evaluation_results_df.iloc[row,2])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Semantic Similarity Evaluation \n",
    "\n",
    "The **SemanticSimilarityEvaluator** evaluates the quality of a question answering system by comparing the similarity between embeddings of the generated answer and the reference answer. Since we have reference answers (ground truth) within the generated dataset created by calling **RagDataSetGenerator**, we can use this evaluator to check the quality of this Q&A dataset via semantic similarity. Behind the scenes, it calculates the similarity score between embeddings of the generated answer and the reference answer (ground truth).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Generated Answer\n",
      " In 2023, Amazon made significant strides in improving delivery speeds by re-architecting its\n",
      "network to store items closer to customers and expanding same-day facilities. This resulted in a\n",
      "nearly 70% year-over-year increase in the number of items delivered same day or overnight. The\n",
      "faster delivery times led to more frequent shopping on Amazon, as customers found it more convenient\n",
      "to fulfill their needs with the platform. This trend was particularly noticeable in the growth of\n",
      "Amazon's everyday essentials business, which experienced over 20% year-over-year growth in Q4 2023.\n",
      "The regionalization efforts also helped trim transportation distances, leading to a reduction in\n",
      "cost to serve on a per unit basis for the first time since 2018. This cost savings allowed Amazon to\n",
      "invest further in speed improvements and add more selection at lower prices, making it a more\n",
      "attractive option for customers.\n",
      "------ Ground Truth\n",
      " In 2023, Amazon improved delivery speeds by re-architecting its network to store items closer to\n",
      "customers and expanding same-day facilities. This resulted in more than 7 billion items being\n",
      "delivered same or next day, with over 4 billion in the U.S. and over 2 billion in Europe. The\n",
      "expansion of same-day facilities led to an increase of nearly 70% in the number of items delivered\n",
      "same day or overnight year over year. This fast delivery service encouraged customers to choose\n",
      "Amazon for their shopping needs more frequently, contributing to the growth of Amazon's everyday\n",
      "essentials business, which grew over 20% year over year in Q4 2023. Additionally, the\n",
      "regionalization efforts helped trim transportation distances, lowering the cost to serve. For the\n",
      "first time since 2018, Amazon reduced its cost to serve on a per unit basis globally, with a\n",
      "decrease of over $0.45 per unit year over year in the U.S. alone.\n",
      "------\n",
      "\n",
      "Score:  0.8825475612011769\n",
      "\n",
      "Passing:  True\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "\n",
    "similiratity_evaluator = SemanticSimilarityEvaluator()\n",
    "\n",
    "# Picks generated ansnwer and ground truth for the 3rd row of DataFrame. (feel free to use any other)\n",
    "response = evaluation_results_df.loc[3,'generated_answer'] # generated_answer column\n",
    "reference = evaluation_results_df.loc[3,'ground_truth'] # ground_truth column\n",
    "\n",
    "print_ww(\"------ Generated Answer\")\n",
    "print_ww(response)\n",
    "print_ww(\"------ Ground Truth\")\n",
    "print_ww(reference)\n",
    "print_ww(\"------\")\n",
    "\n",
    "similarity_result = await similiratity_evaluator.aevaluate(\n",
    "    response=response,\n",
    "    reference=reference,\n",
    ")\n",
    "\n",
    "print(\"\\nScore: \", similarity_result.score)\n",
    "print(\"\\nPassing: \", similarity_result.passing)  # default similarity threshold is 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Automating Q&A Generation and Evaluation with Ragas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragas (RAG Assessment) offers a framework designed to assess RAG pipelines. The evaluation metrics used by Ragas, similarly to the ones we used above with LlamaIndex evaluators, are a set of metrics designed to assess the performance and safety of AI applications, particularly in the context of grounded conversational AI systems. Below you will find the metric we will be using with Ragas in this notebook.\n",
    "\n",
    "1. **Faithfulness**\n",
    "Faithfulness measures the extent to which an AI assistant's response is faithful to the provided context or information. It evaluates whether the assistant's response is consistent with the given facts and does not contradict or deviate from the provided context.\n",
    "2. **Answer Relevancy**\n",
    "Answer relevancy assesses how relevant the AI assistant's response is to the user's query or question. It evaluates whether the response addresses the core intent of the query and provides information that is directly relevant to the user's needs.\n",
    "3. **Answer Similarity**\n",
    "Answer similarity measures the semantic similarity between the AI assistant's response and the expected or ideal answer. It evaluates how closely the generated response matches the desired or ground truth answer in terms of meaning and content.\n",
    "4. **Answer Correctness**\n",
    "Answer correctness evaluates the factual accuracy and correctness of the AI assistant's response. It assesses whether the information provided in the response is true, accurate, and free from factual errors or inconsistencies.\n",
    "5. **Context Precision**\n",
    "Context precision measures the precision of the AI assistant's response concerning the provided context. It evaluates how well the assistant's response incorporates and utilizes the relevant information from the given context, without including irrelevant or extraneous information.\n",
    "6. **Context Recall**\n",
    "Context recall measures the recall of the AI assistant's response concerning the provided context. It evaluates how much of the relevant information from the given context is included and covered in the assistant's response.\n",
    "7. **Context Entity Recall**\n",
    "Context entity recall specifically evaluates the AI assistant's ability to identify and include relevant entities (e.g., names, places, organizations) from the provided context in its response.\n",
    "8. **Harmfulness**\n",
    "Harmfulness assesses the potential for the AI assistant's response to cause harm or promote harmful or unethical content. It evaluates whether the response contains offensive, biased, or potentially harmful language or information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,    \n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_entity_recall\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_entity_recall,\n",
    "    harmfulness,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Generate a Synthetic Test Set with Ragas TestsetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging the **TestsetGenerator**, you can create test sets tailored to specific domains, topics, or use cases, ensuring that your AI assistant is thoroughly evaluated across a wide range of scenarios. The generated test sets can include various types of queries, contexts, and expected responses, allowing you to assess the assistant's performance metrics such as faithfulness, relevance, similarity, correctness, precision, recall, and potential harmfulness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "# init the Embeddings model\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    region_name=AWS_REGION,\n",
    "    model_id=DEFAULT_EMBEDDINGS\n",
    ")\n",
    "\n",
    "bedrock_model_mistral = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs=model_kwargs_mistral,\n",
    "    region_name=AWS_REGION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TestsetGenerator:** This module is responsible for generating test sets for evaluating RAG pipelines. It provides a variety of test generation strategies, including simple, reasoning, and multi-context strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=bedrock_model_mistral,\n",
    "    critic_llm=bedrock_model_mistral,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1 - Generate with LlamaIndex documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step will involve loading a set of documents and text chunks, allowing the Mistral model to generate potential questions based on these documents. Additionally, it will create reference answers (referred to as 'ground truth') for these questions, all based on the provided documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `distributions` parameter in the `generate_with_llamaindex_docs` function is used to specify the probability distribution for generating different types of questions or prompts during testing or evaluation. It's a dictionary where the keys represent the types of questions or prompts, and the values represent the corresponding probabilities. In the given example:\n",
    "\n",
    "```python\n",
    "distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "```\n",
    "\n",
    "This means that the generator will generate questions or prompts based on the following probabilities:\n",
    "\n",
    "#### simple\n",
    "- Probability: 0.5 (50%)\n",
    "- Likely straightforward questions requiring simple answers.\n",
    "\n",
    "#### reasoning \n",
    "- Probability: 0.25 (25%)\n",
    "- Questions may require some reasoning or logical thinking.\n",
    "\n",
    "#### multi_context\n",
    "- Probability: 0.25 (25%) \n",
    "- Questions may involve multiple contexts or require information from multiple sources.\n",
    "\n",
    "The sum of the probabilities equals 1.0 (100%). This allows controlling the mix of different question types generated for testing/evaluation purposes.By specifying the `distributions`, you can test the system's performance on different types of questions, ranging from simple to more complex multi-context scenarios. This can help identify areas for improvement and ensure robust performance across various question difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 683 ms, sys: 84.7 ms, total: 767 ms\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "print(\"Start dataset generation...\")\n",
    "\n",
    "testset = generator.generate_with_llamaindex_docs(documents=docs, \n",
    "                                                  test_size=4,\n",
    "                                                  raise_exceptions=False,\n",
    "                                                  with_debugging_logs=False, \n",
    "                                                  distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "print(\"Test dataset generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How did Amazon's Free Cash Flow improve in 2...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>Amazon's Free Cash Flow improved from -$12.8B ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'page_label': '1', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"How did Amazon's revenue grow in 2023 compar...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>Amazon's total revenue grew 12% year-over-year...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'page_label': '1', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which Amazon financial metric and business sec...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>In 2023, Amazon's AWS revenue experienced the ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'page_label': '1', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How did enhancements in customer experience an...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>In 2023, Amazon's Free Cash Flow (FCF) improve...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'page_label': '1', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   \"How did Amazon's Free Cash Flow improve in 2...   \n",
       "1   \"How did Amazon's revenue grow in 2023 compar...   \n",
       "2  Which Amazon financial metric and business sec...   \n",
       "3  How did enhancements in customer experience an...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "1  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "2  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "3  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Amazon's Free Cash Flow improved from -$12.8B ...         simple   \n",
       "1  Amazon's total revenue grew 12% year-over-year...         simple   \n",
       "2  In 2023, Amazon's AWS revenue experienced the ...      reasoning   \n",
       "3  In 2023, Amazon's Free Cash Flow (FCF) improve...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'page_label': '1', 'file_name': 'Amazon-com-...          True  \n",
       "1  [{'page_label': '1', 'file_name': 'Amazon-com-...          True  \n",
       "2  [{'page_label': '1', 'file_name': 'Amazon-com-...          True  \n",
       "3  [{'page_label': '1', 'file_name': 'Amazon-com-...          True  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = testset.to_pandas()\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - Generate with Langchain documents (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is optional since documents have been loaded via LlamaIndex above using _**generator.generate_with_llamaindex_docs()**_ method. This is only for those who want to use Ragas with Langchain instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "data = PyPDFLoader(\"data/Amazon-com-Inc-2023-Shareholder-Letter.pdf\").load_and_split()\n",
    "\n",
    "print(\"Start dataset generation...\")\n",
    "\n",
    "# generate testset\n",
    "testset_langchain = generator.generate_with_langchain_docs(documents=data, \n",
    "                                                           test_size=4,\n",
    "                                                           raise_exceptions=False,\n",
    "                                                           with_debugging_logs=False, \n",
    "                                                           distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "\n",
    "print(\"Test dataset generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_langchain = testset_langchain.to_pandas()\n",
    "df_langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Question ---------------\n",
      " \"How did Amazon's Free Cash Flow improve in 2023 compared to the previous year?\n",
      "Answer:  The Free Cash Flow (FCF) of Amazon improved significantly in 2023 compared to the previous\n",
      "year. The FCF increased from a negative value of $12.8 billion in 2022 to a positive value of $35.5\n",
      "billion in 2023, representing an improvement of $48.3 billion. This improvement can be attributed to\n",
      "the company's focus on cost optimization, operational efficiency, and revenue growth across its\n",
      "various business segments.\n",
      "\n",
      "--------------- Question ---------------\n",
      " \"How did Amazon's revenue grow in 2023 compared to the previous year, and in which segments did this growth occur?\n",
      "Answer:  In 2023, Amazon's total revenue grew by 12% year-over-year from $514B to $575B. This growth\n",
      "was observed across all segments. North America revenue increased by 12% YoY from $316B to $353B,\n",
      "International revenue grew by 11% YoY from $118B to $131B, and AWS revenue grew by 13% YoY from $80B\n",
      "to $91B.\n",
      "\n",
      "--------------- Question ---------------\n",
      "Which Amazon financial metric and business sector experienced the greatest growth in 2023? And how did they achieve a decrease in costs last year? Also, where did Amazon witness notable advancements in their sales during 2023, in both developed and emerging markets?\n",
      "Answer:  In 2023, Amazon's AWS revenue experienced the greatest growth, increasing by 13% year-over-\n",
      "year from $80B to $91B. This growth was driven by several factors, including the adoption of more\n",
      "efficient cloud solutions, the introduction of more powerful and cost-effective technologies like\n",
      "Graviton chips and S3 Intelligent Tiering, and the signing of new deals and migrations.\n",
      "\n",
      "Amazon was able to decrease costs in 2023 by focusing on regionalization and the expansion of same-\n",
      "day facilities. By storing items closer to customers and increasing the number of items delivered\n",
      "same day or overnight, Amazon was able to get items to customers faster, leading to more frequent\n",
      "purchases and a growing everyday essentials business. This, in turn, helped trim transportation\n",
      "distances and lower the cost to serve. In the U.S. alone, cost to serve was down by more than $0.45\n",
      "per unit year-over-year.\n",
      "\n",
      "Notable advancements in sales during 2023 were seen in both developed and emerging markets. In\n",
      "established countries, Amazon continued to expand selection and features and move toward\n",
      "profitability. Mexico became the latest international Stores locale to turn profitable. In emerging\n",
      "geographies like India, Brazil, Australia, Mexico, Middle East, Africa, and others, Amazon saw\n",
      "meaningful progress as these markets continued to grow.\n",
      "\n",
      "In the Stores business, Amazon continued to focus on selection, price, and convenience, leading to\n",
      "the broadest retail selection with hundreds of millions of products available. Prime members\n",
      "responded enthusiastically to exclusive events like Prime Big Deal Days and the extended Black\n",
      "Friday and Cyber Monday holiday shopping event, resulting in significant savings for customers and\n",
      "record-breaking revenue.\n",
      "\n",
      "In the Advertising business, growth was driven by sponsored ads, with the addition of Sponsored TV\n",
      "and the expansion of streaming TV advertising. This new offering allows brands to reach over 200\n",
      "million monthly viewers in popular entertainment offerings, including hit movies and shows, award-\n",
      "winning Amazon MGM Originals, and live sports like Thursday Night Football.\n",
      "\n",
      "In the AWS business, advancements included the announcement of next-generation generalized CPU chips\n",
      "(Graviton4) and AWS Trainium2 chips, which deliver improved performance and capabilities for machine\n",
      "learning training. Amazon also expanded its infrastructure footprint, offering 105 Availability\n",
      "Zones within 33 geographic Regions\n",
      "\n",
      "--------------- Question ---------------\n",
      "How did enhancements in customer experience and cost structure impact Free Cash Flow growth for Amazon in 2023, compared to the previous year?\n",
      "Answer:  In 2023, Amazon experienced significant improvements in both customer experience and cost\n",
      "structure, which positively impacted its Free Cash Flow (FCF). The company's total revenue grew by\n",
      "12% year-over-year (YoY), with operating income and FCF dramatically improving. Operating income\n",
      "increased by 201% YoY, and FCF improved from a deficit of $12.8B in 2022 to $35.5B in 2023,\n",
      "representing a substantial increase of $48.3B.\n",
      "\n",
      "The enhancements in customer experience played a crucial role in these financial improvements. In\n",
      "the Stores business, Amazon continued to focus on selection, price, and convenience, resulting in\n",
      "the broadest retail selection with hundreds of millions of products available. The company also\n",
      "introduced exclusive events like Prime Big Deal Days and extended Black Friday and Cyber Monday\n",
      "shopping events, which became its largest revenue event ever. These efforts led to increased\n",
      "customer engagement and more frequent shopping on Amazon.\n",
      "\n",
      "Additionally, Amazon made significant strides in reducing its cost to serve. By re-architecting its\n",
      "network to store items closer to customers and expanding same-day facilities, the company was able\n",
      "to deliver items faster, which in turn led to more frequent purchases. This regionalization and\n",
      "expansion of same-day delivery helped trim transportation distances and lower costs, resulting in a\n",
      "reduction in cost to serve on a per unit basis for the first time since 2018.\n",
      "\n",
      "These improvements in customer experience and cost structure contributed to the substantial increase\n",
      "in FCF for Amazon in 2023. By focusing on enhancing the shopping experience and optimizing costs,\n",
      "the company was able to attract and retain more customers, leading to increased sales and improved\n",
      "financial performance.\n"
     ]
    }
   ],
   "source": [
    "# add a new column to the DataFrame as some of the metrics \n",
    "# being evaluated require the answer column in the dataset\n",
    "df[\"answer\"]=''\n",
    "\n",
    "# iterate through df variable and populate answer column \n",
    "for i, row in df.iterrows():\n",
    "    # add a new column to the DataFrame 'df' created by the generator.generate_with_llamaindex_docs method\n",
    "    print(\"\\n--------------- Question ---------------\")\n",
    "    print(df.question[i])\n",
    "    response = query_engine.query(df.question[i])\n",
    "    df.loc[i, \"answer\"]=response.response\n",
    "    print_ww(\"Answer: \" + df.answer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How did Amazon's Free Cash Flow improve in 2...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>Amazon's Free Cash Flow improved from -$12.8B ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>The Free Cash Flow (FCF) of Amazon improved s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"How did Amazon's revenue grow in 2023 compar...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>Amazon's total revenue grew 12% year-over-year...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>In 2023, Amazon's total revenue grew by 12% y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which Amazon financial metric and business sec...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>In 2023, Amazon's AWS revenue experienced the ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>In 2023, Amazon's AWS revenue experienced the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How did enhancements in customer experience an...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>In 2023, Amazon's Free Cash Flow (FCF) improve...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>In 2023, Amazon experienced significant impro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   \"How did Amazon's Free Cash Flow improve in 2...   \n",
       "1   \"How did Amazon's revenue grow in 2023 compar...   \n",
       "2  Which Amazon financial metric and business sec...   \n",
       "3  How did enhancements in customer experience an...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "1  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "2  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "3  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Amazon's Free Cash Flow improved from -$12.8B ...         simple   \n",
       "1  Amazon's total revenue grew 12% year-over-year...         simple   \n",
       "2  In 2023, Amazon's AWS revenue experienced the ...      reasoning   \n",
       "3  In 2023, Amazon's Free Cash Flow (FCF) improve...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \\\n",
       "0  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "1  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "2  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "3  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "\n",
       "                                              answer  \n",
       "0   The Free Cash Flow (FCF) of Amazon improved s...  \n",
       "1   In 2023, Amazon's total revenue grew by 12% y...  \n",
       "2   In 2023, Amazon's AWS revenue experienced the...  \n",
       "3   In 2023, Amazon experienced significant impro...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "synthetic_dataset = Dataset.from_pandas(df)\n",
    "synthetic_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas Evaluation module\n",
    "\n",
    "The evaluation step leverages the questions from the generated test set to assess the performance of the RAG pipeline. In our example scenario, Mistral is utilized to validate the answers produced by our RAG pipeline against the questions provided in the previously created test set.\n",
    "\n",
    "Then, the LLM is tasked with evaluating how well the retrieved contexts align with the given questions. This step ensures that the contextual information provided to the LLM is relevant and appropriate for answering the queries.\n",
    "\n",
    "Finally, the answers generated by Mistral are compared against the ground truth answers included in the test set. This comparison allows for a comprehensive evaluation of the LLM's performance in generating accurate and relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa3eec52f964f9bad1718f4856beafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 525 ms, sys: 50.5 ms, total: 576 ms\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ragas import evaluate\n",
    "import nest_asyncio  \n",
    "\n",
    "# Only used when running on a jupyter notebook, otherwise you may want to remove this function\n",
    "nest_asyncio.apply()\n",
    "\n",
    "result = evaluate(\n",
    "    synthetic_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=bedrock_model_mistral,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "      <th>answer</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>harmfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How did Amazon's Free Cash Flow improve in 2...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>Amazon's Free Cash Flow improved from -$12.8B ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>The Free Cash Flow (FCF) of Amazon improved s...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892206</td>\n",
       "      <td>0.887614</td>\n",
       "      <td>0.721903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"How did Amazon's revenue grow in 2023 compar...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>Amazon's total revenue grew 12% year-over-year...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>In 2023, Amazon's total revenue grew by 12% y...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823959</td>\n",
       "      <td>0.978041</td>\n",
       "      <td>0.994510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which Amazon financial metric and business sec...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>In 2023, Amazon's AWS revenue experienced the ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>In 2023, Amazon's AWS revenue experienced the...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.545551</td>\n",
       "      <td>0.868412</td>\n",
       "      <td>0.563257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How did enhancements in customer experience an...</td>\n",
       "      <td>[Dear Shareholders:\\nLast year at this time, I...</td>\n",
       "      <td>In 2023, Amazon's Free Cash Flow (FCF) improve...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'creation_date': '2024-05-15', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>In 2023, Amazon experienced significant impro...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.786150</td>\n",
       "      <td>0.906229</td>\n",
       "      <td>0.414057</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   \"How did Amazon's Free Cash Flow improve in 2...   \n",
       "1   \"How did Amazon's revenue grow in 2023 compar...   \n",
       "2  Which Amazon financial metric and business sec...   \n",
       "3  How did enhancements in customer experience an...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "1  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "2  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "3  [Dear Shareholders:\\nLast year at this time, I...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Amazon's Free Cash Flow improved from -$12.8B ...         simple   \n",
       "1  Amazon's total revenue grew 12% year-over-year...         simple   \n",
       "2  In 2023, Amazon's AWS revenue experienced the ...      reasoning   \n",
       "3  In 2023, Amazon's Free Cash Flow (FCF) improve...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \\\n",
       "0  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "1  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "2  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "3  [{'creation_date': '2024-05-15', 'file_name': ...          True   \n",
       "\n",
       "                                              answer  faithfulness  \\\n",
       "0   The Free Cash Flow (FCF) of Amazon improved s...      1.000000   \n",
       "1   In 2023, Amazon's total revenue grew by 12% y...      1.000000   \n",
       "2   In 2023, Amazon's AWS revenue experienced the...      0.727273   \n",
       "3   In 2023, Amazon experienced significant impro...      1.000000   \n",
       "\n",
       "   answer_relevancy  answer_similarity  answer_correctness  context_precision  \\\n",
       "0          0.892206           0.887614            0.721903                1.0   \n",
       "1          0.823959           0.978041            0.994510                1.0   \n",
       "2          0.545551           0.868412            0.563257                1.0   \n",
       "3          0.786150           0.906229            0.414057                1.0   \n",
       "\n",
       "   context_recall  context_entity_recall  harmfulness  \n",
       "0             1.0               0.500000            0  \n",
       "1             1.0               0.210526            0  \n",
       "2             1.0               0.214286            1  \n",
       "3             1.0               0.090909            0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "### Benefits of Using LLM Evaluators\n",
    "\n",
    "Utilizing LLM evaluators like LlamaIndex's evaluator and Ragas can provide valuable insights into the performance and reliability of your RAG pipeline as you have explored along this example, particularly when evaluating the outputs of language models such as Mistral. Here are some key benefits:\n",
    "\n",
    "### Assessing Response Quality\n",
    "\n",
    "LLM evaluators can help assess the quality of the responses generated by your RAG pipeline by comparing them against various criteria, such as:\n",
    "\n",
    "- **Correctness**: Evaluating if the generated answer matches the reference or ground truth answer, if available\n",
    "- **Semantic Similarity**: Measuring the semantic similarity between the generated answer and the reference answer, even if they differ in wording.\n",
    "- **Faithfulness**: Determining if the generated answer is faithful to the retrieved context, avoiding hallucinations or irrelevant information.\n",
    "\n",
    "### Evaluating Retrieval Relevance\n",
    "These evaluators can also assess the relevance of the retrieved context to the input query, ensuring that the RAG pipeline is providing appropriate information to the language model.\n",
    "\n",
    "### Guideline Adherence\n",
    "Evaluators like LlamaIndex and Ragas, can evaluate if the generated responses adhere to specific guidelines or constraints, which is crucial for maintaining control over the language model's outputs.\n",
    "\n",
    "### Automated Question Generation\n",
    "Tools like LlamaIndex and Ragas can automatically generate questions based on your data, allowing you to test your RAG pipeline's performance on a diverse set of queries without manual effort.\n",
    "\n",
    "By leveraging these evaluation capabilities, you can systematically identify areas for improvement in your RAG pipeline, retrieval components, and ultimately enhance the accuracy and reliability of your Generative AI-powered applications."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
