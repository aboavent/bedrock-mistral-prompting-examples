{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530c6e00-d006-4aa9-a27d-eadb083eae97",
   "metadata": {},
   "source": [
    "# End-to-End Deployment of a Fine-Tuned Mistral Model on AWS with SageMaker, Bedrock, and Neuron Integration\n",
    "\n",
    "This notebook provides a comprehensive, hands-on guide to deploying a fine-tuned Mistral model for the insurance domain on AWS using Amazon SageMaker, Amazon Bedrock, and Hugging Face. The [Mistral-7B-Insurance](https://huggingface.co/bitext/Mistral-7B-Insurance) model is optimized to perform well in customer support scenarios, particularly for answering insurance-related queries. Also, it is a fine-tuned version of the [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "By following the steps in this notebook, you will learn how to optimize, deploy, and interact with a Mistral fine-tuned LLM using AWS’s robust infrastructure alongside Hugging Face tools. This workflow demonstrates how to leverage Hugging Face’s model repository for storage and distribution, AWS Inferentia instances on SageMaker for cost-effective inference, and Amazon Bedrock for flexible and scalable model serving. Finally, you will build a Streamlit application to interact with the model in real time, showcasing how AWS, Hugging Face, and Mistral services can support real-world customer support applications.\n",
    "\n",
    "## Goals\n",
    "\n",
    "The primary goals of this notebook are:\n",
    "1. **Model Preparation with Hugging Face**: Use Hugging Face’s tools to prepare, convert, and upload the model, enabling streamlined storage and versioning of large models.\n",
    "2. **Model Optimization for AWS Neuron**: Convert the fine-tuned Mistral model to a Neuron-compatible format using Hugging Face’s `optimum-cli`, optimizing it for performance on Inferentia-based SageMaker instances.\n",
    "3. **Deploying on Amazon SageMaker**: Set up a cost-effective, high-performance deployment on Amazon SageMaker using `ml.inf2.xlarge` instances, enabling fast inference at lower costs.\n",
    "4. **Model Import to Amazon Bedrock**: Convert the model from Hugging Face’s storage to `safetensors` format for Amazon Bedrock compatibility. Import it into Amazon Bedrock, allowing for seamless integration with other AWS services and enabling the use of the Converse API for inference.\n",
    "5. **Real-Time Interaction via Streamlit**: Build an interactive Streamlit application to test and compare model performance on both SageMaker and Bedrock, providing a hands-on experience for user interaction with deployed models.\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By completing this notebook, you will achieve the following outcomes:\n",
    "- **Experience with Hugging Face Model Handling**: Learn how to use Hugging Face’s model repository for storing, versioning, and retrieving large models, simplifying the model management process.\n",
    "- **Understanding Model Compilation and Optimization**: Gain hands-on experience with model compilation for AWS Neuron, optimizing the model for deployment on Inferentia instances, and learn how to convert models to the Hugging Face `safetensors` format for efficient storage and compatibility.\n",
    "- **Proficiency in Multi-Platform Deployment**: Deploy a model on both Amazon SageMaker and Amazon Bedrock, gaining flexibility in model serving options, and understanding the integration between Hugging Face and AWS services.\n",
    "- **Real-Time Streaming Inference**: Use Amazon Bedrock’s Converse Streaming API to interact with the model in real-time, receiving streamed responses that enhance user interaction.\n",
    "- **End-to-End Application Development**: Build a Streamlit application with a user-friendly interface that allows side-by-side testing of models deployed on both SageMaker and Bedrock, demonstrating the ease of integrating any LLM with AWS Generative AI services.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3c55b-beb5-45dc-a8c4-c7e85bd372d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700599cd-364b-4d4f-aece-4d9e6e74ae39",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import necessary libraries to facilitate model conversion, deployment, and API interactions across AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabc627-53f9-4f16-9c6c-17f0f629403e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers \\\n",
    "                sagemaker \\\n",
    "                boto3 \\\n",
    "                tiktoken \\\n",
    "                torch \\\n",
    "                blobfile \\\n",
    "                sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9de4538-d595-48bd-b113-abf0df631068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92884ef1-402b-4a43-b25c-1a4f799ee113",
   "metadata": {},
   "source": [
    "## Section 2: Export Variables for Mistral-7B-Insurance Deployment\n",
    "\n",
    "Define essential variables for model deployment, including model details, batch size, sequence length, and AWS-specific configurations.\n",
    "\n",
    "### Key Variables in This Notebook\n",
    "\n",
    "1. **`MODEL_ID`**: Hugging Face ID for the fine-tuned Mistral model in the insurance domain, used throughout the compilation and deployment process.\n",
    "\n",
    "2. **`BATCH_SIZE`**: Batch size for processing inputs simultaneously, optimizing performance on Inferentia.\n",
    "\n",
    "3. **`SEQUENCE_LENGTH`**: Maximum input length for each sequence, balancing memory and model capacity.\n",
    "\n",
    "4. **`MAX_TOTAL_TOKENS`**: Upper limit for tokens in each response, managing output length during inference.\n",
    "\n",
    "5. **`NUM_CORES`**: Number of Neuron cores used in parallel for efficient processing on Inferentia instances.\n",
    "\n",
    "6. **`HF_MODEL_ID_TO_PUSH`**: The Hugging Face model name(e.g., \"aboavent/Mistral-7B-Insurance-neuron\") under which the compiled model will be saved.\n",
    "\n",
    "7. **`HF_TOKEN`**: Hugging Face authentication token for model access and upload. Check out [Creating User Access Tokens](https://huggingface.co/docs/hub/en/security-tokens) for details on how to create one for you Hugging Face account.\n",
    "\n",
    "8. **`PRECISION`**: Model precision (`fp16`), reducing memory while maintaining inference speed.\n",
    "\n",
    "9. **`MODEL_OUTPUT_NAME`**: Designated name for the compiled model, aiding in file tracking and reference.\n",
    "\n",
    "10. **`COMPILED_MODEL_OUTPUT_PATH`**: Directory path for saving the compiled model after export, used in later deployment steps.\n",
    "\n",
    "11. **`sagemaker_region`**: The region for SageMaker where the model will be deployed.\n",
    "\n",
    "12. **`bedrock_region`**: The region for Amazon Bedrock where the model will be imported later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2cbc89-59d2-4dcd-a335-dcc67f3d569d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"bitext/Mistral-7B-Insurance\"\n",
    "BATCH_SIZE = 4\n",
    "SEQUENCE_LENGTH = 2048\n",
    "MAX_TOTAL_TOKENS = 4096  # Set independently for the total token limit\n",
    "NUM_CORES = 2\n",
    "HF_MODEL_ID_TO_PUSH = \"aboavent/Mistral-7B-Insurance-neuron\" # Set your HF model name\n",
    "HF_TOKEN = \"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\"\n",
    "PRECISION = \"fp16\"\n",
    "MODEL_OUTPUT_NAME = \"Mistral-7B-Insurance-neuron\"\n",
    "COMPILED_MODEL_OUTPUT_PATH = f\"./{MODEL_OUTPUT_NAME}\" \n",
    "sagemaker_region = \"us-east-2\"  # Region for SageMaker endpoint\n",
    "bedrock_region = \"us-west-2\"    # Region for Bedrock model\n",
    "\n",
    "os.environ[\"AWS_REGION\"] = sagemaker_region \n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = sagemaker_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d6563-cd4e-4158-b133-ee2d12a879c3",
   "metadata": {},
   "source": [
    "## Section 3: Model Compilation with Optimum CLI (Optional)\n",
    "\n",
    "#### Important Notes before you get started\n",
    "\n",
    "> This section is *optional* since the compiled model was already made available at [Hugging Face - Mistral-7b-Insurance-Neuron](https://huggingface.co/aboavent/Mistral-7B-Insurance-neuron), however I strongly recommend you to read through this section as it'll provide additional insights on how to compile an existing model to the [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/) architecture.\n",
    "\n",
    "> ⏰ Note: If you want to compile this model from scratch, please note that this step can take about 30-45 minutes to complete.\n",
    "\n",
    "In this section, we compile the Mistral model using Hugging Face’s `optimum-cli` tool, which prepares it for efficient inference on AWS Inferentia instances with AWS Neuron. Model compilation is a critical step in optimizing performance, as it converts the model into a format compatible with AWS Neuron, enabling accelerated inference on hardware optimized for deep learning. As we are working with a Mistral LLM that requires full loading into compute memory for compilation, we recommend using a high-memory instance, such as **`inf2.24xlarge`**, **`inf2.48xlarge`**, or **`trn1.32xlarge`**. These instances offer substantial memory and Neuron cores, providing the resources necessary for efficient model processing and compiling tokenizers, `config.json`, and other essential components. You can perform this compilation step on an EC2 instance provisioned with [AWS Deep Learning AMI Neuron](https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html). After logging in to the instance, activate the pre-built transformers environment for Inf2 and Trn* by running: \n",
    "```bash\n",
    "source /opt/aws_neuronx_venv_transformers_neuronx/bin/activate\n",
    "```\n",
    "\n",
    "#### Steps for compiling the model into the AWS Neuron architecture\n",
    "\n",
    "1. **Configure Compilation Parameters**:\n",
    "   - To guide the model compilation process, we define several key parameters: `MODEL_ID` to specify the pretrained Mistral model from Hugging Face, `BATCH_SIZE` to determine the number of inputs processed per batch, and `SEQUENCE_LENGTH` to set the maximum input length, impacting both memory and processing time. We set `NUM_CORES` to define the number of Neuron cores used for parallelization, and `PRECISION` to `fp16` to balance memory usage and performance. Finally, `COMPILED_MODEL_OUTPUT_PATH` specifies where the Neuron-compatible compiled model will be saved.\n",
    "\n",
    "2. **Set Up Environment and Install Neuron Requirements**:\n",
    "   - Set up `pip` to use the Neuron repository and install `optimum` with NeuronX support to ensure compatibility with Inferentia hardware.\n",
    "\n",
    "3. **Log in to Hugging Face**:\n",
    "   - Use the Hugging Face CLI to authenticate and enable access to the model repository.\n",
    "\n",
    "4. **Run Optimum CLI for Model Compilation**:\n",
    "   - We use the Hugging Face `optimum-cli` tool to compile the model for AWS Inferentia hardware. The command includes the model ID, batch size, sequence length, number of cores, precision level, and output path.\n",
    "\n",
    "5. **Create a New Repository on Hugging Face and Upload the Compiled Model**:\n",
    "   - After compilation, create a new repository on Hugging Face to store the compiled model and upload the model files.\n",
    "\n",
    "```bash\n",
    "# Set Neuron repository for pip\n",
    "pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "\n",
    "# Install optimum with NeuronX support and update dependencies\n",
    "pip install --upgrade-strategy eager optimum[neuronx]\n",
    "\n",
    "# Log in to Hugging Face\n",
    "huggingface-cli login --token $HF_TOKEN\n",
    "\n",
    "# Export model for Neuron\n",
    "optimum-cli export neuron \\\n",
    "    -m $MODEL_ID \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --sequence_length $SEQUENCE_LENGTH \\\n",
    "    --num_cores $NUM_CORES \\\n",
    "    --auto_cast_type $PRECISION \\\n",
    "    --trust-remote-code \\\n",
    "    $COMPILED_MODEL_OUTPUT_PATH\n",
    "\n",
    "# Create a new repository on Hugging Face and upload the compiled model \n",
    "huggingface-cli repo create $MODEL_OUTPUT_NAME\n",
    "huggingface-cli upload $HF_MODEL_ID_TO_PUSH $COMPILED_MODEL_OUTPUT_PATH ./\n",
    "\n",
    "```\n",
    "\n",
    "#### Why Compile the Model?\n",
    "\n",
    "- **Optimized for Inferentia**: Compilation enables the model to leverage the specialized capabilities of AWS Inferentia, reducing inference latency and improving cost-efficiency.\n",
    "- **Memory Efficiency**: Using `fp16` precision and Neuron-compatible optimizations helps reduce memory usage, allowing for higher throughput.\n",
    "- **Scalability**: A compiled model is more scalable and cost-effective in production, making it ideal for customer support applications.\n",
    "\n",
    "For more details on Hugging Face Optimum CLI and its features, refer to the official [Optimum Neuron CLI documentation](https://huggingface.co/docs/optimum-neuron/guides/export_model). Also, check out the [Optimum Neuron official documentation](https://huggingface.co/docs/optimum-neuron/index) and the [Optmimum Neuron github repo]( https://github.com/huggingface/optimum-neuron/tree/main) with additional samples and tutorials for your reference. Some additional examples can also be found at [Optimum Neuron Sample Notebooks](https://github.com/huggingface/optimum-neuron/tree/main/notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7424ec-946f-4582-94aa-afe433ca6815",
   "metadata": {},
   "source": [
    "## Section 4: Deploy Model to SageMaker Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ebde0-1a86-42b8-b3ae-e3c2e5a0cddf",
   "metadata": {},
   "source": [
    "In this section, we set up the necessary configurations for deploying our fine-tuned Mistral model on Amazon SageMaker. This involves defining the IAM role for SageMaker and setting up key model parameters for optimal performance on Inferentia instances with AWS Neuron. \n",
    "\n",
    "\n",
    "### Set Model Deployment Configuration:\n",
    "   - Configure the model’s environment variables using a dictionary (`hub`). These parameters optimize the model's performance on the `ml.inf2.xlarge` instance type and control important aspects of inference to ensure that the model is optimized for Inferentia instances, balancing speed, cost-efficiency, and accuracy. Key settings include:\n",
    "      - **`HF_MODEL_ID`**: The Hugging Face model ID of the compiled Mistral model.\n",
    "      - **`HF_NUM_CORES`**: Number of Neuron cores to allocate, providing parallel processing for efficient inference.\n",
    "      - **`HF_SEQUENCE_LENGTH`**: Maximum sequence length for inputs, which impacts memory usage and latency.\n",
    "      - **`HF_AUTO_CAST_TYPE`**: Precision setting, typically `fp16` to balance performance and memory efficiency.\n",
    "      - **`MAX_BATCH_SIZE`**: Defines the maximum number of inputs processed in a single batch, balancing latency and throughput.\n",
    "      - **`MAX_INPUT_TOKENS` and `MAX_TOTAL_TOKENS`**: Specifies the maximum tokens allowed for input and total tokens per request, ensuring responses fit within the configured limits.\n",
    "      - **`HF_TOKEN`**: Authentication token for Hugging Face, enabling access to model files stored on Hugging Face.\n",
    "      - **`MESSAGES_API_ENABLED`**: Enables conversational interactions by allowing message-based API requests.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a972c4b0-94c6-48ee-a440-d58f53cdbea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#role = sagemaker.get_execution_role()\n",
    "boto_session = boto3.Session(region_name=sagemaker_region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "role = sagemaker.get_execution_role(sagemaker_session=sagemaker_session)\n",
    "\n",
    "hub = {\n",
    "    \"HF_MODEL_ID\": HF_MODEL_ID_TO_PUSH,\n",
    "    \"HF_NUM_CORES\": str(NUM_CORES),\n",
    "    \"HF_SEQUENCE_LENGTH\": str(SEQUENCE_LENGTH),\n",
    "    \"HF_AUTO_CAST_TYPE\": PRECISION,\n",
    "    \"MAX_BATCH_SIZE\": str(BATCH_SIZE),\n",
    "    \"MAX_INPUT_TOKENS\": \"1800\",\n",
    "    \"MAX_TOTAL_TOKENS\": str(MAX_TOTAL_TOKENS),\n",
    "    \"HF_TOKEN\": HF_TOKEN,\n",
    "    \"MESSAGES_API_ENABLED\": \"true\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b485ab-8535-43e4-83cf-a5e230941614",
   "metadata": {},
   "source": [
    "### Deploy the compiled model on a `ml.inf2.xlarge` instance in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d5def9-1a33-4012-a0fa-5bc0f0acdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------!CPU times: user 515 ms, sys: 44.8 ms, total: 560 ms\n",
      "Wall time: 9min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface-neuronx\", \n",
    "                                            version=\"0.0.24\",\n",
    "                                            session=sagemaker_session,\n",
    "                                            region=sagemaker_region),\n",
    "    env=hub,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "# Set this flag to indicate that the model is precompiled\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# Deploy the model and get the predictor\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    container_startup_health_check_timeout=2400,\n",
    "    volume_size=512\n",
    ")\n",
    "\n",
    "sagemaker_endpoint_name = predictor.endpoint_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cd895-2a2f-4bd8-a2ad-c1347d3128a6",
   "metadata": {},
   "source": [
    "## Section 5: Test SageMaker Endpoint with both the HuggingFace's and SageMaker's APIs\n",
    "\n",
    "Send a sample request to the SageMaker endpoint to verify that the model is deployed and functioning correctly using the **HuggingFace API**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4a59ef-4170-4c88-a16f-763b2f116a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample Request 1 ---\n",
      "System Prompt: You are an expert in health insurance policies.\n",
      "User Query: What benefits do I get with my current health plan?\n",
      "Model Response:  To obtain a detailed understanding of the benefits covered under your health plan, please adhere to the following procedure:\n",
      "\n",
      "1. Access the {{WEBSITE_URL}}.\n",
      "2. Input your login information in the designated fields.\n",
      "3. Locate the {{BRIEF_SUMMARY_SECTION}} within the site.\n",
      "4. Select your specific health insurance policy from the options available.\n",
      "5. Examine the comprehensive list of benefits provided in your plan.\n",
      "\n",
      "\n",
      "--- Sample Request 2 ---\n",
      "System Prompt: You are an insurance advisor.\n",
      "User Query: How can I reduce my monthly insurance premium?\n",
      "Model Response:  To assist you in obtaining a reduction in your monthly insurance premium, please follow the steps outlined below:\n",
      "\n",
      "1. Examine the specifics of your existing policy, such as your coverage limits and deductibles, to ensure you have a comprehensive understanding of your current situation.\n",
      "2. Review your past claim history and identify any ways in which you can improve your risks. For instance, you may take safety measures to minimize damage or accidents in your property.\n",
      "3. Compile all relevant\n",
      "\n",
      "\n",
      "--- Sample Request 3 ---\n",
      "System Prompt: You are an expert in auto insurance policies.\n",
      "User Query: What happens if my car is totaled?\n",
      "Model Response:  If your vehicle is declared a total loss, your insurance provider will offer you a settlement based on the value of the vehicle at the time of the loss. Generally, the settlement process involves the following steps:\n",
      "\n",
      "1. Assessment of the damage: An adjuster from your insurance company will evaluate the extent of the damage to your vehicle and determine whether it is deemed a total loss.\n",
      "2. Estimation of the value of your vehicle: Your insurance provider will determine the actual cash value\n",
      "\n",
      "\n",
      "--- Sample Request 4 ---\n",
      "System Prompt: You are an expert in life insurance.\n",
      "User Query: Can you explain the difference between term and whole life insurance?\n",
      "Model Response:  To effectively compare term life insurance and whole life insurance, it is crucial to appreciate their fundamental differences.\n",
      "\n",
      "1. Term Life Insurance:\n",
      "\n",
      "Term life insurance is a type of coverage that offers protection for a specified term, such as 5, 10, 20, or 30 years. The policy only pays out the death benefit if the insured passes away during the designated term. Should the insured survive the term, the coverage expires, and there is\n",
      "\n",
      "\n",
      "--- Sample Request 5 ---\n",
      "System Prompt: You are an insurance claims specialist.\n",
      "User Query: What documents are needed to file a claim for home insurance?\n",
      "Model Response:  To initiate a claim for your {{INSURANCE_TYPE}} insurance coverage, please adhere to the following procedure for the submission of necessary documentation:\n",
      "\n",
      "1. Access our website at {{WEBSITE_URL}}.\n",
      "2. Sign in using your policy holder account information.\n",
      "3. Click on the {{CLAIM_SECTION}} section available on the site.\n",
      "4. Choose the option to {{FILE_CLAIM_OPTION}}.\n",
      "5. Complete\n",
      "\n",
      "\n",
      "--- Sample Request 6 ---\n",
      "System Prompt: You are a customer service representative for health insurance.\n",
      "User Query: Can I add my spouse to my health insurance policy?\n",
      "Model Response:  To include your spouse in your health insurance policy, please adhere to the following steps:\n",
      "\n",
      "1. Access your account on {{WEBSITE_URL}}.\n",
      "2. Proceed to the {{HEALTH_INSURANCE_SECTION}} section.\n",
      "3. Click on the option for managing dependents.\n",
      "4. Select the {{ADD_DEPENDENT_OPTION}} option.\n",
      "5. Input your spouse's personal information and ensure all details are correct.\n",
      "\n",
      "\n",
      "--- Sample Request 7 ---\n",
      "System Prompt: You are an expert in travel insurance policies.\n",
      "User Query: What coverage do I have if my flight is canceled?\n",
      "Model Response:  For travelers facing the cancellation of a flight, understanding the coverage linked to their travel insurance policy is essential. Generally, trip cancellation insurance provides compensation in situations when a traveler's trip is called off due to predefined reasons. These reasons may encompass personal issues, financial defaults, or environmental factors that could impact the travel arrangements.\n",
      "\n",
      "To ensure you receive the coverage you are entitled to, please follow the steps outlined below:\n",
      "\n",
      "1. Access the {{WEBSITE_\n",
      "\n",
      "\n",
      "--- Sample Request 8 ---\n",
      "System Prompt: You are a specialist in pet insurance.\n",
      "User Query: Does my policy cover emergency vet visits?\n",
      "Model Response:  To verify whether your pet insurance covers emergency vet visits, please adhere to the following guidelines:\n",
      "\n",
      "1. Access the {{WEBSITE_URL}}.\n",
      "2. Enter your account credentials including username and password.\n",
      "3. Find the {{HEALTH_INSURANCE_SECTION}} section on the website.\n",
      "4. Choose your pet insurance policy from the list presented.\n",
      "5. Select the {{INFORMATION_SECTION}} section which includes pertinent details about\n",
      "\n",
      "\n",
      "--- Sample Request 9 ---\n",
      "System Prompt: You are an insurance fraud investigator.\n",
      "User Query: What are some common signs of insurance fraud?\n",
      "Model Response:  As a professional insurance fraud investigator, I am well-positioned to identify potential signs of insurance fraud. These indicators can serve as valuable alert flags for further investigation.\n",
      "\n",
      "1. Claims with unusual patterns or frequency: A claim that stands out from the norms, either because of its excessive frequency or inconsistencies with past claims, should be carefully examined.\n",
      "2. Exaggerated or excessive claims: Alarm bells should ring when a claim is presented for an\n",
      "\n",
      "\n",
      "--- Sample Request 10 ---\n",
      "System Prompt: You are an advisor on property insurance.\n",
      "User Query: How do I increase the coverage for natural disasters?\n",
      "Model Response:  To enhance your coverage for natural disasters, please adhere to the following steps:\n",
      "\n",
      "1. Access your account via {{WEBSITE_URL}}.\n",
      "2. Proceed to the {{COVERAGE_SECTION}} area.\n",
      "3. Identify the specific policy that requires an upgrade.\n",
      "4. Choose the option for {{UPGRADE_COVERAGE_OPTION}}.\n",
      "5. Follow the instructions displayed on your screen to select the desired coverage for natural\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_sample_request(system_prompt, user_query):\n",
    "    \"\"\"\n",
    "    Creates a sample request structure for the predictor based on the given system prompt and user query.\n",
    "\n",
    "    Parameters:\n",
    "        system_prompt (str): The initial system prompt to set the model's role.\n",
    "        user_query (str): The user's query for the insurance model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A structured request for the SageMaker predictor.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.95,\n",
    "        }\n",
    "    }\n",
    "\n",
    "# List of different system prompts and user queries to test various scenarios\n",
    "system_user_queries = [\n",
    "    (\"You are an expert in health insurance policies.\", \"What benefits do I get with my current health plan?\"),\n",
    "    (\"You are an insurance advisor.\", \"How can I reduce my monthly insurance premium?\"),\n",
    "    (\"You are an expert in auto insurance policies.\", \"What happens if my car is totaled?\"),\n",
    "    (\"You are an expert in life insurance.\", \"Can you explain the difference between term and whole life insurance?\"),\n",
    "    (\"You are an insurance claims specialist.\", \"What documents are needed to file a claim for home insurance?\"),\n",
    "    (\"You are a customer service representative for health insurance.\", \"Can I add my spouse to my health insurance policy?\"),\n",
    "    (\"You are an expert in travel insurance policies.\", \"What coverage do I have if my flight is canceled?\"),\n",
    "    (\"You are a specialist in pet insurance.\", \"Does my policy cover emergency vet visits?\"),\n",
    "    (\"You are an insurance fraud investigator.\", \"What are some common signs of insurance fraud?\"),\n",
    "    (\"You are an advisor on property insurance.\", \"How do I increase the coverage for natural disasters?\")\n",
    "]\n",
    "\n",
    "# Loop through each system prompt and user query, create a request, and get a response from the predictor\n",
    "for i, (system_prompt, user_query) in enumerate(system_user_queries, start=1):\n",
    "    print(f\"--- Sample Request {i} ---\")\n",
    "    request = create_sample_request(system_prompt, user_query)\n",
    "    response = predictor.predict(request)\n",
    "    print(\"System Prompt:\", system_prompt)\n",
    "    print(\"User Query:\", user_query)\n",
    "    print(\"Model Response:\", response['choices'][0]['message']['content'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01069ed1-96db-4230-a8ba-47f3fc87e9d7",
   "metadata": {},
   "source": [
    "### Send a sample request to the model using the SageMaker API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cef969-b503-4e40-9ed4-a78926654711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'chat.completion', 'id': '', 'created': 1731451647, 'model': 'aboavent/Mistral-7B-Insurance-neuron', 'system_fingerprint': '2.1.1-native', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' There are several methods to help you reduce your monthly insurance premium:\\n\\n1. Shop around: Compare prices and features from various insurance providers to identify the most cost-effective option.\\n2. Customize your policy: Adjust your coverage limits to align with your needs and reduce unnecessary coverage that may add to your premium.\\n3. Bundle your policies: If you have multiple insurance needs, try acquiring several insurance products from the same provider to benefit from bundled discounts.'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 100, 'total_tokens': 100}}\n",
      " There are several methods to help you reduce your monthly insurance premium:\n",
      "\n",
      "1. Shop around: Compare prices and features from various insurance providers to identify the most cost-effective option.\n",
      "2. Customize your policy: Adjust your coverage limits to align with your needs and reduce unnecessary coverage that may add to your premium.\n",
      "3. Bundle your policies: If you have multiple insurance needs, try acquiring several insurance products from the same provider to benefit from bundled discounts.\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\", \n",
    "                                region_name=sagemaker_region)\n",
    "\n",
    "# Function to query the model on SageMaker\n",
    "def query_sagemaker_model(endpoint_name, query):\n",
    "    payload = {\n",
    "        \"model\": HF_MODEL_ID_TO_PUSH,  # Updated model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in customer support for Insurance.\"},\n",
    "            {\"role\": \"user\", \"content\": query}  # Send the user query as a string\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 4096,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.90,\n",
    "            \"max_length\": 4096,\n",
    "            \"stop\": None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the request to SageMaker endpoint\n",
    "        response = sagemaker_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        result = json.loads(response['Body'].read())\n",
    "        print(result)\n",
    "        return result['choices'][0]['message']['content']\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred with SageMaker: {e.response['Error']['Message']}\")\n",
    "        return None\n",
    "    \n",
    "if sagemaker_endpoint_name is None:\n",
    "    raise ValueError(\"sagemaker_endpoint_name is not set. Make sure to provide an endpoint name so you can query the model.\")\n",
    "    \n",
    "model_response = query_sagemaker_model(sagemaker_endpoint_name, \n",
    "                                       \"How can I reduce my monthly insurance premium?\")\n",
    "print(model_response)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ec6f0b-4abf-4b50-a870-c680462633dd",
   "metadata": {},
   "source": [
    "## Section 6: Convert Model to Safetensors Format for Bedrock\n",
    "\n",
    "In this section, we convert the compiled Mistral model into the `safetensors` format, which is a lightweight and efficient file format designed for securely storing large model weights. This conversion is necessary to make the model compatible with Amazon Bedrock, allowing us to easily import and use the model within the Bedrock environment.\n",
    "\n",
    "#### Important Note\n",
    "> For this conversion, it is recommended to use a larger instance  with **at least 128 GB of memory**. This ensures sufficient resources for loading and processing the entire model during the conversion process. Attempting this step on smaller instances may result in memory-related errors.\n",
    "\n",
    "#### Steps in This Section\n",
    "\n",
    "1. **Define Conversion Function**:\n",
    "   - We define a `convert_to_safetensors` function that loads the model and its tokenizer, then saves them in `safetensors` format.\n",
    "   - The function uses the Hugging Face Transformers library to load the model and save it in the `safetensors` format for compatibility with Amazon Bedrock.\n",
    "\n",
    "2. **Run Conversion**:\n",
    "   - Call the `convert_to_safetensors` function, specifying the model name (`MODEL_ID`) and the directory (`save_directory`) where the converted model files will be saved.\n",
    "   - This process generates two outputs:\n",
    "      - The model in `safetensors` format.\n",
    "      - The tokenizer, saved alongside the model.\n",
    "\n",
    "3. **Verify Conversion**:\n",
    "   - After conversion, we list the contents of the target directory (`save_directory`) to confirm that the model files have been saved correctly in the desired format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de3ec75-63a9-448f-8d2d-523d20cd988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model bitext/Mistral-7B-Insurance...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d60ae00f0c48f096e60a39950527f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting and saving model to /root/Mistral-7B-Insurance2 in safetensors format with max shard size of 2GB...\n",
      "Conversion complete!\n",
      "['model-00011-of-00015.safetensors', 'model-00002-of-00015.safetensors', 'tokenizer_config.json', 'model-00015-of-00015.safetensors', 'model-00006-of-00015.safetensors', 'model-00013-of-00015.safetensors', 'model-00004-of-00015.safetensors', 'tokenizer.model', 'model.safetensors.index.json', 'model-00008-of-00015.safetensors', 'config.json', 'model-00003-of-00015.safetensors', 'model-00012-of-00015.safetensors', 'generation_config.json', 'special_tokens_map.json', 'model-00007-of-00015.safetensors', 'model-00005-of-00015.safetensors', 'model-00014-of-00015.safetensors', 'model-00009-of-00015.safetensors', 'model-00010-of-00015.safetensors', 'model-00001-of-00015.safetensors']\n",
      "CPU times: user 22.4 s, sys: 1min 30s, total: 1min 52s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def convert_to_safetensors(model_name, save_directory, max_shard_size=\"3GB\"):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face model to safetensors format for Amazon Bedrock compatibility, with sharded saving.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the model to convert.\n",
    "        save_directory (str): Directory to save the converted model and tokenizer.\n",
    "        max_shard_size (str): Maximum size of each shard (e.g., \"3GB\").\n",
    "    \"\"\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    print(f\"Converting and saving model to {save_directory} in safetensors format with max shard size of {max_shard_size}...\")\n",
    "    model.save_pretrained(save_directory, safe=True, max_shard_size=max_shard_size)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    print(\"Conversion complete!\")\n",
    "\n",
    "# Specify the directory and model name\n",
    "save_directory = os.path.expanduser(\"~/Mistral-7B-Insurance\")\n",
    "os.makedirs(save_directory, exist_ok=True,)\n",
    "convert_to_safetensors(MODEL_ID, save_directory, \"2GB\")\n",
    "\n",
    "# List the contents of the save directory to verify the conversion\n",
    "print(os.listdir(save_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2d894-1183-437d-b300-ec341d2e4739",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 7: Upload Converted Model to S3\n",
    "\n",
    "Upload the `safetensors` formatted model files to an S3 bucket, making them accessible to Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fbaef30-ef65-4368-b3fc-51adb50c0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique S3 bucket name for this execution: mistral-7b-insurance-bedrock-import-20241112231130\n",
      "Bucket 'mistral-7b-insurance-bedrock-import-20241112231130' does not exist. Creating bucket...\n",
      "Bucket 'mistral-7b-insurance-bedrock-import-20241112231130' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model-00011-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00011-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:   5%|▍         | 1/21 [00:12<04:08, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00011-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00002-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00002-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  14%|█▍        | 3/21 [00:24<01:57,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00002-of-00015.safetensors uploaded successfully.\n",
      "Uploading tokenizer_config.json to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/tokenizer_config.json...\n",
      "tokenizer_config.json uploaded successfully.\n",
      "Uploading model-00015-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00015-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  19%|█▉        | 4/21 [00:34<02:16,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00015-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00006-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00006-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  24%|██▍       | 5/21 [00:48<02:46, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00006-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00013-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00013-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  29%|██▊       | 6/21 [01:01<02:47, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00013-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00004-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00004-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  33%|███▎      | 7/21 [01:14<02:44, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00004-of-00015.safetensors uploaded successfully.\n",
      "Uploading tokenizer.model to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/tokenizer.model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  43%|████▎     | 9/21 [01:15<01:07,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.model uploaded successfully.\n",
      "Uploading model.safetensors.index.json to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model.safetensors.index.json...\n",
      "model.safetensors.index.json uploaded successfully.\n",
      "Uploading model-00008-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00008-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  48%|████▊     | 10/21 [01:26<01:22,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00008-of-00015.safetensors uploaded successfully.\n",
      "Uploading config.json to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  52%|█████▏    | 11/21 [01:26<00:52,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json uploaded successfully.\n",
      "Uploading model-00003-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00003-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  57%|█████▋    | 12/21 [01:39<01:05,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00003-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00012-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00012-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  67%|██████▋   | 14/21 [01:50<00:42,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00012-of-00015.safetensors uploaded successfully.\n",
      "Uploading generation_config.json to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/generation_config.json...\n",
      "generation_config.json uploaded successfully.\n",
      "Uploading special_tokens_map.json to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/special_tokens_map.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  71%|███████▏  | 15/21 [01:50<00:25,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map.json uploaded successfully.\n",
      "Uploading model-00007-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00007-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  76%|███████▌  | 16/21 [02:02<00:32,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00007-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00005-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00005-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  81%|████████  | 17/21 [02:13<00:31,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00005-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00014-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00014-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  86%|████████▌ | 18/21 [02:26<00:28,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00014-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00009-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00009-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  90%|█████████ | 19/21 [02:39<00:20, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00009-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00010-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00010-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3:  95%|█████████▌| 20/21 [02:51<00:10, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00010-of-00015.safetensors uploaded successfully.\n",
      "Uploading model-00001-of-00015.safetensors to s3://mistral-7b-insurance-bedrock-import-20241112231130/safetensors/model-00001-of-00015.safetensors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to S3: 100%|██████████| 21/21 [03:05<00:00,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00001-of-00015.safetensors uploaded successfully.\n",
      "CPU times: user 1min 12s, sys: 41.5 s, total: 1min 53s\n",
      "Wall time: 3min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from datetime import datetime\n",
    "\n",
    "# Define S3 and local directory configurations\n",
    "s3_client = boto3.client(\"s3\", region_name=bedrock_region)\n",
    "\n",
    "base_bucket_name = \"mistral-7b-insurance-bedrock-import\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "s3_bucket_name = f\"{base_bucket_name}-{timestamp}\"\n",
    "s3_model_directory = \"safetensors\"\n",
    "\n",
    "local_model_directory = save_directory  # Use save_directory from previous step\n",
    "\n",
    "print(f\"Unique S3 bucket name for this execution: {s3_bucket_name}\")\n",
    "\n",
    "\n",
    "def create_bucket_if_not_exists(bucket_name, region=\"us-west-2\"):\n",
    "    \"\"\"\n",
    "    Creates the S3 bucket if it does not exist.\n",
    "    \n",
    "    Parameters:\n",
    "        bucket_name (str): The name of the bucket to create.\n",
    "        region (str): The AWS region for the bucket.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            print(f\"Bucket '{bucket_name}' does not exist. Creating bucket...\")\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "            print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "# Create the bucket if it doesn't exist\n",
    "create_bucket_if_not_exists(s3_bucket_name, bedrock_region)\n",
    "\n",
    "def upload_to_s3(local_directory, bucket, s3_directory):\n",
    "    \"\"\"\n",
    "    Uploads all files from a local directory to the specified S3 bucket and directory.\n",
    "\n",
    "    Parameters:\n",
    "        local_directory (str): Path to the local directory containing files to upload.\n",
    "        bucket (str): Name of the S3 bucket.\n",
    "        s3_directory (str): Directory path within the S3 bucket to store the files.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(local_directory) if os.path.isfile(os.path.join(local_directory, f))]\n",
    "    \n",
    "    # Progress bar for uploads\n",
    "    for filename in tqdm(files, desc=\"Uploading files to S3\"):\n",
    "        file_path = os.path.join(local_directory, filename)\n",
    "        s3_path = f\"{s3_directory}/{filename}\"\n",
    "        print(f\"Uploading {filename} to s3://{bucket}/{s3_path}...\")\n",
    "        s3_client.upload_file(file_path, bucket, s3_path)\n",
    "        print(f\"{filename} uploaded successfully.\")\n",
    "\n",
    "# Run the upload function\n",
    "upload_to_s3(local_model_directory, s3_bucket_name, s3_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddc69d-5133-402c-961a-d3ff4bb6326a",
   "metadata": {},
   "source": [
    "## Section 8: Import Model into Amazon Bedrock\n",
    "\n",
    "Create an IAM Execution Role for Bedrock with parameters to be used by\n",
    "a model import job in Amazon Bedrock using the Mistral 7b Insurance model files uploaded to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b27007-7f38-4c52-afda-845c2488e0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source account: 603555443475\n",
      "Creating IAM Role...\n",
      "Role 'BedrockModelImportExecutionRole' already exists.\n",
      "Attaching policy to IAM Role...\n",
      "Policy attached successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "# Retrieve the current account ID dynamically\n",
    "sts_client = boto3.client(\"sts\")\n",
    "source_account = sts_client.get_caller_identity()[\"Account\"]\n",
    "print(\"Source account: \" + source_account)\n",
    "\n",
    "# IAM client and role/policy details\n",
    "iam_client = boto3.client('iam')\n",
    "role_name = \"BedrockModelImportExecutionRole\"\n",
    "policy_name = \"BedrockModelImportPolicy\"\n",
    "\n",
    "# Define the trust policy to allow Bedrock to assume this role with specific conditions\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": source_account  \n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{bedrock_region}:{source_account}:model-import-job/*\"  \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define the permissions policy for S3 and Bedrock access\n",
    "permissions_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}\",\n",
    "                f\"arn:aws:s3:::{s3_bucket_name}/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModel\",\n",
    "                \"bedrock:GetModel\",\n",
    "                \"bedrock:ListModels\",\n",
    "                \"bedrock:CreateModelImportJob\",\n",
    "                \"bedrock:GetModelImportJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    print(\"Creating IAM Role...\")\n",
    "    role_response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description=\"Role for Amazon Bedrock model import job with S3 access\"\n",
    "    )\n",
    "    role_arn = role_response['Role']['Arn']\n",
    "    print(f\"IAM Role created with ARN: {role_arn}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "        print(f\"Role '{role_name}' already exists.\")\n",
    "        role_arn = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Attach the permissions policy to the role\n",
    "try:\n",
    "    print(\"Attaching policy to IAM Role...\")\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=policy_name,\n",
    "        PolicyDocument=json.dumps(permissions_policy)\n",
    "    )\n",
    "    print(\"Policy attached successfully.\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error attaching policy: {e}\")\n",
    "    raise\n",
    "\n",
    "# The role ARN will be used in the next cell to create the model import job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576a8cb-f4f8-4429-9c0d-5fc6743a8e6b",
   "metadata": {},
   "source": [
    "## Section 9: Create and Submit Model Import Job to Amazon Bedrock\n",
    "\n",
    "In this section, we set up and submit a model import job to Amazon Bedrock, which imports our converted model files from Amazon S3 into Bedrock for deployment and inference. This process imports the model to Bedrock, preparing it for deployment and usage with the Converse API.\n",
    "\n",
    "\n",
    "1. **Initialize Variables**: \n",
    "   - **`bedrock_client`**: Establishes a Bedrock client connection in the specified `bedrock_region`.\n",
    "   - **`s3_model_uri`**: Constructs the S3 URI path to the directory where our model files are stored.\n",
    "   - **`imported_model_name`**: Provides a user-friendly name for the model once it’s imported to Amazon Bedrock.\n",
    "   - **`job_name`**: Generates a unique name for the model import job by appending a timestamp, ensuring that each job has a unique identifier.\n",
    "\n",
    "2. **Submit the Model Import Job**:\n",
    "   - We use the `create_model_import_job` method of the `bedrock_client` to initiate the import job.\n",
    "   - This method requires:\n",
    "      - **`jobName`**: The unique job identifier (`job_name`).\n",
    "      - **`importedModelName`**: The name under which the model will be imported to Bedrock.\n",
    "      - **`roleArn`**: The ARN of the IAM role with permissions to access the S3 bucket and perform import operations.\n",
    "      - **`modelDataSource`**: Specifies the S3 URI containing the model files in `safetensors` format.\n",
    "      \n",
    "   - After initiating the job, the response from Bedrock includes details about the job status and configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec79f7da-29c8-4c50-baca-69c0a92625fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model import job created: {'ResponseMetadata': {'RequestId': '64e75bfd-de78-40ae-a19e-8e582cf74d75', 'HTTPStatusCode': 201, 'HTTPHeaders': {'date': 'Tue, 12 Nov 2024 23:22:23 GMT', 'content-type': 'application/json', 'content-length': '81', 'connection': 'keep-alive', 'x-amzn-requestid': '64e75bfd-de78-40ae-a19e-8e582cf74d75'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:bedrock:us-west-2:603555443475:model-import-job/9jh3bfmmr6wo'}\n",
      "{\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"64e75bfd-de78-40ae-a19e-8e582cf74d75\",\n",
      "        \"HTTPStatusCode\": 201,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"date\": \"Tue, 12 Nov 2024 23:22:23 GMT\",\n",
      "            \"content-type\": \"application/json\",\n",
      "            \"content-length\": \"81\",\n",
      "            \"connection\": \"keep-alive\",\n",
      "            \"x-amzn-requestid\": \"64e75bfd-de78-40ae-a19e-8e582cf74d75\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    },\n",
      "    \"jobArn\": \"arn:aws:bedrock:us-west-2:603555443475:model-import-job/9jh3bfmmr6wo\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "bedrock_client = boto3.client('bedrock', region_name=bedrock_region)  \n",
    "s3_model_uri = f\"s3://{s3_bucket_name}/{s3_model_directory}/\"  \n",
    "imported_model_name = f\"Mistral-7B-Insurance-Model-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# Use the IAM role ARN created in the previous cell\n",
    "job_name = f\"mistral-7b-insurance-import-job-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock_client.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,  # Use the ARN from the IAM role created in the previous cell\n",
    "    modelDataSource={'s3DataSource': {'s3Uri': s3_model_uri}}\n",
    ")\n",
    "\n",
    "print(\"Model import job created:\", response)\n",
    "print(json.dumps(response, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc77d7f-4778-4c48-96e3-760f9edf5c81",
   "metadata": {},
   "source": [
    "## Section 10: Monitor Bedrock Model Import Job Status\n",
    "\n",
    "In this section, we track the status of our model import job in Amazon Bedrock to ensure that the model is successfully imported and ready for deployment. Since model import jobs can take time, a periodic status check allows us to monitor the progress and handle any errors that may arise.\n",
    "\n",
    "1. **Set Polling Interval**:\n",
    "   - Define `polling_interval` to specify the time (in seconds) between each status check. Here, we set it to 30 seconds.\n",
    "\n",
    "2. **Define `check_job_status` Function**:\n",
    "   - The `check_job_status` function queries the current status of the model import job using the `get_model_import_job` API.\n",
    "   - Parameters:\n",
    "      - **`job_name`**: The unique identifier for the model import job, which we generated in the previous step.\n",
    "   - Returns:\n",
    "      - A dictionary with the job's current status (`Completed` or `Failed`), any failure message if applicable, and the `importedModelArn` if the job is successful.\n",
    "\n",
    "3. **Periodic Status Check Loop**:\n",
    "   - We initialize `imported_model_arn` as `None`.\n",
    "   - In an infinite loop, we call `check_job_status` every `polling_interval` seconds to retrieve the latest job status.\n",
    "   - The current status and any failure message are printed to provide real-time feedback.\n",
    "\n",
    "4. **Job Completion or Failure Handling**:\n",
    "   - The loop exits if the job reaches a final state (`Completed` or `Failed`).\n",
    "      - If `Completed`, the `importedModelArn` is stored for further use, and a success message is printed.\n",
    "      - If `Failed`, an error message is printed along with any specific failure message.\n",
    "\n",
    "5. **Set `imported_model_id` for Further Use**:\n",
    "   - Once the job completes successfully, `imported_model_id` is assigned the value of `importedModelArn`, which will be used to interact with the model in subsequent steps.\n",
    "\n",
    "This monitoring process allows us to seamlessly track the job’s progress and handle any issues, ensuring that the model is ready for deployment as soon as the import is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4259592e-43c2-4598-8536-9478e583920b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking status for job mistral-7b-insurance-import-job-20241112232223 every 30 seconds...\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: InProgress\n",
      "Current status: Completed\n",
      "Job mistral-7b-insurance-import-job-20241112232223 finished with status: Completed\n",
      "Imported Model ARN: arn:aws:bedrock:us-west-2:603555443475:imported-model/7qig1rjkxppp\n",
      "CPU times: user 137 ms, sys: 40.4 ms, total: 177 ms\n",
      "Wall time: 12min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Use the job name from the response of create_model_import_job to track the job\n",
    "polling_interval = 30  # Time in seconds between each status check\n",
    "\n",
    "def check_job_status(job_name):\n",
    "    \"\"\"\n",
    "    Checks the status of the model import job and returns the current status, failure message, and imported model ARN if available.\n",
    "\n",
    "    Parameters:\n",
    "        job_name (str): The name of the model import job to check.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the status, failure message, and imported model ARN if the job is completed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        status_response = bedrock_client.get_model_import_job(jobIdentifier=job_name)\n",
    "        return {\n",
    "            \"status\": status_response[\"status\"],\n",
    "            \"failureMessage\": status_response.get(\"failureMessage\", \"\"),\n",
    "            \"importedModelArn\": status_response.get(\"importedModelArn\", None)\n",
    "        }\n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop to check the job status periodically\n",
    "print(f\"Checking status for job {job_name} every {polling_interval} seconds...\")\n",
    "imported_model_arn = None\n",
    "while True:\n",
    "    result = check_job_status(job_name)\n",
    "    if result is None:\n",
    "        print(\"Unable to retrieve job status. Exiting.\")\n",
    "        break\n",
    "\n",
    "    status = result[\"status\"]\n",
    "    failure_message = result[\"failureMessage\"]\n",
    "    imported_model_arn = result[\"importedModelArn\"]\n",
    "    print(f\"Current status: {status}\")\n",
    "\n",
    "    # Check if the job has reached a final state\n",
    "    if status in [\"Completed\", \"Failed\"]:\n",
    "        if status == \"Failed\" and failure_message:\n",
    "            print(f\"Job failed with message: {failure_message}\")\n",
    "            imported_model_arn = None  # Clear the ARN if the job failed\n",
    "        else:\n",
    "            print(f\"Job {job_name} finished with status: {status}\")\n",
    "            print(f\"Imported Model ARN: {imported_model_arn}\")\n",
    "        break\n",
    "\n",
    "    # Wait before the next status check\n",
    "    time.sleep(polling_interval)\n",
    "\n",
    "# Set the model ID to the imported model ARN if the job was successful\n",
    "if imported_model_arn:\n",
    "    imported_model_id = imported_model_arn  # Assign the model ARN to model_id for further use\n",
    "else:\n",
    "    print(\"Model import job did not complete successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac39fe-7012-449b-95dc-faf60c391443",
   "metadata": {},
   "source": [
    "## Section 11: Call Imported Model Using Amazon Bedrock Converse API\n",
    "\n",
    "In this section, we send a test request to our imported model on Amazon Bedrock using the Converse API. Since models may take some time to become ready after import, we implement a retry mechanism with exponential backoff to handle cases where the model is temporarily unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a297cea-86d0-45bd-84bf-2b8d25376fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1 of 10: Sending conversation request...\n",
      "Error: Model is not ready for inference. Wait and try your request again. Retrying in 30 seconds...\n",
      "\n",
      "Attempt 2 of 10: Sending conversation request...\n",
      "Error: Model is not ready for inference. Wait and try your request again. Retrying in 60 seconds...\n",
      "\n",
      "Attempt 3 of 10: Sending conversation request...\n",
      "Response: To effectively understand your health insurance benefits, please adhere to the following steps:\n",
      "\n",
      "1. Access our website at {{WEBSITE_URL}}.\n",
      "2. Enter your login credentials to access your account.\n",
      "3. Proceed to the {{HEALTH_INSURANCE_SECTION}} section of your account.\n",
      "4. Click on the {{VIEW_DETAILS_TAB}} tab to review your health insurance details.\n",
      "\n",
      "Should you require additional support, please reach out to our customer service team via our helpline.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=bedrock_region)\n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define the conversation messages, with user role correctly structured\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"You are an expert in customer support for insurance. Please help me understand my health insurance benefits.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the converse function with retry mechanism\n",
    "def converse_with_retry(messages, max_retries=10, initial_wait=30):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock Converse API with retry logic for the 'Model is not ready' error.\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): List of conversation messages.\n",
    "        max_retries (int): Maximum number of retry attempts.\n",
    "        initial_wait (int): Initial wait time (in seconds) between retries, doubled after each attempt.\n",
    "\n",
    "    Returns:\n",
    "        dict: The API response from Bedrock if successful, None if all retries fail.\n",
    "    \"\"\"\n",
    "    retry_attempt = 0\n",
    "    wait_time = initial_wait\n",
    "\n",
    "    while retry_attempt < max_retries:\n",
    "        # Configure the conversation payload\n",
    "        converse_config = {\n",
    "            \"modelId\": imported_model_id,  # Use the imported model ARN as the model ID\n",
    "            \"messages\": messages,\n",
    "            \"inferenceConfig\": {\n",
    "                \"temperature\": 0.5\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nAttempt {retry_attempt + 1} of {max_retries}: Sending conversation request...\")\n",
    "\n",
    "        try:\n",
    "            response = bedrock_runtime_client.converse(**converse_config)\n",
    "            return response  # Return response if successful\n",
    "        except ClientError as e:\n",
    "            error_message = e.response['Error']['Message']\n",
    "            if \"Model is not ready for inference\" in error_message:\n",
    "                print(f\"Error: {error_message}. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)  # Wait before retrying\n",
    "                retry_attempt += 1\n",
    "                wait_time *= 2  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"An error occurred: {error_message}\")\n",
    "                return None  # Exit if error is not 'Model not ready'\n",
    "\n",
    "    print(\"Max retries reached. Model is still not ready.\")\n",
    "    return None\n",
    "\n",
    "# Run the conversation with retry logic\n",
    "response = converse_with_retry(messages)\n",
    "\n",
    "# Function to print the response if received\n",
    "def print_converse_response(response):\n",
    "    if response:\n",
    "        print(f\"Response: {response['output']['message']['content'][0]['text']}\")\n",
    "    else:\n",
    "        print(\"No response received.\")\n",
    "\n",
    "# Print the response\n",
    "print_converse_response(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6f89e-e69d-47e2-b138-3103ccbe76dc",
   "metadata": {},
   "source": [
    "## Section 12: Call Imported Model Using Amazon Bedrock Converse Streaming API\n",
    "\n",
    "In this section, we demonstrate how to use Amazon Bedrock's Converse Streaming API to interact with our imported model in real-time. The Converse Streaming API allows us to receive responses from the model as they are generated, providing a more interactive experience.\n",
    "\n",
    "#### Steps in This Section\n",
    "\n",
    "1. **Define Sample Messages**:\n",
    "   - We create a list of sample messages to simulate different customer support queries related to insurance. Each message serves as a prompt for the model to respond to.\n",
    "\n",
    "2. **Inference Configuration**:\n",
    "   - Set parameters such as `temperature` and `top_k` in `inference_config` and `additional_model_fields` to control the model's response style and randomness.\n",
    "\n",
    "3. **Define the `stream_conversation` Function**:\n",
    "   - The `stream_conversation` function uses the `converse_stream` method to send messages to the model and receive responses in a streamed format.\n",
    "   - Parameters:\n",
    "      - **`bedrock_client`**: The Bedrock runtime client initialized for the specified region.\n",
    "      - **`model_id`**: The ARN of the imported model, used to identify the model for inference.\n",
    "      - **`messages`**: The list of conversation messages to send.\n",
    "      - **`inference_config` and `additional_model_fields`**: Configurations that determine the response style and sampling behavior.\n",
    "   - As the response stream is received, the function processes different event types:\n",
    "      - **`messageStart`**: Indicates the start of a new message from the model.\n",
    "      - **`contentBlockDelta`**: Contains partial content from the model’s response, which is printed in real-time.\n",
    "      - **`messageStop`**: Marks the end of a message, including information on the stop reason.\n",
    "      - **`metadata`**: Provides metrics on token usage and latency.\n",
    "\n",
    "4. **Run the Streaming API for Multiple Test Cases**:\n",
    "   - We loop through each sample message in `sample_messages`, calling `stream_conversation` for each to receive and display responses from the model.\n",
    "   - This approach allows us to simulate different customer queries and observe how the model responds in real-time to each prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41005eb4-5186-426b-9a9d-7c94dc48ed06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/7qig1rjkxppp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #1: Can you help me understand my health insurance benefits?\n",
      "\n",
      "Role: assistant\n",
      "To effectively understand your health insurance benefits, please adhere to the following guidelines:\n",
      "\n",
      "1. Access the {{WEBSITE_URL}} and sign in to your account.\n",
      "2. Proceed to the {{HEALTH_INSURANCE_SECTION}} section of the website.\n",
      "3. Click on the {{VIEW_DETAILS_TAB}} tab to view your health insurance details.\n",
      "4. Analyze the available information regarding your coverage, deductibles, and any applicable limits.\n",
      "\n",
      "Should you require additional support, please reach out to our customer service team via the {{HELP"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/7qig1rjkxppp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SECTION}} section on our website.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 19\n",
      "Output tokens: 138\n",
      "Total tokens: 157\n",
      "Latency: 2109 milliseconds\n",
      "\n",
      "Finished streaming response for sample #1: Can you help me understand my health insurance benefits?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #2: What does my policy cover if I need to see a specialist?\n",
      "\n",
      "Role: assistant\n",
      " To determine the extent of coverage for visiting a specialist under your insurance policy, please adhere to the following guidelines:\n",
      "\n",
      "1. Access your account at {{WEBSITE_URL}}.\n",
      "2. Proceed to the {{COVERAGE_SECTION}} section of the website.\n",
      "3. Identify the specific insurance policy you are interested in.\n",
      "4. Select the policy to view its details and locate the section titled {{SPECIALIST_COVERAGE}}.\n",
      "5. Analyze the information provided to ascertain your coverage for specialist visits.\n",
      "\n",
      "Should you require any additional support, please reach out"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/7qig1rjkxppp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to our customer service team by calling the designated support number.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 22\n",
      "Output tokens: 141\n",
      "Total tokens: 163\n",
      "Latency: 2209 milliseconds\n",
      "\n",
      "Finished streaming response for sample #2: What does my policy cover if I need to see a specialist?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #3: Are dental treatments covered in my current insurance plan?\n",
      "\n",
      "Role: assistant\n",
      " To verify whether your dental treatments are covered by your insurance plan, please adhere to the following guidelines:\n",
      "\n",
      "1. Access your account at {{WEBSITE_URL}}.\n",
      "2. Proceed to the {{COVERAGE_SECTION}} section of the website.\n",
      "3. Choose your dental insurance policy from the displayed options.\n",
      "4. Select the {{DENTAL_COVERAGE_INFORMATION}} link to review your coverage details.\n",
      "\n",
      "Should you require additional support, do not hesitate to reach out to our"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/7qig1rjkxppp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " customer service team by dialing our support number.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 19\n",
      "Output tokens: 123\n",
      "Total tokens: 142\n",
      "Latency: 1986 milliseconds\n",
      "\n",
      "Finished streaming response for sample #3: Are dental treatments covered in my current insurance plan?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #4: How do I file a claim for a recent doctor visit?\n",
      "\n",
      "Role: assistant\n",
      " To initiate the claim process for your recent doctor visit, please adhere to the following guidelines:\n",
      "\n",
      "1. Access our website at {{WEBSITE_URL}}.\n",
      "2. Sign in to your account with your registered credentials.\n",
      "3. Proceed to the {{CLAIM_SECTION}} section of the site.\n",
      "4. Click on the {{FILE_CLAIM_OPTION}} to begin.\n",
      "5. Complete the claim form, ensuring all mandatory information is provided, along with any necessary documentation such as invoices or receipts.\n",
      "6. Double-check all entered details for correctness.\n",
      "7. Finalize your submission by selecting the {{SUBMIT_BUTTON}}.\n",
      "\n",
      "Upon receipt of your"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Streaming messages with model arn:aws:bedrock:us-west-2:603555443475:imported-model/7qig1rjkxppp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " submission, our claims department will assess your application and respond promptly.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 21\n",
      "Output tokens: 171\n",
      "Total tokens: 192\n",
      "Latency: 2638 milliseconds\n",
      "\n",
      "Finished streaming response for sample #4: How do I file a claim for a recent doctor visit?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Starting streaming response for sample #5: Can you explain what deductible means in my policy?\n",
      "\n",
      "Role: assistant\n",
      " A deductible is the amount you are required to pay out of pocket before your insurance coverage begins. For example, if your health insurance policy has a deductible of $1,000, you would be responsible for paying the first $1,000 of your healthcare expenses before your insurance provider starts covering the remaining costs. This arrangement helps to reduce the overall cost of insurance by encouraging policyholders to be more mindful of their healthcare expenditures.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 21\n",
      "Output tokens: 97\n",
      "Total tokens: 118\n",
      "Latency: 1724 milliseconds\n",
      "\n",
      "Finished streaming response for sample #5: Can you explain what deductible means in my policy?\n",
      "\n",
      "Finished streaming all test cases with model arn:aws:bedrock:us-west-2:603555443475:imported-model/7qig1rjkxppp.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=\"us-west-2\")  # Replace with your region\n",
    "\n",
    "# Ensure imported_model_id is set from the previous section where the model import job completed\n",
    "if not imported_model_id:\n",
    "    raise ValueError(\"Model ID (importedModelArn) is not set. Ensure the model import job completed successfully.\")\n",
    "\n",
    "# Define multiple conversation samples without system prompts\n",
    "sample_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you help me understand my health insurance benefits?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"What does my policy cover if I need to see a specialist?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Are dental treatments covered in my current insurance plan?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"How do I file a claim for a recent doctor visit?\"}]\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"Can you explain what deductible means in my policy?\"}]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Inference parameters\n",
    "inference_config = {\"temperature\": 0.5}\n",
    "additional_model_fields = {\"top_k\": 200}\n",
    "\n",
    "# Define the streaming converse function\n",
    "def stream_conversation(bedrock_client, model_id, messages, inference_config, additional_model_fields):\n",
    "    \"\"\"\n",
    "    Calls the Bedrock converse_stream API and handles streaming response.\n",
    "\n",
    "    Parameters:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (list): The messages to send.\n",
    "        inference_config (dict): The inference configuration to use.\n",
    "        additional_model_fields (dict): Additional model fields to use.\n",
    "    \"\"\"\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in metadata:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "# Example usage of streaming for multiple test cases\n",
    "try:\n",
    "    for i, messages in enumerate(sample_messages, 1):\n",
    "        print(\"\\n\" + \"=\"*50)  # Line separator for clarity\n",
    "        print(f\"\\nStarting streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "        stream_conversation(\n",
    "            bedrock_runtime_client,\n",
    "            imported_model_id,  # Use the imported model ARN as model ID\n",
    "            messages,\n",
    "            inference_config,\n",
    "            additional_model_fields\n",
    "        )\n",
    "        print(f\"\\nFinished streaming response for sample #{i}: {messages[0]['content'][0]['text']}\")\n",
    "except ClientError as err:\n",
    "    error_message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", error_message)\n",
    "    print(\"A client error occurred: \" + format(error_message))\n",
    "else:\n",
    "    print(f\"\\nFinished streaming all test cases with model {imported_model_id}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d365b-c379-4aff-8d3b-fa85640a81c7",
   "metadata": {},
   "source": [
    "## Section 13: Running the Streamlit Application for Model Interaction\n",
    "\n",
    "In this section, we’ll generate a configuration file for our Streamlit application and then run the app to interact with our deployed models on Amazon SageMaker and Amazon Bedrock.\n",
    "\n",
    "### Step 1: Generate `app-config.json`\n",
    "\n",
    "The Streamlit application (`app.py`) requires certain configuration details, such as the SageMaker region, Bedrock region, SageMaker endpoint name, Bedrock model ID, and SageMaker model ID (`sagemaker_model_id`). These parameters are essential for connecting the app to our deployed models.\n",
    "\n",
    "Run the following code to create the configuration file `app-config.json` with the necessary details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da5b3104-3278-41e0-9ccc-26d1efb037f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved to app-config.json\n"
     ]
    }
   ],
   "source": [
    "# Define configuration for Streamlit app\n",
    "config_data = {\n",
    "    \"sagemaker_region\": sagemaker_region,\n",
    "    \"bedrock_region\": bedrock_region,\n",
    "    \"sagemaker_endpoint_name\": sagemaker_endpoint_name,\n",
    "    \"bedrock_model_id\": imported_model_id,  # Use the imported model ARN as the Bedrock model ID\n",
    "    \"sagemaker_model_id\": HF_MODEL_ID_TO_PUSH  # Set the SageMaker model ID from the notebook variable\n",
    "}\n",
    "\n",
    "# Write configuration to app-config.json\n",
    "with open(\"app-config.json\", \"w\") as config_file:\n",
    "    json.dump(config_data, config_file, indent=4)\n",
    "\n",
    "print(\"Configuration saved to app-config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad786c5d-c21d-4ebb-82c8-5628dd5b0c91",
   "metadata": {},
   "source": [
    "### Step 2: Start the Streamlit Application\n",
    "\n",
    "Once `app-config.json` has been generated, you can launch the Streamlit app by running the following command in your terminal:\n",
    "\n",
    ">**streamlit run app.py --server.port 8501 --server.headless true**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84129c03-fc09-4d32-8179-1bf38696cb16",
   "metadata": {},
   "source": [
    "### Step 3: Access the Application\n",
    "\n",
    "Once Streamlit is running, you can access the application in your browser.\n",
    "\n",
    "- **If running locally**: Open your browser and navigate to **[http://localhost:8501](http://localhost:8501)**.\n",
    "\n",
    "- **If running on Amazon SageMaker Studio**:\n",
    "    1. Take the base URL of your SageMaker Studio environment. This typically looks like:\n",
    "       ```\n",
    "       https://<your-host-name>/jupyter/default/lab\n",
    "       ```\n",
    "    2. Replace `/lab` at the end of the URL with `/proxy/8501/`, so the final URL becomes:\n",
    "       ```\n",
    "       https://<your-host-name>/jupyter/default/proxy/8501/\n",
    "       ```\n",
    "       This URL will direct you to the Streamlit application within SageMaker Studio.\n",
    "\n",
    "- **For other cloud or remote environments**: You may need to configure port forwarding to access the application. Check the environment’s documentation for instructions on setting up port forwarding or consult the Streamlit URL provided by the specific platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa1ec3a-b1e3-43ac-84fd-8d0b4a46941a",
   "metadata": {},
   "source": [
    "![Streamlit Chatbot App](streamlit-chatbot-video.gif \"Streamlit Chatbot App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18ff5c-681f-4356-ace1-78092dc0ce26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e4a80-eada-46cf-a664-b5a712c29bfd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we walked through the end-to-end process of deploying a fine-tuned Mistral model for insurance-related customer support on two different AWS platforms: Amazon SageMaker and Amazon Bedrock. This exercise provided a comprehensive look at how to optimize, deploy, and interact with a Mistral model using AWS’s specialized infrastructure.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "By completing this notebook, you should now have an understanding of:\n",
    "- **Model Optimization and Compilation**: How to prepare a model for efficient deployment on AWS Inferentia instances using Neuron, including the compilation of a Hugging Face model to a Neuron-compatible format with `optimum-cli`.\n",
    "- **Deploying Models on Amazon SageMaker**: How to set up and deploy the model on a `ml.inf2.xlarge` instance, leveraging SageMaker’s managed inference capabilities for low-latency, cost-effective serving.\n",
    "- **Converting Models for Amazon Bedrock**: How to convert the model to Hugging Face’s `safetensors` format and upload it to Amazon S3 for import into Amazon Bedrock.\n",
    "- **Using Amazon Bedrock's Converse API**: How to use the Bedrock Converse API to query the model, including real-time streaming of responses for enhanced interactivity.\n",
    "- **Building an Interactive Application with Streamlit**: How to connect both deployed models (SageMaker and Bedrock) to a custom chatbot interface, providing a hands-on experience for interacting with the model through a simple UI.\n",
    "\n",
    "### Value Proposition\n",
    "\n",
    "The techniques demonstrated in this notebook highlight the versatility and scalability of AWS’s machine learning and AI infrastructure:\n",
    "- **Cost-Effective Inference**: With Inferentia-based SageMaker deployment, we can run high-performance inferences while minimizing costs.\n",
    "- **Flexibility Across Platforms**: Amazon Bedrock provides a flexible environment for model hosting and deployment, enabling broader access to generative AI.\n",
    "- **Seamless User Interaction**: By connecting the models to a Streamlit app, we created an accessible interface that makes it easy for end-users to interact with the model and get real-time responses.\n",
    "\n",
    "This workflow demonstrated a full deployment lifecycle, from model preparation and compilation to deployment, interaction, and user application, empowering you to build scalable AI solutions on AWS. With these skills, you’re now well-equipped to deploy similar language models across different AWS environments to meet diverse operational needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557f153-e77f-4d96-bebb-918288814918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 4.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-311-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
